"""
================================================================================
æ•°æ®é›†åŠ è½½å‡½æ•°æ¨¡å—
================================================================================

åŒ…å«: load_dataset
"""

import torch
from torch.utils.data import DataLoader, Subset

from ..utils import get_logger
from .preloaded import DATASET_REGISTRY
from .robustness import CorruptionDataset, OODDataset

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ•°æ®é›†åŠ è½½å‡½æ•°                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_dataset(cfg, mode: str = "all"):
    """
    æŒ‰éœ€åŠ è½½æ•°æ®é›†

    Args:
        cfg: é…ç½®å¯¹è±¡
        mode: åŠ è½½æ¨¡å¼
            - "train": ä»…è¿”å› (train_loader, val_loader)
            - "eval": ä»…è¿”å› (test_loader, corruption_dataset, ood_dataset)
            - "all": è¿”å›å…¨éƒ¨ (train_loader, val_loader, test_loader, c_ds, o_ds)
    """
    dataset_name = cfg.dataset_name.lower()
    DatasetClass = _get_dataset_class(dataset_name)

    if mode == "train":
        train_loader, val_loader = _prepare_train_loaders(cfg, DatasetClass)
        get_logger().info(f"ğŸ“Š è®­ç»ƒæ•°æ®é›†åŠ è½½å®Œæˆ: {dataset_name.upper()}")
        return train_loader, val_loader

    elif mode == "eval":
        test_loader = _prepare_test_loader(cfg, DatasetClass)
        robustness_suite = _init_robustness_group(cfg, dataset_name)
        get_logger().info(f"ğŸ“Š è¯„ä¼°æ•°æ®é›†åŠ è½½å®Œæˆ: {dataset_name.upper()}")
        return test_loader, *robustness_suite


def _get_dataset_class(name):
    """ä»æ³¨å†Œè¡¨è·å–ç±»ï¼Œå¤„ç†é”™è¯¯"""
    if name not in DATASET_REGISTRY:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ•°æ®é›†: {name}. å¯ç”¨: {list(DATASET_REGISTRY.keys())}"
        )
    return DATASET_REGISTRY[name]


def _prepare_train_loaders(cfg, DatasetClass):
    """åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    extra = {}
    if not getattr(DatasetClass, "HAS_OFFICIAL_SPLIT", True):
        extra = {"test_split": cfg.test_split, "seed": cfg.seed}
    train_full = DatasetClass(root=cfg.data_root, train=True, **extra)

    # è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
    v_size = int(len(train_full) * cfg.val_split)
    t_size = len(train_full) - v_size
    idx = torch.randperm(
        len(train_full), generator=torch.Generator().manual_seed(cfg.seed)
    )

    train_sub = Subset(train_full, idx[:t_size].tolist())
    val_sub = Subset(train_full, idx[t_size:].tolist())

    kwargs = _get_loader_kwargs(cfg)
    train_loader = DataLoader(
        train_sub, batch_size=cfg.batch_size, shuffle=True, **kwargs
    )
    val_loader = DataLoader(
        val_sub, batch_size=cfg.batch_size * 2, shuffle=False, **kwargs
    )
    return train_loader, val_loader


def _prepare_test_loader(cfg, DatasetClass):
    """ä»…åŠ è½½æµ‹è¯•é›†"""
    extra = {}
    if not getattr(DatasetClass, "HAS_OFFICIAL_SPLIT", True):
        extra = {"test_split": cfg.test_split, "seed": cfg.seed}
    test_ds = DatasetClass(root=cfg.data_root, train=False, **extra)

    kwargs = _get_loader_kwargs(cfg)
    return DataLoader(test_ds, batch_size=cfg.batch_size * 2, shuffle=False, **kwargs)


def _get_loader_kwargs(cfg):
    """è·å– DataLoader å…¬å…±å‚æ•°"""
    return {
        "num_workers": cfg.num_workers,
        "pin_memory": cfg.pin_memory,
        "persistent_workers": cfg.persistent_workers and cfg.num_workers > 0,
    }


def _init_robustness_group(cfg, name):
    """æŒ‰éœ€æ¢ç´¢å¹¶åŠ è½½é²æ£’æ€§æ•°æ®é›†"""
    results = []

    # Corruption
    c_ds = None
    if cfg.corruption_dataset:
        try:
            c_ds = CorruptionDataset(name, cfg.data_root)
        except Exception as e:
            get_logger().warning(f"   âš ï¸ Corruption æ•°æ®é›†ä¸å¯ç”¨: {e}")
    results.append(c_ds)

    # OOD
    o_ds = None
    if cfg.ood_dataset:
        try:
            o_ds = OODDataset(id_dataset=name, root=cfg.data_root)
        except Exception as e:
            get_logger().warning(f"   âš ï¸ OOD æ•°æ®é›†ä¸å¯ç”¨: {e}")
    results.append(o_ds)

    return tuple(results)
