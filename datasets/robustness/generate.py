"""
================================================================================
ç»Ÿä¸€æ•°æ®ç”Ÿæˆè„šæœ¬ (SDXL Lightning ç‰ˆ)
================================================================================

æ”¯æŒä¸‰ç§æ•°æ®ç±»å‹çš„ç”Ÿæˆ:
- Corruption: ä½¿ç”¨ imagecorruptions åº“ç”ŸæˆæŸåæ•°æ®
- Domain Shift: ä½¿ç”¨ SDXL Lightning Img2Img ç”Ÿæˆé£æ ¼è¿ç§»æ•°æ®
- OOD: ä½¿ç”¨ SDXL Lightning Text2Img ç”Ÿæˆåˆ†å¸ƒå¤–æ•°æ®

ä½¿ç”¨ç¤ºä¾‹:
    python -m ensemble.datasets.robustness.generate --type corruption --dataset cifar10
    python -m ensemble.datasets.robustness.generate --type domain --dataset cifar10
    python -m ensemble.datasets.robustness.generate --type ood --dataset cifar10
"""

import argparse
import warnings
from pathlib import Path
from typing import Optional

import numpy as np
from PIL import Image
from tqdm import tqdm


def patch_dependencies():
    """Monkey-patch dependencies for imagecorruptions compatibility."""
    try:
        import skimage.filters

        original_gaussian = skimage.filters.gaussian

        def patched_gaussian(*args, **kwargs):
            if "multichannel" in kwargs:
                multichannel = kwargs.pop("multichannel")
                if multichannel and "channel_axis" not in kwargs:
                    kwargs["channel_axis"] = -1
            return original_gaussian(*args, **kwargs)

        skimage.filters.gaussian = patched_gaussian
    except (ImportError, AttributeError):
        pass
    try:
        import numpy as np

        if not hasattr(np, "float_"):
            np.float_ = np.float64
    except (ImportError, AttributeError):
        pass


patch_dependencies()

import torch
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file

warnings.filterwarnings("ignore", category=UserWarning, module="pkg_resources")
warnings.filterwarnings("ignore", category=UserWarning, module="imagecorruptions")

try:
    from diffusers import (
        EulerDiscreteScheduler,
        StableDiffusionXLImg2ImgPipeline,
        StableDiffusionXLPipeline,
        UNet2DConditionModel,
    )
except ImportError:
    pass

from ...config import Config
from ...utils import ensure_dir, get_logger
from ..preloaded import DATASET_REGISTRY
from .corruption import CORRUPTIONS, SEVERITIES

# =============================================================================
# å¯è§†åŒ–å·¥å…·
# =============================================================================


def save_visual_comparison(
    original_imgs: np.ndarray,
    processed_imgs: np.ndarray,
    output_path: Path,
    title: str,
    num_samples: int = 8,
):
    """ä¿å­˜åŸå§‹å›¾åƒä¸å¤„ç†åå›¾åƒçš„å¯¹æ¯”ç½‘æ ¼"""
    n = min(len(original_imgs), num_samples)
    if n == 0:
        return

    indices = np.linspace(0, len(original_imgs) - 1, n, dtype=int)
    orig = original_imgs[indices]
    proc = processed_imgs[indices]

    h, w = orig.shape[1:3]
    grid = Image.new("RGB", (w * n, h * 2))

    for i, (o, p) in enumerate(zip(orig, proc)):
        grid.paste(Image.fromarray(o.astype(np.uint8)), (i * w, 0))
        grid.paste(Image.fromarray(p.astype(np.uint8)), (i * w, h))

    ensure_dir(output_path.parent)
    grid.save(str(output_path))
    get_logger().info(f"ğŸ“Š å¯è§†åŒ–ä¿å­˜: {output_path}")


# =============================================================================
# å›¾åƒå¤„ç†å·¥å…·
# =============================================================================


def _prepare_pil_batch(images_np: np.ndarray, target_size: int = 1024):
    """å°† numpy æ‰¹é‡å›¾åƒè½¬æ¢ä¸º PIL æ ¼å¼å¹¶ç»Ÿä¸€ç¼©æ”¾"""
    return [
        Image.fromarray(img.astype(np.uint8)).resize(
            (target_size, target_size), Image.LANCZOS
        )
        for img in images_np
    ]


def _convert_to_numpy_batch(images_pil: list, target_size: tuple):
    """å°† PIL æ‰¹é‡å›¾åƒæ¢å¤åˆ°ç›®æ ‡å°ºå¯¸å¹¶è½¬å› numpy æ ¼å¼"""
    return [np.array(img.resize(target_size, Image.LANCZOS)) for img in images_pil]


def _get_gpu_id(device: str):
    """ä»è®¾å¤‡å­—ç¬¦ä¸²ä¸­æå– GPU ID"""
    try:
        return int(device.split(":")[-1])
    except (ValueError, IndexError):
        return 0


def _check_existing_dataset(output_dir: Path, force: bool):
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨"""
    if output_dir.exists() and not force:
        get_logger().info(f"â­ï¸ æ•°æ®é›†å·²å­˜åœ¨: {output_dir}ï¼Œè·³è¿‡ç”Ÿæˆ")
        return True
    ensure_dir(output_dir)
    return False


def _load_test_set_numpy(DatasetClass, root, seed=42):
    """åŠ è½½æµ‹è¯•é›†å¹¶è½¬æ¢ä¸º numpy æ ¼å¼"""
    extra_kwargs = {}
    if not getattr(DatasetClass, "HAS_OFFICIAL_SPLIT", True):
        extra_kwargs["seed"] = seed
    dataset = DatasetClass(root=root, train=False, **extra_kwargs)
    images_np = dataset.images.permute(0, 2, 3, 1).numpy()
    labels_np = dataset.targets.numpy()
    return images_np, labels_np


def _sample_dataset(images_np, labels_np, n, seed=42):
    """å¯¹æ•°æ®é›†è¿›è¡ŒéšæœºæŠ½æ ·"""
    if n is None or n >= len(images_np):
        return images_np, labels_np
    np.random.seed(seed)
    indices = np.random.choice(len(images_np), size=n, replace=False)
    return images_np[indices], labels_np[indices]


# =============================================================================
# Corruption ç”Ÿæˆå™¨
# =============================================================================


class CorruptionGenerator:
    """Corruption ç”Ÿæˆå™¨ - åŸºäº imagecorruptions åº“"""

    CORRUPTIONS = CORRUPTIONS
    SEVERITIES = SEVERITIES

    @staticmethod
    def apply(img: np.ndarray, corruption_type: str, severity: int = 5) -> np.ndarray:
        """å¯¹å•å¼ å›¾åƒåº”ç”¨ corruption"""
        try:
            from imagecorruptions import corrupt
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£… imagecorruptions: pip install imagecorruptions")

        if corruption_type not in CorruptionGenerator.CORRUPTIONS:
            raise ValueError(f"Unknown corruption: {corruption_type}")

        img_uint8 = np.clip(img, 0, 255).astype(np.uint8)
        corrupted = corrupt(
            img_uint8, corruption_name=corruption_type, severity=severity
        )
        return corrupted.astype(np.float32)

    @staticmethod
    def apply_batch(
        images: np.ndarray,
        corruption_type: str,
        severity: int = 5,
        seed: Optional[int] = None,
    ) -> np.ndarray:
        """æ‰¹é‡åº”ç”¨ corruption"""
        if seed is not None:
            np.random.seed(seed)
        return np.stack(
            [
                CorruptionGenerator.apply(img, corruption_type, severity)
                for img in images
            ]
        )


# =============================================================================
# SDXL Lightning Pipeline åŠ è½½å™¨
# =============================================================================


class LightningPipelineLoader:
    """SDXL Lightning Pipeline åŠ è½½å™¨ (å•ä¾‹æ¨¡å¼)"""

    _text2img_cache = {}
    _img2img_cache = {}

    @classmethod
    def get_text2img(
        cls, device: str, base_model: str, repo: str, ckpt: str
    ) -> "StableDiffusionXLPipeline":
        """è·å– Text2Img Pipeline (å¸¦ç¼“å­˜)"""
        if device not in cls._text2img_cache:
            get_logger().info(f"ğŸ“¥ [{device}] åŠ è½½ SDXL Lightning Text2Img...")
            unet = cls._load_unet(device, base_model, repo, ckpt)
            pipe = StableDiffusionXLPipeline.from_pretrained(
                base_model, unet=unet, torch_dtype=torch.float16, variant="fp16"
            ).to(device)
            pipe.scheduler = EulerDiscreteScheduler.from_config(
                pipe.scheduler.config, timestep_spacing="trailing"
            )
            pipe.set_progress_bar_config(disable=True)
            cls._try_enable_optimizations(pipe)
            cls._text2img_cache[device] = pipe
        return cls._text2img_cache[device]

    @classmethod
    def get_img2img(
        cls, device: str, base_model: str, repo: str, ckpt: str
    ) -> "StableDiffusionXLImg2ImgPipeline":
        """è·å– Img2Img Pipeline (å¸¦ç¼“å­˜)"""
        if device not in cls._img2img_cache:
            get_logger().info(f"ğŸ“¥ [{device}] åŠ è½½ SDXL Lightning Img2Img...")
            unet = cls._load_unet(device, base_model, repo, ckpt)
            pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
                base_model, unet=unet, torch_dtype=torch.float16, variant="fp16"
            ).to(device)
            pipe.scheduler = EulerDiscreteScheduler.from_config(
                pipe.scheduler.config, timestep_spacing="trailing"
            )
            pipe.set_progress_bar_config(disable=True)
            cls._try_enable_optimizations(pipe)
            cls._img2img_cache[device] = pipe
        return cls._img2img_cache[device]

    @staticmethod
    def _load_unet(device: str, base_model: str, repo: str, ckpt: str):
        """åŠ è½½ Lightning UNet (æ”¯æŒæœ¬åœ°è·¯å¾„)"""
        unet = UNet2DConditionModel.from_config(base_model, subfolder="unet").to(
            device, torch.float16
        )

        local_path = Path(repo) / ckpt
        if local_path.exists():
            get_logger().info(f"ğŸš€ åŠ è½½æœ¬åœ° Lightning æƒé‡: {local_path}")
            state_dict = load_file(str(local_path), device=device)
        else:
            get_logger().info(f"ğŸŒ ä» Hugging Face ä¸‹è½½æƒé‡: {repo}/{ckpt}")
            state_dict = load_file(hf_hub_download(repo, ckpt), device=device)

        unet.load_state_dict(state_dict)
        return unet

    @staticmethod
    def _try_enable_optimizations(pipe):
        """å°è¯•å¯ç”¨æ˜¾å­˜ä¼˜åŒ–"""
        try:
            pipe.enable_xformers_memory_efficient_attention()
        except Exception:
            pass


# =============================================================================
# Domain Shift ç”Ÿæˆå™¨ (SDXL Lightning)
# =============================================================================


class DomainGenerator:
    """Domain Shift ç”Ÿæˆå™¨ - åŸºäº SDXL Lightning Img2Img"""

    def __init__(
        self,
        device: str = "cuda",
        base_model: str = "stabilityai/stable-diffusion-xl-base-1.0",
        lightning_repo: str = "ByteDance/SDXL-Lightning",
        lightning_ckpt: str = "sdxl_lightning_4step_unet.safetensors",
        styles: Optional[dict] = None,
        num_steps: int = 4,
    ):
        self.device = device
        self.base_model = base_model
        self.lightning_repo = lightning_repo
        self.lightning_ckpt = lightning_ckpt
        self.styles = styles or {}
        self.num_steps = num_steps
        self._pipe = None

    def _get_pipe(self):
        """è·å– Img2Img Pipeline"""
        if self._pipe is None:
            self._pipe = LightningPipelineLoader.get_img2img(
                self.device, self.base_model, self.lightning_repo, self.lightning_ckpt
            )
        return self._pipe

    def apply_batch(
        self, images: np.ndarray, style: str, strength: float, batch_size: int = 24
    ) -> np.ndarray:
        """æ‰¹é‡é£æ ¼è½¬æ¢"""
        if style not in self.styles:
            raise ValueError(f"Unknown style: {style}")

        pipe = self._get_pipe()
        prompt = self.styles[style]
        results = []

        pbar = tqdm(
            range(0, len(images), batch_size),
            desc=f"      [{self.device}] {style}/{strength}",
            position=_get_gpu_id(self.device),
            leave=False,
            mininterval=1.0,
        )

        for i in pbar:
            batch = images[i : i + batch_size]
            orig_h, orig_w = batch.shape[1], batch.shape[2]

            pils = _prepare_pil_batch(batch, target_size=1024)
            outputs = pipe(
                prompt=[prompt] * len(pils),
                image=pils,
                strength=strength,
                guidance_scale=0.0,
                num_inference_steps=self.num_steps,
            ).images

            results.extend(_convert_to_numpy_batch(outputs, (orig_w, orig_h)))

        return np.stack(results)


# =============================================================================
# OOD ç”Ÿæˆå™¨ (SDXL Lightning)
# =============================================================================


class OODGenerator:
    """OOD ç”Ÿæˆå™¨ - åŸºäº SDXL Lightning Text2Img"""

    def __init__(
        self,
        device: str = "cuda",
        base_model: str = "stabilityai/stable-diffusion-xl-base-1.0",
        lightning_repo: str = "ByteDance/SDXL-Lightning",
        lightning_ckpt: str = "sdxl_lightning_4step_unet.safetensors",
        prompts: Optional[list] = None,
        num_steps: int = 4,
    ):
        self.device = device
        self.base_model = base_model
        self.lightning_repo = lightning_repo
        self.lightning_ckpt = lightning_ckpt
        self.prompts = prompts or []
        self.num_steps = num_steps
        self._pipe = None

    def _get_pipe(self):
        """è·å– Text2Img Pipeline"""
        if self._pipe is None:
            self._pipe = LightningPipelineLoader.get_text2img(
                self.device, self.base_model, self.lightning_repo, self.lightning_ckpt
            )
        return self._pipe

    def generate_batch(
        self,
        num_samples: int,
        target_size: int = 64,
        batch_size: int = 24,
        seed: Optional[int] = None,
    ) -> np.ndarray:
        """æ‰¹é‡ç”Ÿæˆ OOD å›¾åƒ"""
        import random

        if seed is not None:
            random.seed(seed)

        pipe = self._get_pipe()
        results = []

        pbar = tqdm(
            range(0, num_samples, batch_size),
            desc=f"      [{self.device}] OOD ç”Ÿæˆ",
            position=_get_gpu_id(self.device),
            leave=False,
            mininterval=1.0,
        )

        for i in pbar:
            current_bs = min(batch_size, num_samples - i)
            prompts = [random.choice(self.prompts) for _ in range(current_bs)]

            outputs = pipe(
                prompt=prompts,
                height=1024,
                width=1024,
                guidance_scale=0.0,
                num_inference_steps=self.num_steps,
            ).images

            results.extend(_convert_to_numpy_batch(outputs, (target_size, target_size)))

        return np.stack(results)


# =============================================================================
# å¹¶è¡Œå¤„ç†åŠ©æ‰‹
# =============================================================================


def _process_single_corruption(args):
    """å•ç§ corruption å¤„ç†å‡½æ•° (ç”¨äº multiprocessing)"""
    corruption, images_np, severities, output_dir, seed = args
    all_severities = []
    for severity in severities:
        corrupted = CorruptionGenerator.apply_batch(
            images_np, corruption, severity, seed=seed
        )
        all_severities.append(corrupted.astype(np.uint8))

    stacked = np.concatenate(all_severities, axis=0)
    np.save(str(output_dir / f"{corruption}.npy"), stacked)
    return corruption


def _worker_domain(
    device,
    styles,
    strengths,
    images_np,
    labels_np,
    output_dir,
    dataset_name,
    batch_size,
    base_model,
    lightning_repo,
    lightning_ckpt,
    full_styles_dict,
    num_steps,
):
    """Domain å·¥ä½œè€…çº¿ç¨‹ (ç”¨äº GPU å¹¶è¡Œ)"""
    generator = DomainGenerator(
        device=device,
        base_model=base_model,
        lightning_repo=lightning_repo,
        lightning_ckpt=lightning_ckpt,
        styles=full_styles_dict,
        num_steps=num_steps,
    )
    DatasetClass = DATASET_REGISTRY[dataset_name]

    for style in styles:
        for strength in strengths:
            get_logger().info(f"   [{device}] ç”Ÿæˆ: {style} (strength={strength})...")
            strength_dir = output_dir / style / str(strength)

            for class_idx in range(DatasetClass.NUM_CLASSES):
                ensure_dir(strength_dir / f"class_{class_idx:04d}")

            styled_images = generator.apply_batch(
                images_np, style, strength, batch_size=batch_size
            )

            for i, (img, label) in enumerate(zip(styled_images, labels_np)):
                img_path = strength_dir / f"class_{label:04d}" / f"img_{i}.png"
                Image.fromarray(img).save(str(img_path))


def _worker_ood_gpu(
    gpu_id,
    n,
    target_size,
    bs,
    seed,
    q,
    base_model,
    lightning_repo,
    lightning_ckpt,
    prompts,
    num_steps,
):
    """OOD å·¥ä½œè€…çº¿ç¨‹ (ç”¨äº GPU å¹¶è¡Œ)"""
    generator = OODGenerator(
        device=f"cuda:{gpu_id}",
        base_model=base_model,
        lightning_repo=lightning_repo,
        lightning_ckpt=lightning_ckpt,
        prompts=prompts,
        num_steps=num_steps,
    )
    imgs = generator.generate_batch(
        num_samples=n, target_size=target_size, batch_size=bs, seed=seed + gpu_id
    )
    q.put(imgs)


# =============================================================================
# ç”Ÿæˆå‡½æ•°
# =============================================================================


def generate_corruption_dataset(
    dataset_name: str,
    root: str = "./data",
    seed: int = 42,
    force: bool = False,
) -> Path:
    """é¢„ç”Ÿæˆ corruption æ•°æ®é›† (ä½¿ç”¨ CPU å¤šè¿›ç¨‹åŠ é€Ÿ)"""
    import multiprocessing
    import os

    if dataset_name not in DATASET_REGISTRY:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")

    DatasetClass = DATASET_REGISTRY[dataset_name]
    output_dir = Path(root) / f"{DatasetClass.NAME}-C"

    if _check_existing_dataset(output_dir, force):
        return output_dir

    get_logger().info(f"ğŸ”§ ç”Ÿæˆ Corruption: {DatasetClass.NAME}-C...")

    images_np, labels_np = _load_test_set_numpy(DatasetClass, root, seed)
    total_samples = len(labels_np)

    tasks = [(c, images_np, SEVERITIES, output_dir, seed) for c in CORRUPTIONS]
    num_cpus = os.cpu_count()

    with multiprocessing.Pool(processes=min(len(CORRUPTIONS), num_cpus)) as pool:
        list(
            tqdm(
                pool.imap_unordered(_process_single_corruption, tasks),
                total=len(tasks),
                desc="   Corruption æ€»è¿›åº¦",
            )
        )

    np.save(str(output_dir / "labels.npy"), labels_np)

    get_logger().info(
        f"âœ… {DatasetClass.NAME}-C ç”Ÿæˆå®Œæˆ: {len(CORRUPTIONS)} corruptions Ã— {total_samples} samples"
    )
    return output_dir


def generate_domain_dataset(
    dataset_name: str,
    root: str = "./data",
    samples_per_group: Optional[int] = 1000,
    seed: int = 42,
    force: bool = False,
    batch_size: int = 24,
    base_model: str = "stabilityai/stable-diffusion-xl-base-1.0",
    lightning_repo: str = "ByteDance/SDXL-Lightning",
    lightning_ckpt: str = "sdxl_lightning_4step_unet.safetensors",
    styles: Optional[dict] = None,
    strengths: Optional[list] = None,
    num_steps: int = 4,
) -> Path:
    """é¢„ç”Ÿæˆ domain shift æ•°æ®é›†"""
    import multiprocessing

    if dataset_name not in DATASET_REGISTRY:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")

    DatasetClass = DATASET_REGISTRY[dataset_name]
    output_dir = Path(root) / f"{DatasetClass.NAME}-Domain"

    if _check_existing_dataset(output_dir, force):
        return output_dir

    images_np, labels_np = _load_test_set_numpy(DatasetClass, root, seed)
    images_np, labels_np = _sample_dataset(
        images_np, labels_np, samples_per_group, seed
    )

    num_gpus = torch.cuda.device_count()
    get_logger().info(
        f"ğŸ”§ ç”Ÿæˆ Domain: {DatasetClass.NAME} (SDXL Lightning 4-step, GPU={num_gpus})"
    )

    styles_list = list(styles.keys()) if styles else []

    if num_gpus == 0:
        # CPU ä¸²è¡Œæ¨¡å¼
        generator = DomainGenerator(
            device="cpu",
            base_model=base_model,
            lightning_repo=lightning_repo,
            lightning_ckpt=lightning_ckpt,
            styles=styles,
            num_steps=num_steps,
        )
        for style in styles_list:
            for str_val in strengths:
                strength_dir = output_dir / style / str(str_val)
                for c in range(DatasetClass.NUM_CLASSES):
                    ensure_dir(strength_dir / f"class_{c:04d}")
                styled = generator.apply_batch(
                    images_np, style, str_val, batch_size=batch_size
                )
                for i, (img, lbl) in enumerate(zip(styled, labels_np)):
                    Image.fromarray(img).save(
                        str(strength_dir / f"class_{lbl:04d}" / f"img_{i}.png")
                    )
    else:
        # GPU å¹¶è¡Œæ¨¡å¼
        processes = []
        for i in range(num_gpus):
            gpu_styles = styles_list[i::num_gpus]
            if not gpu_styles:
                continue
            p = multiprocessing.Process(
                target=_worker_domain,
                args=(
                    f"cuda:{i}",
                    gpu_styles,
                    strengths,
                    images_np,
                    labels_np,
                    output_dir,
                    dataset_name,
                    batch_size,
                    base_model,
                    lightning_repo,
                    lightning_ckpt,
                    styles,
                    num_steps,
                ),
            )
            p.start()
            processes.append(p)
        for p in processes:
            p.join()

    get_logger().info(f"âœ… {DatasetClass.NAME}-Domain ç”Ÿæˆå®Œæˆ!")
    return output_dir


def generate_ood_dataset(
    dataset_name: str,
    root: str = "./data",
    num_samples: int = 1000,
    seed: int = 42,
    force: bool = False,
    batch_size: int = 24,
    base_model: str = "stabilityai/stable-diffusion-xl-base-1.0",
    lightning_repo: str = "ByteDance/SDXL-Lightning",
    lightning_ckpt: str = "sdxl_lightning_4step_unet.safetensors",
    prompts: Optional[list] = None,
    num_steps: int = 4,
) -> Path:
    """é¢„ç”Ÿæˆ OOD æ•°æ®é›†"""
    if dataset_name not in DATASET_REGISTRY:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")

    DatasetClass = DATASET_REGISTRY[dataset_name]
    output_dir = Path(root) / f"{DatasetClass.NAME}-OOD"

    if _check_existing_dataset(output_dir, force):
        return output_dir

    get_logger().info(
        f"ğŸ”§ ç”Ÿæˆ OOD: {DatasetClass.NAME} ({num_samples} å¼ , SDXL Lightning 4-step)"
    )

    num_gpus = torch.cuda.device_count()
    if num_gpus <= 1:
        device = "cuda:0" if num_gpus == 1 else "cpu"
        generator = OODGenerator(
            device=device,
            base_model=base_model,
            lightning_repo=lightning_repo,
            lightning_ckpt=lightning_ckpt,
            prompts=prompts,
            num_steps=num_steps,
        )
        imgs = generator.generate_batch(
            num_samples=num_samples,
            target_size=DatasetClass.IMAGE_SIZE,
            batch_size=batch_size,
            seed=seed,
        )
        np.save(str(output_dir / "images.npy"), imgs)
    else:
        import multiprocessing

        samples_per_gpu = num_samples // num_gpus
        q = multiprocessing.Queue()
        processes = []

        for i in range(num_gpus):
            gpu_n = samples_per_gpu + (
                num_samples % num_gpus if i == num_gpus - 1 else 0
            )
            p = multiprocessing.Process(
                target=_worker_ood_gpu,
                args=(
                    i,
                    gpu_n,
                    DatasetClass.IMAGE_SIZE,
                    batch_size,
                    seed,
                    q,
                    base_model,
                    lightning_repo,
                    lightning_ckpt,
                    prompts,
                    num_steps,
                ),
            )
            p.start()
            processes.append(p)

        all_imgs = [q.get() for _ in range(num_gpus)]
        for p in processes:
            p.join()
        np.save(str(output_dir / "images.npy"), np.concatenate(all_imgs, axis=0))

    get_logger().info(f"âœ… {DatasetClass.NAME}-OOD ç”Ÿæˆå®Œæˆ!")
    return output_dir


def visualize_corruption(dataset_name: str, root: str = "./data", num_vis: int = 8):
    """ä¸º Corruption ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾"""
    DatasetClass = DATASET_REGISTRY[dataset_name]
    output_dir = Path(root) / f"{DatasetClass.NAME}-C"
    vis_dir = output_dir / "visuals"
    ensure_dir(vis_dir)

    test_dataset = DatasetClass(root=root, train=False)
    images_np = test_dataset.images.permute(0, 2, 3, 1).numpy()

    get_logger().info("ğŸ¨ æ­£åœ¨ç”Ÿæˆ Corruption å¯è§†åŒ–å¯¹æ¯”å›¾...")

    for c in ["gaussian_noise", "fog", "glass_blur"]:
        for s in [3, 5]:
            corrupted = CorruptionGenerator.apply_batch(
                images_np[:num_vis], c, s, seed=42
            )
            save_visual_comparison(
                images_np[:num_vis],
                corrupted,
                vis_dir / f"{c}_s{s}.png",
                f"{c} (severity={s})",
                num_samples=num_vis,
            )


def visualize_domain(
    dataset_name: str, root: str = "./data", num_vis: int = 8, gen_cfg=None
):
    """ä¸º Domain Shift ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾"""
    DatasetClass = DATASET_REGISTRY[dataset_name]
    output_dir = Path(root) / f"{DatasetClass.NAME}-Domain"
    vis_dir = output_dir / "visuals"
    ensure_dir(vis_dir)

    test_dataset = DatasetClass(root=root, train=False)
    images_np = test_dataset.images.permute(0, 2, 3, 1).numpy()[:num_vis]

    get_logger().info("ğŸ¨ æ­£åœ¨ç”Ÿæˆ Domain å¯è§†åŒ–å¯¹æ¯”å›¾...")

    generator = DomainGenerator(
        device="cuda" if torch.cuda.is_available() else "cpu",
        base_model=gen_cfg.base_model,
        lightning_repo=gen_cfg.lightning_repo,
        lightning_ckpt=gen_cfg.lightning_ckpt,
        styles=gen_cfg.styles,
        num_steps=gen_cfg.num_steps,
    )

    for style in list(gen_cfg.styles.keys()):
        for strength in [0.3, 0.7]:
            styled = generator.apply_batch(
                images_np, style, strength, batch_size=num_vis
            )
            save_visual_comparison(
                images_np,
                styled,
                vis_dir / f"{style}_st{strength}.png",
                f"{style} (strength={strength})",
                num_samples=num_vis,
            )


# =============================================================================
# CLI å…¥å£
# =============================================================================


def main():
    """CLI å…¥å£"""
    args = _parse_args()
    config = _load_config()
    _execute_generation(args, config)

    if config.generation.visualize:
        _execute_visualization(args, config)


def _parse_args():
    parser = argparse.ArgumentParser(
        description="é¡¹ç›®é²æ£’æ€§æ•°æ®ç”Ÿæˆå™¨ (SDXL Lightning)"
    )
    parser.add_argument(
        "--type", type=str, required=True, choices=["corruption", "domain", "ood"]
    )
    parser.add_argument(
        "--dataset", type=str, required=True, choices=list(DATASET_REGISTRY.keys())
    )
    parser.add_argument("--force", action="store_true", help="å¿½ç•¥ç¼“å­˜å¼ºåˆ¶ç”Ÿæˆ")
    return parser.parse_args()


def _load_config():
    cfg_path = Path(__file__).parents[2] / "config" / "default.yaml"
    config, _, _ = Config.load_yaml(str(cfg_path))
    return config


def _execute_generation(args, config):
    gen_cfg = config.generation
    if args.type == "corruption":
        generate_corruption_dataset(
            args.dataset, config.data_root, config.seed, args.force
        )
    elif args.type == "domain":
        generate_domain_dataset(
            args.dataset,
            config.data_root,
            gen_cfg.samples_per_group,
            config.seed,
            args.force,
            gen_cfg.batch_size,
            gen_cfg.base_model,
            gen_cfg.lightning_repo,
            gen_cfg.lightning_ckpt,
            gen_cfg.styles,
            gen_cfg.strengths,
            gen_cfg.num_steps,
        )
    elif args.type == "ood":
        generate_ood_dataset(
            args.dataset,
            config.data_root,
            gen_cfg.samples_per_group * 2,
            config.seed,
            args.force,
            gen_cfg.batch_size,
            gen_cfg.base_model,
            gen_cfg.lightning_repo,
            gen_cfg.lightning_ckpt,
            gen_cfg.ood_prompts,
            gen_cfg.num_steps,
        )


def _execute_visualization(args, config):
    if args.type == "corruption":
        visualize_corruption(args.dataset, config.data_root, config.generation.num_vis)
    elif args.type == "domain":
        visualize_domain(
            args.dataset, config.data_root, config.generation.num_vis, config.generation
        )
    elif args.type == "ood":
        visualize_ood(args.dataset, config.data_root, config.generation.num_vis)


def save_visual_grid(
    images: np.ndarray,
    output_path: Path,
    title: str,
    num_samples: int = 8,
    nrow: int = 4,
):
    """ä¿å­˜å•ç»„å›¾åƒçš„ç½‘æ ¼"""
    n = min(len(images), num_samples)
    if n == 0:
        return

    # Randomly select n images if we have more than n
    if len(images) > n:
        indices = np.linspace(0, len(images) - 1, n, dtype=int)
        imgs = images[indices]
    else:
        imgs = images

    h, w = imgs.shape[1:3]
    ncols = (n + nrow - 1) // nrow

    grid = Image.new("RGB", (w * nrow, h * ncols))

    for i, img in enumerate(imgs):
        r = i // nrow
        c = i % nrow
        grid.paste(Image.fromarray(img.astype(np.uint8)), (c * w, r * h))

    ensure_dir(output_path.parent)
    grid.save(str(output_path))
    get_logger().info(f"ğŸ“Š å¯è§†åŒ–ä¿å­˜: {output_path}")


def visualize_ood(dataset_name: str, root: str = "./data", num_vis: int = 8):
    """ä¸º OOD ç”Ÿæˆå¯è§†åŒ–ç½‘æ ¼"""
    DatasetClass = DATASET_REGISTRY[dataset_name]
    output_dir = Path(root) / f"{DatasetClass.NAME}-OOD"
    vis_dir = output_dir / "visuals"
    ensure_dir(vis_dir)

    images_path = output_dir / "images.npy"
    if not images_path.exists():
        get_logger().warning(f"âš ï¸ OOD æ•°æ®æœªæ‰¾åˆ°: {images_path}")
        return

    get_logger().info("ğŸ¨ æ­£åœ¨ç”Ÿæˆ OOD å¯è§†åŒ–...")

    # Load images (using mmap to avoid loading everything if large)
    images = np.load(str(images_path), mmap_mode="r")

    # Take a subset for visualization
    total_images = len(images)
    indices = np.linspace(0, total_images - 1, min(total_images, num_vis), dtype=int)
    vis_images = images[indices]

    save_visual_grid(
        vis_images,
        vis_dir / "ood_samples.png",
        "OOD Samples",
        num_samples=num_vis,
        nrow=4,
    )


if __name__ == "__main__":
    main()
