"""
================================================================================
è¯„åˆ†ç³»ç»Ÿæ¨¡å—
================================================================================

åŒ…å«: ScoreCalculator - å¤šç»´åº¦è¯„åˆ†è®¡ç®—å™¨
"""

import math
from typing import Any, Dict, List, Tuple

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ç»´åº¦é…ç½®                                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# å„ç»´åº¦æƒé‡ (æ€»å’Œ = 1.0)
DIMENSION_WEIGHTS = {
    "accuracy": 0.20,  # å‡†ç¡®åº¦
    "calibration": 0.10,  # æ ¡å‡†æ€§
    "diversity": 0.15,  # å¤šæ ·æ€§ (å« distance_matrix)
    "fairness": 0.10,  # å…¬å¹³æ€§
    "corruption": 0.15,  # Corruption é²æ£’æ€§
    "ood": 0.10,  # OOD æ£€æµ‹
    "adversarial": 0.10,  # å¯¹æŠ—é²æ£’æ€§
    "interpretability": 0.10,  # å¯è§£é‡Šæ€§ (GradCAM)
}

# è¯„åˆ†ç­‰çº§é˜ˆå€¼
GRADE_THRESHOLDS = [
    (90, "S"),
    (80, "A"),
    (70, "B"),
    (60, "C"),
    (0, "D"),
]

# å„ç»´åº¦çš„æŒ‡æ ‡é…ç½®: (æŒ‡æ ‡å, æ˜¯å¦è¶Šé«˜è¶Šå¥½, å‚è€ƒèŒƒå›´ [min, max])
DIMENSION_METRICS = {
    "accuracy": [
        ("ensemble_acc", True, (0, 100)),
        ("oracle_acc", True, (0, 100)),
        ("balanced_acc", True, (0, 100)),
    ],
    "calibration": [
        ("ece", False, (0, 0.5)),  # ECE è¶Šä½è¶Šå¥½, å…¸å‹èŒƒå›´ 0-0.5
        ("nll", False, (0, 5)),  # NLL è¶Šä½è¶Šå¥½, å…¸å‹èŒƒå›´ 0-5
    ],
    "diversity": [
        ("disagreement", True, (0, 50)),  # åˆ†æ­§åº¦ï¼Œé«˜æ›´å¥½
        ("cka_diversity", True, (0, 1)),  # CKA å¤šæ ·æ€§ï¼Œé«˜æ›´å¥½
        ("avg_distance", True, (0, 500)),  # å¹³å‡æ¨¡å‹è·ç¦»ï¼Œé«˜æ›´å¥½
        ("std_distance", True, (0, 100)),  # è·ç¦»æ ‡å‡†å·®ï¼Œé«˜=æœ‰ç¦»ç¾¤æ¨¡å‹
        ("direction_diversity", True, (0, 1)),  # æ–¹å‘å¤šæ ·æ€§ï¼Œé«˜=æ¢ç´¢æ–¹å‘åˆ†æ•£
    ],
    "fairness": [
        ("fairness_score", True, (0, 100)),
        ("acc_gini_coef", False, (0, 1)),  # è¶Šä½è¶Šå…¬å¹³
        ("bottom_3_class_acc", True, (0, 100)),
        ("bottom_5_class_acc", True, (0, 100)),
    ],
    "corruption": [
        ("corruption_overall", True, (0, 100)),
        ("corruption_sev_1", True, (0, 100)),
        ("corruption_sev_3", True, (0, 100)),
        ("corruption_sev_5", True, (0, 100)),
    ],
    "ood": [
        ("ood_auroc_msp", True, (50, 100)),
        ("ood_auroc_entropy", True, (50, 100)),
        ("ood_fpr95_msp", False, (0, 100)),  # è¶Šä½è¶Šå¥½
        ("ood_fpr95_entropy", False, (0, 100)),
    ],
    "adversarial": [
        ("clean_acc", True, (0, 100)),
        ("fgsm_acc", True, (0, 100)),
        ("pgd_acc", True, (0, 100)),
    ],
    "interpretability": [
        ("avg_cam_entropy", True, (0, 10)),  # ç†µè¶Šé«˜ï¼Œå…³æ³¨è¶Šåˆ†æ•£ï¼Œå¯èƒ½æ›´åˆç†
        ("avg_cam_similarity", False, (0, 1)),  # è¶Šä½è¶Šå¤šæ ·
        ("avg_cam_overlap", True, (0, 1)),  # é‡å åº¦ï¼Œé€‚ä¸­å³å¯
    ],
}

# ç»´åº¦æ˜¾ç¤ºé…ç½®
DIMENSION_DISPLAY = {
    "accuracy": ("ğŸ¯", "å‡†ç¡®åº¦", "Accuracy"),
    "calibration": ("ğŸ“Š", "æ ¡å‡†æ€§", "Calibration"),
    "diversity": ("ğŸ”€", "å¤šæ ·æ€§", "Diversity"),
    "fairness": ("âš–ï¸", "å…¬å¹³æ€§", "Fairness"),
    "corruption": ("ğŸŒªï¸", "Corruption", "Corruption"),
    "ood": ("ğŸ”®", "OODæ£€æµ‹", "OOD Detection"),
    "adversarial": ("âš”ï¸", "å¯¹æŠ—é²æ£’", "Adversarial"),
    "interpretability": ("ğŸ”", "å¯è§£é‡Šæ€§", "Interpretability"),
}


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è¯„åˆ†è®¡ç®—å™¨                                                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ScoreCalculator:
    """å¤šç»´åº¦è¯„åˆ†è®¡ç®—å™¨"""

    @staticmethod
    def _normalize_score(
        value: float, higher_is_better: bool, ref_range: Tuple[float, float]
    ) -> float:
        """å°†æŒ‡æ ‡å€¼å½’ä¸€åŒ–åˆ° 0-100 åˆ†"""
        min_val, max_val = ref_range
        if max_val == min_val:
            return 50.0

        # çº¿æ€§å½’ä¸€åŒ–
        if higher_is_better:
            score = (value - min_val) / (max_val - min_val) * 100
        else:
            score = (max_val - value) / (max_val - min_val) * 100

        return max(0, min(100, score))

    @staticmethod
    def _extract_flat_metrics(result: Dict[str, Any]) -> Dict[str, float]:
        """å°†åµŒå¥—çš„ç»“æœå­—å…¸æ‰å¹³åŒ–ä¸ºå•å±‚å­—å…¸"""
        flat = {}

        # Standard metrics
        std = result.get("standard_metrics", {})
        for k, v in std.items():
            if isinstance(v, (int, float)):
                flat[k] = float(v)

        # Corruption results
        corr = result.get("corruption_results") or {}
        if corr:
            flat["corruption_overall"] = corr.get("overall_avg", 0)
            by_sev = corr.get("by_severity", {})
            for sev, val in by_sev.items():
                flat[f"corruption_sev_{sev}"] = val

        # OOD results
        ood = result.get("ood_results") or {}
        for k, v in ood.items():
            flat[k] = float(v) if isinstance(v, (int, float)) else 0

        # Adversarial results
        adv = result.get("adversarial_results") or {}
        for k, v in adv.items():
            if isinstance(v, (int, float)):
                flat[k] = float(v)

        # GradCAM metrics
        cam = result.get("gradcam_metrics") or {}
        for k, v in cam.items():
            if isinstance(v, (int, float)):
                flat[k] = float(v)

        # Distance matrix -> avg_distance, std_distance, direction_diversity
        dist_matrix = result.get("distance_matrix")
        # æ”¯æŒ list å’Œ numpy.ndarrayï¼Œéœ€ç”¨ is not None é¿å… numpy æ•°ç»„å¸ƒå°”åˆ¤æ–­æ­§ä¹‰
        if dist_matrix is not None and hasattr(dist_matrix, "__len__"):
            n = len(dist_matrix)
            if n > 1:
                # æå–ä¸Šä¸‰è§’è·ç¦»
                distances = [
                    dist_matrix[i][j] for i in range(n) for j in range(i + 1, n)
                ]
                count = len(distances)

                # å¹³å‡è·ç¦»
                avg_dist = sum(distances) / count if count > 0 else 0
                flat["avg_distance"] = avg_dist

                # è·ç¦»æ ‡å‡†å·® (è¯†åˆ«ç¦»ç¾¤æ¨¡å‹)
                if count > 1:
                    variance = sum((d - avg_dist) ** 2 for d in distances) / count
                    flat["std_distance"] = math.sqrt(variance)
                else:
                    flat["std_distance"] = 0

                # æ–¹å‘å¤šæ ·æ€§: ä½¿ç”¨å˜å¼‚ç³»æ•° (CV) å½’ä¸€åŒ–
                # CV = std / meanï¼Œå€¼è¶Šé«˜è¡¨ç¤ºè·ç¦»åˆ†å¸ƒè¶Šåˆ†æ•£
                # è½¬æ¢ä¸º 0-1 èŒƒå›´: tanh(CV) æˆ– min(CV, 1)
                if avg_dist > 0:
                    cv = flat["std_distance"] / avg_dist
                    flat["direction_diversity"] = min(cv, 1.0)
                else:
                    flat["direction_diversity"] = 0

        return flat

    @classmethod
    def calculate_dimension_score(
        cls, flat_metrics: Dict[str, float], dimension: str
    ) -> Tuple[float, Dict[str, float]]:
        """è®¡ç®—å•ä¸ªç»´åº¦çš„ç»¼åˆå¾—åˆ†

        Returns:
            (dimension_score, {metric_name: individual_score})
        """
        metrics_config = DIMENSION_METRICS.get(dimension, [])
        if not metrics_config:
            return 0.0, {}

        scores = {}
        valid_scores = []

        for metric_name, higher_is_better, ref_range in metrics_config:
            value = flat_metrics.get(metric_name)
            if value is not None:
                score = cls._normalize_score(value, higher_is_better, ref_range)
                scores[metric_name] = score
                valid_scores.append(score)

        dim_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
        return dim_score, scores

    @classmethod
    def calculate_all_scores(cls, result: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ‰€æœ‰ç»´åº¦çš„åˆ†æ•°

        Returns:
            {
                "total_score": float,
                "grade": str,
                "dimensions": {
                    "accuracy": {"score": float, "metrics": {...}},
                    ...
                }
            }
        """
        flat = cls._extract_flat_metrics(result)
        dimensions = {}
        weighted_sum = 0.0
        total_weight = 0.0

        for dim_name, weight in DIMENSION_WEIGHTS.items():
            dim_score, metric_scores = cls.calculate_dimension_score(flat, dim_name)

            # åªæœ‰æœ‰æœ‰æ•ˆåˆ†æ•°çš„ç»´åº¦æ‰è®¡å…¥
            if metric_scores:
                dimensions[dim_name] = {
                    "score": dim_score,
                    "weight": weight,
                    "metrics": metric_scores,
                }
                weighted_sum += dim_score * weight
                total_weight += weight

        total_score = weighted_sum / total_weight if total_weight > 0 else 0.0
        grade = cls.get_grade(total_score)

        return {
            "total_score": total_score,
            "grade": grade,
            "dimensions": dimensions,
        }

    @staticmethod
    def get_grade(score: float) -> str:
        """æ ¹æ®åˆ†æ•°è¿”å›ç­‰çº§"""
        for threshold, grade in GRADE_THRESHOLDS:
            if score >= threshold:
                return grade
        return "D"

    @staticmethod
    def get_medal(rank: int, total: int) -> str:
        """è¿”å›é‡‘é“¶é“œç‰Œæˆ–æ’å"""
        if total < 2:
            return ""
        medals = {0: "ğŸ¥‡", 1: "ğŸ¥ˆ", 2: "ğŸ¥‰"}
        return medals.get(rank, f"#{rank + 1}")

    @classmethod
    def rank_experiments(
        cls, results: Dict[str, Dict], key: str = "total_score"
    ) -> List[Tuple[str, float, str]]:
        """å¯¹å®éªŒæŒ‰æŒ‡å®šæŒ‡æ ‡æ’å

        Returns:
            [(exp_name, score, medal), ...]
        """
        # è®¡ç®—æ‰€æœ‰å®éªŒçš„åˆ†æ•°
        exp_scores = []
        for name, result in results.items():
            score_data = cls.calculate_all_scores(result)
            exp_scores.append((name, score_data[key]))

        # æ’åº
        exp_scores.sort(key=lambda x: x[1], reverse=True)
        total = len(exp_scores)

        # æ·»åŠ å¥–ç‰Œ
        return [
            (name, score, cls.get_medal(i, total))
            for i, (name, score) in enumerate(exp_scores)
        ]
