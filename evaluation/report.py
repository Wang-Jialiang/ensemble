"""
================================================================================
æŠ¥å‘Šç”Ÿæˆå™¨æ¨¡å—
================================================================================

åŒ…å«: ReportGenerator - å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆ
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from ..datasets.robustness.corruption import CorruptionDataset
    from ..datasets.robustness.domain import DomainShiftDataset
    from ..datasets.robustness.ood import OODDataset

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from ..config import Config
from ..utils import ensure_dir, format_duration, get_logger
from .adversarial import evaluate_adversarial
from .checkpoint import CheckpointLoader
from .corruption_robustness import evaluate_corruption
from .domain_robustness import evaluate_domain_shift
from .gradcam import GradCAMAnalyzer, ModelListWrapper
from .inference import get_all_models_logits, get_models_from_source
from .landscape import LossLandscapeVisualizer
from .metrics import MetricsCalculator
from .ood import evaluate_ood
from .saver import ResultsSaver
from .strategies import get_ensemble_fn
from .visualizer import ReportVisualizer

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŠ¥å‘Šç”Ÿæˆå™¨ (è¯„ä¼° + æŠ¥å‘Š)                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ReportGenerator:
    """å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆå™¨

    ä¸¤ç§ä¸»è¦ä½¿ç”¨æ–¹å¼:
        1. ä»å†…å­˜è¯„ä¼° (è®­ç»ƒåç«‹å³è¯„ä¼°):
           ReportGenerator.evaluate_trainers(trainers=[...], ...)

        2. ä»ç£ç›˜è¯„ä¼° (åŠ è½½ checkpoint):
           ReportGenerator.evaluate_checkpoints(checkpoint_paths=[...], ...)
    """

    @staticmethod
    def _get_rank_marker(
        value: float, all_values: List[float], higher_is_better: bool
    ) -> str:
        """è·å–æ’åæ ‡è®° (ä»…å¤šå®éªŒæ—¶æ˜¾ç¤º)"""
        if len(all_values) <= 1:
            return ""
        sorted_values = sorted(all_values, reverse=higher_is_better)
        if value == sorted_values[0]:
            return " ğŸ¥‡"
        elif value == sorted_values[1]:
            return " ğŸ¥ˆ"
        return ""

    @staticmethod
    def _evaluate_models(
        models: List[nn.Module],
        exp_name: str,
        test_loader: DataLoader,
        cfg: Config,
        device: torch.device,
        training_time: float = 0.0,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        ood_dataset: Optional["OODDataset"] = None,
        domain_dataset: Optional["DomainShiftDataset"] = None,
        run_gradcam: bool = False,
        run_adversarial: bool = True,
    ) -> Dict[str, Any]:
        """é€šç”¨æ¨¡å‹è¯„ä¼°æ–¹æ³• - æ ¸å¿ƒè¯„ä¼°é€»è¾‘"""
        get_logger().info(f"\nğŸ“Š Evaluating: {exp_name}")
        ensemble_fn = get_ensemble_fn(cfg)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # æ ‡å‡†è¯„ä¼° (Standard Metrics)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        get_logger().info("   ğŸ” Standard evaluation...")
        all_logits, all_targets = get_all_models_logits(models, test_loader, device)
        metrics_calc = MetricsCalculator(cfg.num_classes, cfg.ece_n_bins)
        standard_metrics = metrics_calc.calculate_all_metrics(
            all_logits, all_targets, ensemble_fn=ensemble_fn
        )

        get_logger().info(f"   Ensemble Acc:   {standard_metrics['ensemble_acc']:.2f}%")
        get_logger().info(f"   ECE:            {standard_metrics['ece']:.4f}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # é²æ£’æ€§è¯„ä¼° (Robustness Evaluation)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        corruption_results = None
        if corruption_dataset is not None:
            get_logger().info("   ğŸ” Corruption evaluation...")
            corruption_results = evaluate_corruption(
                models, corruption_dataset, config=cfg
            )

        # OOD è¯„ä¼°
        ood_results = None
        if ood_dataset is not None:
            get_logger().info("   ğŸ” OOD detection evaluation...")
            ood_loader = ood_dataset.get_loader(config=cfg)
            ood_results = evaluate_ood(
                models, test_loader, ood_loader, ood_name=ood_dataset.name
            )

        # Domain Shift è¯„ä¼° (éå†æ‰€æœ‰ style Ã— strength ç»„åˆ)
        domain_results = None
        if domain_dataset is not None:
            get_logger().info("   ğŸ” Domain shift evaluation...")
            domain_results = {"by_style_strength": {}, "overall_avg": 0.0}
            all_accs = []

            for style in domain_dataset.STYLES:
                for strength in domain_dataset.STRENGTHS:
                    try:
                        domain_loader = domain_dataset.get_loader(style, strength, cfg)
                        result = evaluate_domain_shift(
                            models,
                            domain_loader,
                            domain_name=f"{style}_{strength}",
                            num_classes=cfg.num_classes,
                        )
                        key = f"{style}_{strength}"
                        domain_results["by_style_strength"][key] = result
                        all_accs.append(result["domain_acc"])
                    except FileNotFoundError:
                        get_logger().warning(
                            f"      è·³è¿‡æœªç”Ÿæˆçš„ç»„åˆ: {style}/{strength}"
                        )

            if all_accs:
                domain_results["overall_avg"] = sum(all_accs) / len(all_accs)
                get_logger().info(
                    f"   âœ… Domain Overall Avg: {domain_results['overall_avg']:.2f}%"
                )

        # å¯¹æŠ—é²æ£’æ€§è¯„ä¼°
        adversarial_results = None
        if run_adversarial:
            get_logger().info("   ğŸ” Adversarial evaluation...")
            adversarial_results = evaluate_adversarial(
                models,
                test_loader,
                eps=cfg.adv_eps,
                alpha=cfg.adv_alpha,
                pgd_steps=cfg.adv_pgd_steps,
                dataset_name=cfg.dataset_name,
            )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # å¯è§£é‡Šæ€§åˆ†æ (Grad-CAM)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        gradcam_metrics = None
        if run_gradcam:
            get_logger().info("   ğŸ” Grad-CAM analysis...")
            workers = [ModelListWrapper(models)]
            gradcam_analyzer = GradCAMAnalyzer(cfg)
            gradcam_metrics = gradcam_analyzer.analyze_ensemble_quality(
                workers, test_loader, num_samples=50, image_size=cfg.image_size
            )

        return {
            "experiment_name": exp_name,
            "training_time_seconds": training_time,
            "standard_metrics": standard_metrics,
            "corruption_results": corruption_results,
            "ood_results": ood_results,
            "domain_results": domain_results,
            "adversarial_results": adversarial_results,
            "gradcam_metrics": gradcam_metrics,
        }

    @staticmethod
    def _evaluate_trainer(
        trainer: Any,
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        run_gradcam: bool = False,
        run_adversarial: bool = True,
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ª trainer å¹¶è¿”å›ç»“æœå­—å…¸"""
        models, device = get_models_from_source(trainer)
        return ReportGenerator._evaluate_models(
            models=models,
            exp_name=trainer.name,
            test_loader=test_loader,
            cfg=cfg,
            device=device,
            training_time=getattr(trainer, "total_training_time", 0.0),
            corruption_dataset=corruption_dataset,
            run_gradcam=run_gradcam,
            run_adversarial=run_adversarial,
        )

    @classmethod
    def _generate_report(cls, results: Dict[str, Any]) -> str:
        """ç”ŸæˆæŠ¥å‘Šå­—ç¬¦ä¸²"""
        lines = []

        def log(s=""):
            lines.append(str(s))

        exp_names = list(results.keys())
        is_single = len(exp_names) == 1

        # æ ‡é¢˜
        log("=" * 115)
        if is_single:
            log(f"ğŸ“Š EXPERIMENT RESULTS: {exp_names[0]}")
        else:
            log("ğŸ“Š EXPERIMENTAL RESULTS COMPARISON")
            log(
                "   ğŸ¥‡ = Best, ğŸ¥ˆ = Second Best | â†‘ = Higher is better, â†“ = Lower is better"
            )
        log("=" * 115)

        # è¡¨æ ¼
        log("\nğŸ¯ Performance Metrics")
        log("-" * 115)
        log(
            f"{'Experiment':<25} | {'EnsAccâ†‘':<10} | {'AvgIndâ†‘':<10} | {'Oracleâ†‘':<10} | {'ECEâ†“':<10} | {'NLLâ†“':<10} | {'Time':<12}"
        )
        log("-" * 115)

        acc_vals = [
            results[n].get("standard_metrics", {}).get("ensemble_acc", 0)
            for n in exp_names
        ]

        for name in exp_names:
            m = results[name].get("standard_metrics", {})
            t = format_duration(
                results[name].get(
                    "training_time_seconds", results[name].get("training_time", 0)
                )
            )
            acc = m.get("ensemble_acc", 0)
            mark = cls._get_rank_marker(acc, acc_vals, True)
            log(
                f"{name:<25} | {acc:<7.2f}{mark:<3} | {m.get('avg_individual_acc', 0):<10.2f} | "
                f"{m.get('oracle_acc', 0):<10.2f} | {m.get('ece', 0):<10.4f} | {m.get('nll', 0):<10.4f} | {t:<12}"
            )
        log("-" * 115)

        # è¯¦ç»†æŒ‡æ ‡ (æ¯ä¸ªå®éªŒä¾æ¬¡å±•ç¤º)
        log("\nğŸ“‹ Detailed Metrics")
        log("=" * 115)

        for name in exp_names:
            m = results[name].get("standard_metrics", {})
            log(f"\nğŸ”¹ {name}")
            log("-" * 40)

            # Diversity
            log("   ğŸ”€ Diversity & Confidence")
            log(
                f"      Disagreement: {m.get('disagreement', 0):.2f}%  |  JSæ•£åº¦: {m.get('js_divergence', 0):.4f}  |  Spearman: {m.get('spearman_correlation', 1.0):.4f}"
            )
            log(
                f"      Confidence: avg={m.get('avg_confidence', 0):.4f}, correct={m.get('avg_correct_confidence', 0):.4f}, incorrect={m.get('avg_incorrect_confidence', 0):.4f}"
            )

            # Fairness
            log("\n   âš–ï¸ Fairness")
            log(
                f"      Balanced Acc: {m.get('balanced_acc', 0):.2f}%  |  Disparity: {m.get('acc_disparity', 0):.2f}%  |  Score: {m.get('fairness_score', 0):.2f}"
            )
            log("-" * 40)

        # Corruption
        has_corruption = any(results[n].get("corruption_results") for n in exp_names)
        if has_corruption:
            log("\nğŸ§ª Corruption Robustness")
            log("-" * 60)
            overall_vals = [
                results[n].get("corruption_results", {}).get("overall_avg", 0)
                for n in exp_names
                if results[n].get("corruption_results")
            ]
            for name in exp_names:
                c = results[name].get("corruption_results", {})
                if c and "overall_avg" in c:
                    val = c["overall_avg"]
                    mark = cls._get_rank_marker(val, overall_vals, True)
                    log(f"   {name:<25} | Overall: {val:.2f}%{mark}")
            log("-" * 60)

        # Adversarial Robustness
        has_adversarial = any(results[n].get("adversarial_results") for n in exp_names)
        if has_adversarial:
            log("\nğŸ—¡ï¸ Adversarial Robustness")
            log("-" * 80)
            log(
                f"   {'Experiment':<25} | {'Cleanâ†‘':<10} | {'FGSMâ†‘':<10} | {'PGDâ†‘':<10} | {'Îµ':<10}"
            )
            log("-" * 80)
            pgd_vals = [
                results[n].get("adversarial_results", {}).get("pgd_acc", 0)
                for n in exp_names
                if results[n].get("adversarial_results")
            ]
            for name in exp_names:
                adv = results[name].get("adversarial_results", {})
                if adv:
                    clean = adv.get("clean_acc", 0)
                    fgsm = adv.get("fgsm_acc", 0)
                    pgd = adv.get("pgd_acc", 0)
                    eps = adv.get("eps_255", 8)
                    mark = cls._get_rank_marker(pgd, pgd_vals, True)
                    log(
                        f"   {name:<25} | {clean:<10.2f} | {fgsm:<10.2f} | {pgd:<7.2f}{mark:<3} | {eps:.0f}/255"
                    )
            log("-" * 80)

        log("\n" + "=" * 115)
        return "\n".join(lines)

    @classmethod
    def _save_and_print(cls, results: Dict[str, Dict], save_dir: str):
        """ä¿å­˜å¹¶æ‰“å°æŠ¥å‘Š"""
        saver = ResultsSaver(save_dir)
        report_content = cls._generate_report(results)

        # ä¿å­˜ç»“æœ (ç»Ÿä¸€æ ¼å¼)
        saver.save_comparison(results, "comprehensive_results")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (ä¸æ‰“å°åˆ°æ§åˆ¶å°)
        report_path = Path(save_dir) / "detailed_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        get_logger().info(f"\nâœ… Detailed report saved to: {report_path}")
        get_logger().info(f"âœ… All results saved to: {save_dir}")

    @classmethod
    def evaluate_trainers(
        cls,
        trainers: List,  # List of StagedEnsembleTrainer instances
        test_loader: DataLoader,
        cfg: Config,
        save_dir: str,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        run_gradcam: bool = False,
        run_adversarial: bool = True,
    ):
        """
        ä»å†…å­˜è¯„ä¼°å¤šä¸ª trainer å¹¶ç”ŸæˆæŠ¥å‘Š

        é€‚ç”¨åœºæ™¯: è®­ç»ƒåˆšå®Œæˆï¼Œæ¨¡å‹è¿˜åœ¨å†…å­˜ä¸­
        """
        get_logger().info(
            f"\n{'=' * 80}\nğŸ“Š EVALUATION MODE | Models: {len(trainers)}\n{'=' * 80}"
        )

        # è¯„ä¼°æ‰€æœ‰ trainers
        results = {}
        for idx, trainer in enumerate(trainers, 1):
            get_logger().info(f"\n[{idx}/{len(trainers)}] {trainer.name}")
            result = cls._evaluate_trainer(
                trainer,
                test_loader,
                cfg,
                corruption_dataset,
                run_gradcam,
                run_adversarial,
            )
            results[trainer.name] = result

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        get_logger().info("\nğŸ“Š Generating visualizations...")
        visualizer = ReportVisualizer(save_dir, dpi=cfg.plot_dpi)
        visualizer.generate_all(results)

        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        cls._save_and_print(results, save_dir)

    @classmethod
    def evaluate_checkpoints(
        cls,
        checkpoint_paths: List[str],
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        ood_dataset: Optional["OODDataset"] = None,
        domain_dataset: Optional["DomainShiftDataset"] = None,
        run_gradcam: bool = False,
        run_loss_landscape: bool = False,
        run_adversarial: bool = True,
    ):
        """
        ä»ç£ç›˜åŠ è½½ checkpoint å¹¶è¯„ä¼°

        é€‚ç”¨åœºæ™¯: è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä¸è®­ç»ƒè§£è€¦
        è¿™æ˜¯ evaluation æ¨¡å—çš„ä¸»å…¥å£ï¼Œå®Œå…¨ç‹¬ç«‹äº training æ¨¡å—ã€‚
        """
        get_logger().info(f"\n{'=' * 80}")
        get_logger().info(
            f"ğŸ“Š EVALUATION FROM CHECKPOINTS | Count: {len(checkpoint_paths)}"
        )
        get_logger().info(f"{'=' * 80}")

        output_dir = cfg.save_dir
        ensure_dir(output_dir)
        results = {}
        all_models = {}  # æ”¶é›†æ‰€æœ‰å®éªŒçš„æ¨¡å‹ç”¨äº Loss Landscape
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        for idx, ckpt_path in enumerate(checkpoint_paths, 1):
            get_logger().info(f"\n[{idx}/{len(checkpoint_paths)}] Loading: {ckpt_path}")

            # åŠ è½½æ¨¡å‹
            ctx = CheckpointLoader.load(ckpt_path, cfg)
            exp_name = ctx["name"]
            models = [m.to(device) for m in ctx["models"]]
            all_models[exp_name] = models  # ä¿å­˜ç”¨äºåç»­åˆ†æ

            # ä½¿ç”¨é€šç”¨è¯„ä¼°æ–¹æ³•
            result = cls._evaluate_models(
                models=models,
                exp_name=exp_name,
                test_loader=test_loader,
                cfg=cfg,
                device=device,
                training_time=ctx["training_time"],
                corruption_dataset=corruption_dataset,
                ood_dataset=ood_dataset,
                domain_dataset=domain_dataset,
                run_gradcam=run_gradcam,
                run_adversarial=run_adversarial,
            )
            result["train_config"] = ctx["config"]
            results[exp_name] = result

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        get_logger().info("\nğŸ“Š Generating visualizations...")
        visualizer = ReportVisualizer(output_dir, dpi=cfg.plot_dpi)
        visualizer.generate_all(results)

        # Loss Landscape åˆ†æ
        if run_loss_landscape and all_models:
            get_logger().info("\nğŸ”ï¸ Generating Loss Landscape visualizations...")
            landscape_viz = LossLandscapeVisualizer(output_dir, dpi=cfg.plot_dpi)

            for exp_name, models in all_models.items():
                # æ¨¡å‹å‚æ•°è·ç¦»çƒ­åŠ›å›¾ (æ— éœ€ loss-landscapes ä¾èµ–)
                landscape_viz.plot_model_distance_heatmap(
                    models, filename=f"{exp_name}_model_distances.png"
                )

                # Loss Landscape æ’å€¼ (éœ€è¦ loss-landscapes)
                landscape_viz.plot_ensemble_interpolations(
                    models,
                    test_loader,
                    device,
                    filename=f"{exp_name}_loss_landscape.png",
                )

                # 2D/3D è¡¨é¢å›¾ - ä¸ºç¬¬ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆ (è®¡ç®—é‡è¾ƒå¤§)
                if len(models) > 0:
                    landscape_viz.plot_2d_plane(
                        models[0],
                        test_loader,
                        device,
                        distance=1.0,
                        steps=20,  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«è®¡ç®—
                        filename=f"{exp_name}_landscape_surface.png",
                        model_name=f"{exp_name}_M1",
                    )

        # ç”Ÿæˆå¹¶ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        cls._save_and_print(results, output_dir)

        get_logger().info(f"\nâœ… Complete! All reports saved to: {output_dir}")
        return results
