"""
================================================================================
æŠ¥å‘Šç”Ÿæˆå™¨æ¨¡å—
================================================================================

åŒ…å«: ReportGenerator - å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆ
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from ..datasets.robustness.corruption import CorruptionDataset
    from ..datasets.robustness.ood import OODDataset

import torch
from torch.utils.data import DataLoader

from ..config import Config
from ..utils import ensure_dir, get_logger
from .adversarial import evaluate_adversarial
from .checkpoint import CheckpointLoader
from .corruption_robustness import evaluate_corruption
from .gradcam import GradCAMAnalyzer, ModelListWrapper
from .inference import get_all_models_logits
from .landscape import ModelDistanceCalculator
from .metrics import MetricsCalculator
from .ood import evaluate_ood
from .saver import ResultsSaver
from .strategies import get_ensemble_fn

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŠ¥å‘Šç”Ÿæˆå™¨ (è¯„ä¼° + æŠ¥å‘Š)                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ReportGenerator:
    """å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆå™¨

    ä¸»å…¥å£:
        ReportGenerator.evaluate_checkpoints(checkpoint_paths=[...], ...)
    """

    @staticmethod
    def _evaluate_models(
        models, exp_name, test_loader, cfg, device, **datasets
    ) -> Dict[str, Any]:
        """é€šç”¨æ¨¡å‹è¯„ä¼°æ–¹æ³• - ç”Ÿå‘½å‘¨æœŸé’©å­æ¨¡å¼"""
        log = get_logger()
        log.info(f"\nâ”Œ{'â”€' * 60}")
        log.info(f"â”‚ ğŸ“Š {exp_name}")
        log.info(f"â””{'â”€' * 60}")
        res = {"experiment_name": exp_name}

        # 1. æ ‡å‡†æ ‡å‡†æŒ‡æ ‡ (Acc, ECE, NLL)
        res["standard_metrics"] = ReportGenerator._run_standard_eval(
            models, test_loader, cfg, device
        )

        # 2. é²æ£’æ€§å¥—ä»¶ (Corruption, OOD, Domain)
        res.update(
            ReportGenerator._run_robustness_eval(models, cfg, test_loader, **datasets)
        )

        # 3. å¯¹æŠ—æ€§ä¸å¯è§£é‡Šæ€§åˆ†æ
        res.update(
            ReportGenerator._run_analysis_eval(models, cfg, test_loader, **datasets)
        )

        return res

    @staticmethod
    def _run_standard_eval(models, loader, cfg, device):
        get_logger().info("  â”œâ”€ ğŸ” Standard metrics")
        all_l, all_t = get_all_models_logits(models, loader, device)
        return MetricsCalculator(cfg.num_classes, cfg.ece_n_bins).calculate_all_metrics(
            all_l, all_t, get_ensemble_fn(cfg)
        )

    @staticmethod
    def _run_robustness_eval(models, cfg, loader, **ds):
        r = {"corruption_results": None, "ood_results": None}

        if ds.get("corruption_dataset"):
            get_logger().info("  â”œâ”€ ğŸŒªï¸ Corruption robustness")
            r["corruption_results"] = evaluate_corruption(
                models, ds["corruption_dataset"], cfg
            )

        if ds.get("ood_dataset"):
            get_logger().info("  â”œâ”€ ğŸ”® OOD detection")
            r["ood_results"] = evaluate_ood(
                models,
                loader,
                ds["ood_dataset"].get_loader(cfg),
                ds["ood_dataset"].name,
            )

        return r

    @staticmethod
    def _run_analysis_eval(models, cfg, loader, **ds):
        a = {"adversarial_results": None, "gradcam_metrics": None}
        if ds.get("run_adversarial", True):
            get_logger().info("  â”œâ”€ âš”ï¸ Adversarial robustness")
            a["adversarial_results"] = evaluate_adversarial(
                models, loader, cfg=cfg, logger=get_logger()
            )

        if ds.get("run_gradcam", False):
            get_logger().info("  â””â”€ ğŸ” Grad-CAM analysis")
            a["gradcam_metrics"] = GradCAMAnalyzer(cfg).analyze_ensemble_quality(
                [ModelListWrapper(models)],
                loader,
                cfg.gradcam_num_samples,
                cfg.image_size,
            )
        return a

    @staticmethod
    def _get_rank_marker(
        value: float, all_values: List[float], higher_is_better: bool
    ) -> str:
        """è·å–æ’åæ ‡è®° ğŸ¥‡ğŸ¥ˆğŸ¥‰"""
        if len(all_values) <= 1:
            return ""
        sorted_values = sorted(all_values, reverse=higher_is_better)
        if value == sorted_values[0]:
            return "ğŸ¥‡"
        elif len(sorted_values) > 1 and value == sorted_values[1]:
            return "ğŸ¥ˆ"
        elif len(sorted_values) > 2 and value == sorted_values[2]:
            return "ğŸ¥‰"
        return ""

    @classmethod
    def _format_val(
        cls,
        value: float,
        all_vals: List[float],
        higher_is_better: bool,
        fmt: str = ".4f",
    ) -> str:
        """æ ¼å¼åŒ–æ•°å€¼å¹¶æ·»åŠ æ’åæ ‡è®°"""
        mark = cls._get_rank_marker(value, all_vals, higher_is_better)
        return f"{value:{fmt}}{mark}"

    @classmethod
    def _generate_report(cls, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š - æŒ‰9ä¸ªç»´åº¦ç»„ç»‡ï¼Œå¸¦æ’åæ ‡è®°å’Œç®­å¤´æŒ‡ç¤º"""
        lines = []
        exps = list(results.keys())

        # è¾…åŠ©å‡½æ•°ï¼šè·å–æŒ‡æ ‡å€¼åˆ—è¡¨
        def get_std_vals(key):
            return [results[n].get("standard_metrics", {}).get(key, 0) for n in exps]

        def get_corr_vals(key):
            return [
                results[n].get("corruption_results", {}).get(key, 0)
                if results[n].get("corruption_results")
                else 0
                for n in exps
            ]

        def get_ood_vals(key):
            return [
                results[n].get("ood_results", {}).get(key, 0)
                if results[n].get("ood_results")
                else 0
                for n in exps
            ]

        def get_adv_vals(key):
            return [
                results[n].get("adversarial_results", {}).get(key, 0)
                if results[n].get("adversarial_results")
                else 0
                for n in exps
            ]

        def get_cam_vals(key):
            return [
                results[n].get("gradcam_metrics", {}).get(key, 0)
                if results[n].get("gradcam_metrics")
                else 0
                for n in exps
            ]

        def get_dist_vals(key):
            return [results[n].get(key, 0) for n in exps]

        lines.append("=" * 120)
        lines.append("ğŸ“‹ ENSEMBLE EVALUATION REPORT")
        lines.append("=" * 120)

        # 1. å‡†ç¡®ç‡ (â†‘ é«˜å¥½)
        lines.append("\nğŸ¯ å‡†ç¡®ç‡ (Accuracy)")
        lines.append("-" * 100)
        lines.append(
            f"{'Experiment':<25} | {'ensemble_accâ†‘':<16} | {'avg_ind_accâ†‘':<16} | {'oracle_accâ†‘':<16}"
        )
        lines.append("-" * 100)
        ens_vals = get_std_vals("ensemble_acc")
        avg_vals = get_std_vals("avg_individual_acc")
        ora_vals = get_std_vals("oracle_acc")
        for n in exps:
            m = results[n].get("standard_metrics", {})
            lines.append(
                f"{n:<25} | {cls._format_val(m.get('ensemble_acc', 0), ens_vals, True):<16} | "
                f"{cls._format_val(m.get('avg_individual_acc', 0), avg_vals, True):<16} | "
                f"{cls._format_val(m.get('oracle_acc', 0), ora_vals, True):<16}"
            )

        # 2. æ ¡å‡†æ€§ (â†“ ä½å¥½)
        lines.append("\nğŸ“ æ ¡å‡†æ€§ (Calibration)")
        lines.append("-" * 70)
        lines.append(f"{'Experiment':<25} | {'eceâ†“':<16} | {'nllâ†“':<16}")
        lines.append("-" * 70)
        ece_vals = get_std_vals("ece")
        nll_vals = get_std_vals("nll")
        for n in exps:
            m = results[n].get("standard_metrics", {})
            lines.append(
                f"{n:<25} | {cls._format_val(m.get('ece', 0), ece_vals, False, '.6f'):<16} | "
                f"{cls._format_val(m.get('nll', 0), nll_vals, False, '.6f'):<16}"
            )

        # 3. å¤šæ ·æ€§ (disagreementâ†‘é«˜å¥½, js_divâ†‘é«˜å¥½, avg_ckaâ†“ä½å¥½è¡¨ç¤ºæ›´å¤šæ ·)
        lines.append("\nğŸ”€ å¤šæ ·æ€§ (Diversity)")
        lines.append("-" * 90)
        lines.append(
            f"{'Experiment':<25} | {'disagreementâ†‘':<16} | {'js_divergenceâ†‘':<16} | {'avg_ckaâ†“':<16}"
        )
        lines.append("-" * 90)
        dis_vals = get_std_vals("disagreement")
        js_vals = get_std_vals("js_divergence")
        cka_vals = get_std_vals("avg_cka")
        for n in exps:
            m = results[n].get("standard_metrics", {})
            lines.append(
                f"{n:<25} | {cls._format_val(m.get('disagreement', 0), dis_vals, True):<16} | "
                f"{cls._format_val(m.get('js_divergence', 0), js_vals, True):<16} | "
                f"{cls._format_val(m.get('avg_cka', 0), cka_vals, False):<16}"
            )

        # 4. å…¬å¹³æ€§
        lines.append("\nâš–ï¸ å…¬å¹³æ€§ (Fairness)")
        lines.append("-" * 140)
        lines.append(
            f"{'Experiment':<25} | {'balanced_accâ†‘':<14} | {'gini_coefâ†“':<14} | "
            f"{'fair_scoreâ†‘':<14} | {'eodâ†“':<12} | {'bottom_3â†‘':<12} | {'bottom_5â†‘':<12}"
        )
        lines.append("-" * 140)
        bal_vals = get_std_vals("balanced_acc")
        gini_vals = get_std_vals("acc_gini_coef")
        fair_vals = get_std_vals("fairness_score")
        eod_vals = get_std_vals("eod")
        b3_vals = get_std_vals("bottom_3_class_acc")
        b5_vals = get_std_vals("bottom_5_class_acc")
        for n in exps:
            m = results[n].get("standard_metrics", {})
            lines.append(
                f"{n:<25} | {cls._format_val(m.get('balanced_acc', 0), bal_vals, True):<14} | "
                f"{cls._format_val(m.get('acc_gini_coef', 0), gini_vals, False):<14} | "
                f"{cls._format_val(m.get('fairness_score', 0), fair_vals, True):<14} | "
                f"{cls._format_val(m.get('eod', 0), eod_vals, False):<12} | "
                f"{cls._format_val(m.get('bottom_3_class_acc', 0), b3_vals, True):<12} | "
                f"{cls._format_val(m.get('bottom_5_class_acc', 0), b5_vals, True):<12}"
            )

        # 5. Corruption é²æ£’æ€§ (â†‘ é«˜å¥½)
        has_corr = any(results[n].get("corruption_results") for n in exps)
        if has_corr:
            lines.append("\nğŸŒªï¸ Corruptioné²æ£’æ€§ (Corruption Robustness)")
            lines.append("-" * 80)
            lines.append(f"{'Experiment':<25} | {'overall_avgâ†‘':<16}")
            lines.append("-" * 50)
            overall_vals = get_corr_vals("overall_avg")
            for n in exps:
                corr = results[n].get("corruption_results") or {}
                lines.append(
                    f"{n:<25} | {cls._format_val(corr.get('overall_avg', 0), overall_vals, True):<16}"
                )

            # by_severity
            first_corr = next(
                (
                    results[n].get("corruption_results")
                    for n in exps
                    if results[n].get("corruption_results")
                ),
                {},
            )
            severities = sorted(
                list((first_corr.get("by_severity") or {}).keys()), key=lambda x: int(x)
            )
            if severities:
                lines.append(
                    f"\nBy Severity: {' | '.join([f'Sev_{s}â†‘' for s in severities])}"
                )
                for n in exps:
                    corr = results[n].get("corruption_results") or {}
                    by_sev = corr.get("by_severity") or {}
                    lines.append(
                        f"{n:<25} | "
                        + " | ".join([f"{by_sev.get(s, 0):.4f}" for s in severities])
                    )

            # by_category
            categories = list((first_corr.get("by_category") or {}).keys())
            if categories:
                lines.append(
                    f"\nBy Category: {' | '.join([f'{c}â†‘' for c in categories])}"
                )
                for n in exps:
                    corr = results[n].get("corruption_results") or {}
                    by_cat = corr.get("by_category") or {}
                    lines.append(
                        f"{n:<25} | "
                        + " | ".join([f"{by_cat.get(c, 0):.4f}" for c in categories])
                    )

        # 6. OOD é²æ£’æ€§ (AUROCâ†‘é«˜å¥½, FPR95â†“ä½å¥½)
        has_ood = any(results[n].get("ood_results") for n in exps)
        if has_ood:
            lines.append("\nğŸ”® OODé²æ£’æ€§ (OOD Robustness)")
            lines.append("-" * 120)
            lines.append(
                f"{'Experiment':<25} | {'auroc_mspâ†‘':<16} | {'auroc_entropyâ†‘':<18} | "
                f"{'fpr95_mspâ†“':<16} | {'fpr95_entropyâ†“':<18}"
            )
            lines.append("-" * 120)
            auroc_msp = get_ood_vals("ood_auroc_msp")
            auroc_ent = get_ood_vals("ood_auroc_entropy")
            fpr_msp = get_ood_vals("ood_fpr95_msp")
            fpr_ent = get_ood_vals("ood_fpr95_entropy")
            for n in exps:
                ood = results[n].get("ood_results") or {}
                lines.append(
                    f"{n:<25} | {cls._format_val(ood.get('ood_auroc_msp', 0), auroc_msp, True):<16} | "
                    f"{cls._format_val(ood.get('ood_auroc_entropy', 0), auroc_ent, True):<18} | "
                    f"{cls._format_val(ood.get('ood_fpr95_msp', 0), fpr_msp, False):<16} | "
                    f"{cls._format_val(ood.get('ood_fpr95_entropy', 0), fpr_ent, False):<18}"
                )

        # 7. Adversarial é²æ£’æ€§ (â†‘ é«˜å¥½)
        has_adv = any(results[n].get("adversarial_results") for n in exps)
        if has_adv:
            lines.append("\nâš”ï¸ Adversarialé²æ£’æ€§ (Adversarial Robustness)")
            lines.append("-" * 70)
            lines.append(f"{'Experiment':<25} | {'fgsm_accâ†‘':<16} | {'pgd_accâ†‘':<16}")
            lines.append("-" * 70)
            fgsm_vals = get_adv_vals("fgsm_acc")
            pgd_vals = get_adv_vals("pgd_acc")
            for n in exps:
                adv = results[n].get("adversarial_results") or {}
                lines.append(
                    f"{n:<25} | {cls._format_val(adv.get('fgsm_acc', 0), fgsm_vals, True):<16} | "
                    f"{cls._format_val(adv.get('pgd_acc', 0), pgd_vals, True):<16}"
                )

        # 8. GradCAM å¤šæ ·æ€§ (entropyâ†‘é«˜å¥½, similarity/overlapâ†“ä½å¥½)
        has_cam = any(results[n].get("gradcam_metrics") for n in exps)
        if has_cam:
            lines.append("\nğŸ” GradCAMå¤šæ ·æ€§ (GradCAM Diversity)")
            lines.append("-" * 100)
            lines.append(
                f"{'Experiment':<25} | {'cam_entropyâ†‘':<16} | {'cam_similarityâ†“':<18} | {'cam_overlapâ†“':<16}"
            )
            lines.append("-" * 100)
            ent_vals = get_cam_vals("avg_cam_entropy")
            sim_vals = get_cam_vals("avg_cam_similarity")
            ovl_vals = get_cam_vals("avg_cam_overlap")
            for n in exps:
                g = results[n].get("gradcam_metrics") or {}
                lines.append(
                    f"{n:<25} | {cls._format_val(g.get('avg_cam_entropy', 0), ent_vals, True):<16} | "
                    f"{cls._format_val(g.get('avg_cam_similarity', 0), sim_vals, False):<18} | "
                    f"{cls._format_val(g.get('avg_cam_overlap', 0), ovl_vals, False):<16}"
                )

        # 9. å‚æ•°ç©ºé—´å¤šæ ·æ€§ (avg_distanceâ†‘é«˜å¥½è¡¨ç¤ºæ›´å¤šæ ·, direction_divâ†‘é«˜å¥½, stdâ†‘é«˜å¥½)
        has_dist = any(results[n].get("distance_matrix") is not None for n in exps)
        if has_dist:
            lines.append("\nğŸ“ å‚æ•°ç©ºé—´å¤šæ ·æ€§ (Parameter Space Diversity)")
            lines.append("-" * 100)
            lines.append(
                f"{'Experiment':<25} | {'avg_distanceâ†‘':<16} | {'direction_divâ†‘':<18} | {'std_distanceâ†‘':<16}"
            )
            lines.append("-" * 100)
            avg_d = get_dist_vals("avg_distance")
            dir_d = get_dist_vals("direction_diversity")
            std_d = get_dist_vals("std_distance")
            for n in exps:
                r = results[n]
                lines.append(
                    f"{n:<25} | {cls._format_val(r.get('avg_distance', 0), avg_d, True):<16} | "
                    f"{cls._format_val(r.get('direction_diversity', 0), dir_d, True):<18} | "
                    f"{cls._format_val(r.get('std_distance', 0), std_d, True):<16}"
                )

        lines.append("\n" + "=" * 120)
        return "\n".join(lines)

    @classmethod
    def _save_and_print(cls, results: Dict[str, Dict], save_dir: str):
        """ä¿å­˜å¹¶æ‰“å°æŠ¥å‘Š"""
        saver = ResultsSaver(save_dir)
        report_content = cls._generate_report(results)

        # ä¿å­˜ç»“æœ (ç»Ÿä¸€æ ¼å¼)
        saver.save_comparison(results, "comprehensive_results")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (ä¸æ‰“å°åˆ°æ§åˆ¶å°)
        report_path = Path(save_dir) / "detailed_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        get_logger().info(f"\nâœ… Detailed report saved to: {report_path}")
        get_logger().info(f"âœ… All results saved to: {save_dir}")

    @classmethod
    def evaluate_checkpoints(
        cls,
        checkpoint_paths: List[str],
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        ood_dataset: Optional["OODDataset"] = None,
        run_gradcam: bool = False,
        run_loss_landscape: bool = False,
        run_adversarial: bool = True,
    ):
        """
        ä»ç£ç›˜åŠ è½½ checkpoint å¹¶è¯„ä¼°

        é€‚ç”¨åœºæ™¯: è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä¸è®­ç»ƒè§£è€¦
        è¿™æ˜¯ evaluation æ¨¡å—çš„ä¸»å…¥å£ï¼Œå®Œå…¨ç‹¬ç«‹äº training æ¨¡å—ã€‚
        """

        output_dir = cfg.evaluation_dir
        ensure_dir(output_dir)
        results = {}
        all_models = {}  # æ”¶é›†æ‰€æœ‰å®éªŒçš„æ¨¡å‹ç”¨äº Loss Landscape
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        for idx, ckpt_path in enumerate(checkpoint_paths, 1):
            progress = f"[{idx:>2}/{len(checkpoint_paths)}]"
            get_logger().info(f"\n{'â•' * 70}")
            get_logger().info(
                f"{progress} ğŸ“¦ Loading: {Path(ckpt_path).parent.parent.name}"
            )

            # åŠ è½½æ¨¡å‹
            ctx = CheckpointLoader.load(ckpt_path, cfg)
            exp_name = ctx["name"]
            # context ä¸­çš„ models å·²ç»è¢« CheckpointLoader åˆ†é…åˆ°äº†å„ä¸ª GPU (å¦‚æœå¯ç”¨)
            models = ctx["models"]
            all_models[exp_name] = models  # ä¿å­˜ç”¨äºåç»­åˆ†æ

            # ä½¿ç”¨é€šç”¨è¯„ä¼°æ–¹æ³•
            result = cls._evaluate_models(
                models=models,
                exp_name=exp_name,
                test_loader=test_loader,
                cfg=cfg,
                device=device,
                corruption_dataset=corruption_dataset,
                ood_dataset=ood_dataset,
                run_gradcam=run_gradcam,
                run_adversarial=run_adversarial,
            )
            results[exp_name] = result

        # æ¨¡å‹è·ç¦»è®¡ç®—
        if run_loss_landscape and all_models:
            get_logger().info("\nğŸ“ Computing model distances...")
            distance_calc = ModelDistanceCalculator()

            for exp_name, models in all_models.items():
                dist_matrix = distance_calc.compute(models)
                results[exp_name]["distance_matrix"] = dist_matrix

                # è®¡ç®—è¡ç”ŸæŒ‡æ ‡å¹¶ä¿å­˜åˆ° results
                n = len(dist_matrix)
                if n > 1:
                    import math

                    distances = [
                        dist_matrix[i][j] for i in range(n) for j in range(i + 1, n)
                    ]
                    count = len(distances)
                    avg_dist = sum(distances) / count if count > 0 else 0
                    results[exp_name]["avg_distance"] = avg_dist

                    if count > 1:
                        variance = sum((d - avg_dist) ** 2 for d in distances) / count
                        std_dist = math.sqrt(variance)
                    else:
                        std_dist = 0
                    results[exp_name]["std_distance"] = std_dist

                    if avg_dist > 0:
                        results[exp_name]["direction_diversity"] = min(
                            std_dist / avg_dist, 1.0
                        )
                    else:
                        results[exp_name]["direction_diversity"] = 0

        # ç”Ÿæˆå¹¶ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        cls._save_and_print(results, output_dir)

        get_logger().info(f"\nâœ… Complete! All reports saved to: {output_dir}")
        return results
