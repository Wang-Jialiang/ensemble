"""
================================================================================
æŠ¥å‘Šç”Ÿæˆå™¨æ¨¡å—
================================================================================

åŒ…å«: ReportGenerator - å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆ
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from ..datasets.robustness.corruption import CorruptionDataset
    from ..datasets.robustness.domain import DomainShiftDataset
    from ..datasets.robustness.ood import OODDataset

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from ..config import Config
from ..utils import ensure_dir, format_duration, get_logger
from .adversarial import evaluate_adversarial
from .checkpoint import CheckpointLoader
from .corruption_robustness import evaluate_corruption
from .domain_robustness import evaluate_domain_shift
from .gradcam import GradCAMAnalyzer, ModelListWrapper
from .inference import get_all_models_logits, get_models_from_source
from .landscape import LossLandscapeVisualizer
from .metrics import MetricsCalculator
from .ood import evaluate_ood
from .saver import ResultsSaver
from .strategies import get_ensemble_fn
from .visualizer import ReportVisualizer

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŠ¥å‘Šç”Ÿæˆå™¨ (è¯„ä¼° + æŠ¥å‘Š)                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ReportGenerator:
    """å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆå™¨

    ä¸¤ç§ä¸»è¦ä½¿ç”¨æ–¹å¼:
        1. ä»å†…å­˜è¯„ä¼° (è®­ç»ƒåç«‹å³è¯„ä¼°):
           ReportGenerator.evaluate_trainers(trainers=[...], ...)

        2. ä»ç£ç›˜è¯„ä¼° (åŠ è½½ checkpoint):
           ReportGenerator.evaluate_checkpoints(checkpoint_paths=[...], ...)
    """

    @staticmethod
    def _get_rank_marker(
        value: float, all_values: List[float], higher_is_better: bool
    ) -> str:
        """è·å–æ’åæ ‡è®° (ä»…å¤šå®éªŒæ—¶æ˜¾ç¤º)"""
        if len(all_values) <= 1:
            return ""
        sorted_values = sorted(all_values, reverse=higher_is_better)
        if value == sorted_values[0]:
            return " ğŸ¥‡"
        elif value == sorted_values[1]:
            return " ğŸ¥ˆ"
        return ""

    @staticmethod
    def _evaluate_models(models, exp_name, test_loader, cfg, device, training_time=0.0, **datasets) -> Dict[str, Any]:
        """é€šç”¨æ¨¡å‹è¯„ä¼°æ–¹æ³• - ç”Ÿå‘½å‘¨æœŸé’©å­æ¨¡å¼"""
        get_logger().info(f"\nğŸ“Š Evaluating: {exp_name}")
        res = {"experiment_name": exp_name, "training_time_seconds": training_time}

        # 1. æ ‡å‡†æ ‡å‡†æŒ‡æ ‡ (Acc, ECE, NLL)
        res["standard_metrics"] = ReportGenerator._run_standard_eval(models, test_loader, cfg, device)
        
        # 2. é²æ£’æ€§å¥—ä»¶ (Corruption, OOD, Domain)
        res.update(ReportGenerator._run_robustness_eval(models, cfg, test_loader, **datasets))
        
        # 3. å¯¹æŠ—æ€§ä¸å¯è§£é‡Šæ€§åˆ†æ
        res.update(ReportGenerator._run_analysis_eval(models, cfg, test_loader, **datasets))
        
        return res

    @staticmethod
    def _run_standard_eval(models, loader, cfg, device):
        get_logger().info("   ğŸ” Standard evaluation...")
        all_l, all_t = get_all_models_logits(models, loader, device)
        m = MetricsCalculator(cfg.num_classes, cfg.ece_n_bins).calculate_all_metrics(all_l, all_t, get_ensemble_fn(cfg))
        get_logger().info(f"   Ensemble Acc: {m['ensemble_acc']:.2f}% | ECE: {m['ece']:.4f}")
        return m

    @staticmethod
    def _run_robustness_eval(models, cfg, loader, **ds):
        r = {"corruption_results": None, "ood_results": None, "domain_results": None}
        
        if ds.get("corruption_dataset"):
            get_logger().info("   ğŸ” Corruption evaluation...")
            r["corruption_results"] = evaluate_corruption(models, ds["corruption_dataset"], cfg)
            
        if ds.get("ood_dataset"):
            get_logger().info("   ğŸ” OOD detection evaluation...")
            r["ood_results"] = evaluate_ood(models, loader, ds["ood_dataset"].get_loader(cfg), ds["ood_dataset"].name)
            
        if ds.get("domain_dataset"):
            get_logger().info("   ğŸ” Domain shift evaluation...")
            r["domain_results"] = ReportGenerator._evaluate_domain_suite(models, ds["domain_dataset"], cfg)
            
        return r

    @staticmethod
    def _run_analysis_eval(models, cfg, loader, **ds):
        a = {"adversarial_results": None, "gradcam_metrics": None}
        if ds.get("run_adversarial", True):
            get_logger().info("   ğŸ” Adversarial evaluation...")
            a["adversarial_results"] = evaluate_adversarial(models, loader, cfg.adv_eps, cfg.adv_alpha, cfg.adv_pgd_steps, cfg.dataset_name)
        
        if ds.get("run_gradcam", False):
            get_logger().info("   ğŸ” Grad-CAM analysis...")
            a["gradcam_metrics"] = GradCAMAnalyzer(cfg).analyze_ensemble_quality([ModelListWrapper(models)], loader, 50, cfg.image_size)
        return a

    @staticmethod
    def _evaluate_domain_suite(models, dataset, cfg):
        """æ‰§è¡Œå…¨é£æ ¼ç»„åˆçš„ Domain Shift è¯„ä¼°"""
        res = {"by_style_strength": {}, "overall_avg": 0.0}
        accs = []
        for s in dataset.STYLES:
            for st in dataset.STRENGTHS:
                try:
                    loader = dataset.get_loader(s, st, cfg)
                    m = evaluate_domain_shift(models, loader, f"{s}_{st}", cfg.num_classes)
                    res["by_style_strength"][f"{s}_{st}"] = m
                    accs.append(m["domain_acc"])
                except FileNotFoundError: continue
        if accs: res["overall_avg"] = sum(accs) / len(accs)
        return res

    @staticmethod
    def _evaluate_trainer(
        trainer: Any,
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        run_gradcam: bool = False,
        run_adversarial: bool = True,
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ª trainer å¹¶è¿”å›ç»“æœå­—å…¸"""
        models, device = get_models_from_source(trainer)
        return ReportGenerator._evaluate_models(
            models=models,
            exp_name=trainer.name,
            test_loader=test_loader,
            cfg=cfg,
            device=device,
            training_time=getattr(trainer, "total_training_time", 0.0),
            corruption_dataset=corruption_dataset,
            run_gradcam=run_gradcam,
            run_adversarial=run_adversarial,
        )

    @classmethod
    def _generate_report(cls, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š (å¤§çº²åŒ–æ¸²æŸ“)"""
        lines = []
        exps = list(results.keys())
        
        # 1. ç»˜åˆ¶ Header
        lines.append("="*115)
        lines.append(f"ğŸ“Š EXPERIMENT COMPARISON" if len(exps)>1 else f"ğŸ“Š RESULTS: {exps[0]}")
        lines.append("="*115)

        # 2. æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”è¡¨
        lines.extend(cls._format_perf_table(results, exps))
        
        # 3. è¯¦ç»†å­ç³»ç»ŸæŠ¥å‘Š
        for name in exps: 
            lines.extend(cls._format_detailed_exp(results[name]))
            
        # 4. é²æ£’æ€§ä¸“é—¨æ¿å—
        lines.extend(cls._format_robustness_sections(results, exps))
        
        return "\n".join(lines)

    @classmethod
    def _format_perf_table(cls, results, names):
        t = ["\nğŸ¯ Performance Metrics", "-"*115, 
             f"{'Experiment':<25} | {'EnsAccâ†‘':<10} | {'AvgIndâ†‘':<10} | {'Oracleâ†‘':<10} | {'ECEâ†“':<10} | {'NLLâ†“':<10} | {'Time':<12}",
             "-"*115]
        accs = [results[n].get("standard_metrics", {}).get("ensemble_acc", 0) for n in names]
        for n in names:
            m = results[n].get("standard_metrics", {})
            tm = format_duration(results[n].get("training_time_seconds", 0))
            mark = cls._get_rank_marker(m.get("ensemble_acc", 0), accs, True)
            t.append(f"{n:<25} | {m.get('ensemble_acc', 0):<7.2f}{mark:<3} | {m.get('avg_individual_acc', 0):<10.2f} | "
                     f"{m.get('oracle_acc', 0):<10.2f} | {m.get('ece', 0):<10.4f} | {m.get('nll', 0):<10.4f} | {tm:<12}")
        t.append("-" * 115)
        return t

    @classmethod
    def _format_detailed_exp(cls, r):
        m = r.get("standard_metrics", {})
        return ["\nğŸ“‹ Detailed Metrics", "="*115, f"\nğŸ”¹ {r['experiment_name']}", "-"*40,
                f"   ğŸ”€ Div: Dis={m.get('disagreement', 0):.2f}% | JS={m.get('js_divergence', 0):.4f} | Spearman={m.get('spearman_correlation', 1.0):.4f}",
                f"   âš–ï¸ Fair: BalAcc={m.get('balanced_acc', 0):.2f}% | Gini={m.get('acc_gini_coef', 0):.4f} | Score={m.get('fairness_score', 0):.2f}",
                "-" * 40]

    @classmethod
    def _format_robustness_sections(cls, results, names):
        s = ["\nğŸ§ª Robustness Summary"]
        for n in names:
            r = results[n]
            corr = r.get("corruption_results", {}).get("overall_avg", 0)
            pgd = r.get("adversarial_results", {}).get("pgd_acc", 0)
            ood = r.get("ood_results", {}).get("auc_roc", 0)
            s.append(f"   {n:<25} | Corr: {corr:2.2f}% | PGD: {pgd:2.2f}% | OOD AUC: {ood:.4f}")
        return s

    @classmethod
    def _save_and_print(cls, results: Dict[str, Dict], save_dir: str):
        """ä¿å­˜å¹¶æ‰“å°æŠ¥å‘Š"""
        saver = ResultsSaver(save_dir)
        report_content = cls._generate_report(results)

        # ä¿å­˜ç»“æœ (ç»Ÿä¸€æ ¼å¼)
        saver.save_comparison(results, "comprehensive_results")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (ä¸æ‰“å°åˆ°æ§åˆ¶å°)
        report_path = Path(save_dir) / "detailed_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        get_logger().info(f"\nâœ… Detailed report saved to: {report_path}")
        get_logger().info(f"âœ… All results saved to: {save_dir}")

    @classmethod
    def evaluate_trainers(
        cls,
        trainers: List,  # List of StagedEnsembleTrainer instances
        test_loader: DataLoader,
        cfg: Config,
        save_dir: str,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        run_gradcam: bool = False,
        run_adversarial: bool = True,
    ):
        """
        ä»å†…å­˜è¯„ä¼°å¤šä¸ª trainer å¹¶ç”ŸæˆæŠ¥å‘Š

        é€‚ç”¨åœºæ™¯: è®­ç»ƒåˆšå®Œæˆï¼Œæ¨¡å‹è¿˜åœ¨å†…å­˜ä¸­
        """
        get_logger().info(
            f"\n{'=' * 80}\nğŸ“Š EVALUATION MODE | Models: {len(trainers)}\n{'=' * 80}"
        )

        # è¯„ä¼°æ‰€æœ‰ trainers
        results = {}
        for idx, trainer in enumerate(trainers, 1):
            get_logger().info(f"\n[{idx}/{len(trainers)}] {trainer.name}")
            result = cls._evaluate_trainer(
                trainer,
                test_loader,
                cfg,
                corruption_dataset,
                run_gradcam,
                run_adversarial,
            )
            results[trainer.name] = result

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        get_logger().info("\nğŸ“Š Generating visualizations...")
        visualizer = ReportVisualizer(save_dir, dpi=cfg.plot_dpi)
        visualizer.generate_all(results)

        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        cls._save_and_print(results, save_dir)

    @classmethod
    def evaluate_checkpoints(
        cls,
        checkpoint_paths: List[str],
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        ood_dataset: Optional["OODDataset"] = None,
        domain_dataset: Optional["DomainShiftDataset"] = None,
        run_gradcam: bool = False,
        run_loss_landscape: bool = False,
        run_adversarial: bool = True,
    ):
        """
        ä»ç£ç›˜åŠ è½½ checkpoint å¹¶è¯„ä¼°

        é€‚ç”¨åœºæ™¯: è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä¸è®­ç»ƒè§£è€¦
        è¿™æ˜¯ evaluation æ¨¡å—çš„ä¸»å…¥å£ï¼Œå®Œå…¨ç‹¬ç«‹äº training æ¨¡å—ã€‚
        """
        get_logger().info(f"\n{'=' * 80}")
        get_logger().info(
            f"ğŸ“Š EVALUATION FROM CHECKPOINTS | Count: {len(checkpoint_paths)}"
        )
        get_logger().info(f"{'=' * 80}")

        output_dir = cfg.save_dir
        ensure_dir(output_dir)
        results = {}
        all_models = {}  # æ”¶é›†æ‰€æœ‰å®éªŒçš„æ¨¡å‹ç”¨äº Loss Landscape
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        for idx, ckpt_path in enumerate(checkpoint_paths, 1):
            get_logger().info(f"\n[{idx}/{len(checkpoint_paths)}] Loading: {ckpt_path}")

            # åŠ è½½æ¨¡å‹
            ctx = CheckpointLoader.load(ckpt_path, cfg)
            exp_name = ctx["name"]
            models = [m.to(device) for m in ctx["models"]]
            all_models[exp_name] = models  # ä¿å­˜ç”¨äºåç»­åˆ†æ

            # ä½¿ç”¨é€šç”¨è¯„ä¼°æ–¹æ³•
            result = cls._evaluate_models(
                models=models,
                exp_name=exp_name,
                test_loader=test_loader,
                cfg=cfg,
                device=device,
                training_time=ctx["training_time"],
                corruption_dataset=corruption_dataset,
                ood_dataset=ood_dataset,
                domain_dataset=domain_dataset,
                run_gradcam=run_gradcam,
                run_adversarial=run_adversarial,
            )
            result["train_config"] = ctx["config"]
            results[exp_name] = result

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        get_logger().info("\nğŸ“Š Generating visualizations...")
        visualizer = ReportVisualizer(output_dir, dpi=cfg.plot_dpi)
        visualizer.generate_all(results)

        # Loss Landscape åˆ†æ
        if run_loss_landscape and all_models:
            get_logger().info("\nğŸ”ï¸ Generating Loss Landscape visualizations...")
            landscape_viz = LossLandscapeVisualizer(output_dir, dpi=cfg.plot_dpi)

            for exp_name, models in all_models.items():
                # æ¨¡å‹å‚æ•°è·ç¦»çƒ­åŠ›å›¾ (æ— éœ€ loss-landscapes ä¾èµ–)
                landscape_viz.plot_model_distance_heatmap(
                    models, filename=f"{exp_name}_model_distances.png"
                )

                # Loss Landscape æ’å€¼ (éœ€è¦ loss-landscapes)
                landscape_viz.plot_ensemble_interpolations(
                    models,
                    test_loader,
                    device,
                    filename=f"{exp_name}_loss_landscape.png",
                )

                # 2D/3D è¡¨é¢å›¾ - ä¸ºç¬¬ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆ (è®¡ç®—é‡è¾ƒå¤§)
                if len(models) > 0:
                    landscape_viz.plot_2d_plane(
                        models[0],
                        test_loader,
                        device,
                        distance=1.0,
                        steps=20,  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«è®¡ç®—
                        filename=f"{exp_name}_landscape_surface.png",
                        model_name=f"{exp_name}_M1",
                    )

        # ç”Ÿæˆå¹¶ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        cls._save_and_print(results, output_dir)

        get_logger().info(f"\nâœ… Complete! All reports saved to: {output_dir}")
        return results
