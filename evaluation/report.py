"""
================================================================================
æŠ¥å‘Šç”Ÿæˆå™¨æ¨¡å—
================================================================================

åŒ…å«: ReportGenerator - å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆ
"""

from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from ..datasets.robustness.corruption import CorruptionDataset
    from ..datasets.robustness.ood import OODDataset

import torch
from torch.utils.data import DataLoader

from ..config import Config
from ..utils import ensure_dir, format_duration, get_logger
from .adversarial import evaluate_adversarial
from .checkpoint import CheckpointLoader
from .corruption_robustness import evaluate_corruption
from .gradcam import GradCAMAnalyzer, ModelListWrapper
from .inference import get_all_models_logits
from .landscape import ModelDistanceCalculator
from .metrics import MetricsCalculator
from .ood import evaluate_ood
from .saver import ResultsSaver
from .strategies import get_ensemble_fn

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŠ¥å‘Šç”Ÿæˆå™¨ (è¯„ä¼° + æŠ¥å‘Š)                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ReportGenerator:
    """å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆå™¨

    ä¸»å…¥å£:
        ReportGenerator.evaluate_checkpoints(checkpoint_paths=[...], ...)
    """

    @staticmethod
    def _get_rank_marker(
        value: float, all_values: List[float], higher_is_better: bool
    ) -> str:
        """è·å–æ’åæ ‡è®° (ä»…å¤šå®éªŒæ—¶æ˜¾ç¤º)"""
        if len(all_values) <= 1:
            return ""
        sorted_values = sorted(all_values, reverse=higher_is_better)
        if value == sorted_values[0]:
            return " ğŸ¥‡"
        elif value == sorted_values[1]:
            return " ğŸ¥ˆ"
        return ""

    @staticmethod
    def _evaluate_models(
        models, exp_name, test_loader, cfg, device, training_time=0.0, **datasets
    ) -> Dict[str, Any]:
        """é€šç”¨æ¨¡å‹è¯„ä¼°æ–¹æ³• - ç”Ÿå‘½å‘¨æœŸé’©å­æ¨¡å¼"""
        get_logger().info(f"\nğŸ“Š Evaluating: {exp_name}")
        res = {"experiment_name": exp_name, "training_time_seconds": training_time}

        # 1. æ ‡å‡†æ ‡å‡†æŒ‡æ ‡ (Acc, ECE, NLL)
        res["standard_metrics"] = ReportGenerator._run_standard_eval(
            models, test_loader, cfg, device
        )

        # 2. é²æ£’æ€§å¥—ä»¶ (Corruption, OOD, Domain)
        res.update(
            ReportGenerator._run_robustness_eval(models, cfg, test_loader, **datasets)
        )

        # 3. å¯¹æŠ—æ€§ä¸å¯è§£é‡Šæ€§åˆ†æ
        res.update(
            ReportGenerator._run_analysis_eval(models, cfg, test_loader, **datasets)
        )

        return res

    @staticmethod
    def _run_standard_eval(models, loader, cfg, device):
        get_logger().info("   ğŸ” Standard evaluation...")
        all_l, all_t = get_all_models_logits(models, loader, device)
        return MetricsCalculator(cfg.num_classes, cfg.ece_n_bins).calculate_all_metrics(
            all_l, all_t, get_ensemble_fn(cfg)
        )

    @staticmethod
    def _run_robustness_eval(models, cfg, loader, **ds):
        r = {"corruption_results": None, "ood_results": None}

        if ds.get("corruption_dataset"):
            get_logger().info("   ğŸ” Corruption evaluation...")
            r["corruption_results"] = evaluate_corruption(
                models, ds["corruption_dataset"], cfg
            )

        if ds.get("ood_dataset"):
            get_logger().info("   ğŸ” OOD detection evaluation...")
            r["ood_results"] = evaluate_ood(
                models,
                loader,
                ds["ood_dataset"].get_loader(cfg),
                ds["ood_dataset"].name,
            )

        return r

    @staticmethod
    def _run_analysis_eval(models, cfg, loader, **ds):
        a = {"adversarial_results": None, "gradcam_metrics": None}
        if ds.get("run_adversarial", True):
            get_logger().info("   ğŸ” Adversarial evaluation...")
            a["adversarial_results"] = evaluate_adversarial(
                models, loader, cfg=cfg, logger=get_logger()
            )

        if ds.get("run_gradcam", False):
            get_logger().info("   ğŸ” Grad-CAM analysis...")
            a["gradcam_metrics"] = GradCAMAnalyzer(cfg).analyze_ensemble_quality(
                [ModelListWrapper(models)],
                loader,
                cfg.gradcam_num_samples,
                cfg.image_size,
            )
        return a

    @classmethod
    def _generate_report(cls, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š (å¤§çº²åŒ–æ¸²æŸ“)"""
        lines = []
        exps = list(results.keys())

        # 1. ç»˜åˆ¶ Header
        lines.append("=" * 115)
        lines.append(
            "ğŸ“Š EXPERIMENT COMPARISON" if len(exps) > 1 else f"ğŸ“Š RESULTS: {exps[0]}"
        )
        lines.append("=" * 115)

        # 2. æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”è¡¨
        lines.extend(cls._format_perf_table(results, exps))

        # 3. å¤šæ ·æ€§/å…¬å¹³æ€§/CAM è¡¨æ ¼
        lines.extend(cls._format_diversity_table(results, exps))

        # 4. CKA è¯¦æƒ… + EOD/Bottom-K
        lines.extend(cls._format_additional_metrics(results, exps))

        # 5. é²æ£’æ€§ä¸“é—¨æ¿å—
        lines.extend(cls._format_robustness_sections(results, exps))

        return "\n".join(lines)

    @classmethod
    def _format_perf_table(cls, results, names):
        """æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”è¡¨ - å¸¦æ’åæ ‡è®°"""
        t = [
            "\nğŸ¯ Performance Metrics",
            "-" * 115,
            f"{'Experiment':<25} | {'EnsAccâ†‘':<10} | {'AvgIndâ†‘':<10} | {'Oracleâ†‘':<10} | {'ECEâ†“':<10} | {'NLLâ†“':<10} | {'Time':<12}",
            "-" * 115,
        ]

        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡å€¼ç”¨äºæ’å
        def get_vals(key):
            return [results[n].get("standard_metrics", {}).get(key, 0) for n in names]

        ens_accs = get_vals("ensemble_acc")
        avg_accs = get_vals("avg_individual_acc")
        oracle_accs = get_vals("oracle_acc")
        eces = get_vals("ece")
        nlls = get_vals("nll")

        for n in names:
            m = results[n].get("standard_metrics", {})
            tm = format_duration(results[n].get("training_time_seconds", 0))

            # è·å–æ¯ä¸ªæŒ‡æ ‡çš„æ’åæ ‡è®°
            ens_mark = cls._get_rank_marker(m.get("ensemble_acc", 0), ens_accs, True)
            avg_mark = cls._get_rank_marker(
                m.get("avg_individual_acc", 0), avg_accs, True
            )
            ora_mark = cls._get_rank_marker(m.get("oracle_acc", 0), oracle_accs, True)
            ece_mark = cls._get_rank_marker(
                m.get("ece", 0), eces, False
            )  # â†“ lower is better
            nll_mark = cls._get_rank_marker(
                m.get("nll", 0), nlls, False
            )  # â†“ lower is better

            t.append(
                f"{n:<25} | {m.get('ensemble_acc', 0):<6.2f}{ens_mark:<4} | "
                f"{m.get('avg_individual_acc', 0):<6.2f}{avg_mark:<4} | "
                f"{m.get('oracle_acc', 0):<6.2f}{ora_mark:<4} | "
                f"{m.get('ece', 0):<6.4f}{ece_mark:<4} | "
                f"{m.get('nll', 0):<6.4f}{nll_mark:<4} | {tm:<12}"
            )
        t.append("-" * 115)
        return t

    @classmethod
    def _format_diversity_table(cls, results, names):
        """ç”Ÿæˆ Div/Fair/CAM æ¨ªå‘è¡¨æ ¼ - å¸¦æ’åæ ‡è®°"""
        has_cam = any(results[n].get("gradcam_metrics") for n in names)

        header = f"{'Experiment':<25} | {'Disâ†‘':<10} | {'CKA_Divâ†‘':<10} | {'BalAccâ†‘':<10} | {'Giniâ†“':<10} | {'Fairâ†‘':<10}"
        if has_cam:
            header += f" | {'Entropy':<8} | {'Simâ†“':<8} | {'Overlapâ†“':<8}"

        t = [
            "\nğŸ”€ Diversity / Fairness / CAM Metrics",
            "-" * (115 if not has_cam else 145),
            header,
            "-" * (115 if not has_cam else 145),
        ]

        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡å€¼
        def get_vals(key):
            return [results[n].get("standard_metrics", {}).get(key, 0) for n in names]

        def get_cam_vals(key):
            return [results[n].get("gradcam_metrics", {}).get(key, 0) for n in names]

        dis_vals = get_vals("disagreement")
        cka_div_vals = get_vals("cka_diversity")
        bal_vals = get_vals("balanced_acc")
        gini_vals = get_vals("acc_gini_coef")
        fair_vals = get_vals("fairness_score")
        sim_vals = get_cam_vals("avg_cam_similarity") if has_cam else []
        overlap_vals = get_cam_vals("avg_cam_overlap") if has_cam else []

        for n in names:
            m = results[n].get("standard_metrics", {})
            g = results[n].get("gradcam_metrics", {})

            # è·å–æ’åæ ‡è®°
            dis_mark = cls._get_rank_marker(m.get("disagreement", 0), dis_vals, True)
            cka_mark = cls._get_rank_marker(
                m.get("cka_diversity", 0), cka_div_vals, True
            )
            bal_mark = cls._get_rank_marker(m.get("balanced_acc", 0), bal_vals, True)
            gini_mark = cls._get_rank_marker(
                m.get("acc_gini_coef", 0), gini_vals, False
            )  # â†“
            fair_mark = cls._get_rank_marker(
                m.get("fairness_score", 0), fair_vals, True
            )

            row = (
                f"{n:<25} | {m.get('disagreement', 0):<6.2f}{dis_mark:<4} | "
                f"{m.get('cka_diversity', 0):<6.4f}{cka_mark:<4} | "
                f"{m.get('balanced_acc', 0):<6.2f}{bal_mark:<4} | "
                f"{m.get('acc_gini_coef', 0):<6.4f}{gini_mark:<4} | "
                f"{m.get('fairness_score', 0):<6.2f}{fair_mark:<4}"
            )
            if has_cam:
                sim_mark = cls._get_rank_marker(
                    g.get("avg_cam_similarity", 0), sim_vals, False
                )
                ovl_mark = cls._get_rank_marker(
                    g.get("avg_cam_overlap", 0), overlap_vals, False
                )
                row += (
                    f" | {g.get('avg_cam_entropy', 0):<8.4f} | "
                    f"{g.get('avg_cam_similarity', 0):<4.4f}{sim_mark:<4} | "
                    f"{g.get('avg_cam_overlap', 0):<4.4f}{ovl_mark:<4}"
                )
            t.append(row)

        t.append("-" * (115 if not has_cam else 145))
        return t

    @classmethod
    def _format_additional_metrics(cls, results, names):
        """CKA è¯¦æƒ… + EOD/Bottom-K è¡¨æ ¼"""
        t = []

        # ===== CKA è¯¦æƒ… =====
        t.append("\nğŸ“Š CKA Similarity Details")
        t.append("-" * 80)
        t.append(
            f"{'Experiment':<25} | {'Avg_CKAâ†“':<12} | {'Min_CKA':<12} | {'Max_CKA':<12} | {'CKA_Divâ†‘':<12}"
        )
        t.append("-" * 80)

        for n in names:
            m = results[n].get("standard_metrics", {})
            t.append(
                f"{n:<25} | {m.get('avg_cka', 0):<12.4f} | "
                f"{m.get('min_cka', 0):<12.4f} | {m.get('max_cka', 0):<12.4f} | "
                f"{m.get('cka_diversity', 0):<12.4f}"
            )
        t.append("-" * 80)

        # ===== EOD + Bottom-K =====
        t.append("\nâš–ï¸ Fairness Details (EOD + Bottom-K)")
        t.append("-" * 80)
        t.append(
            f"{'Experiment':<25} | {'EODâ†“':<10} | {'Bottom3â†‘':<12} | {'Bottom5â†‘':<12}"
        )
        t.append("-" * 80)

        for n in names:
            m = results[n].get("standard_metrics", {})
            t.append(
                f"{n:<25} | {m.get('eod', 0):<10.2f} | "
                f"{m.get('bottom_3_class_acc', 0):<12.2f} | "
                f"{m.get('bottom_5_class_acc', 0):<12.2f}"
            )
        t.append("-" * 80)

        return t

    @classmethod
    def _format_robustness_sections(cls, results, names):
        """é²æ£’æ€§ç»¼åˆæŠ¥å‘Š - åŒ…å« OOD/Adversarial/Corruption å…¨éƒ¨æŒ‡æ ‡"""
        s = []

        # ===== 1. å¯¹æŠ—é²æ£’æ€§ =====
        s.append("\nâš”ï¸ Adversarial Robustness")
        s.append("-" * 100)
        s.append(
            f"{'Experiment':<25} | {'Cleanâ†‘':<10} | {'FGSMâ†‘':<10} | {'PGDâ†‘':<10} | {'Îµ':<8} | {'Steps':<6}"
        )
        s.append("-" * 100)

        for n in names:
            adv = results[n].get("adversarial_results") or {}
            if not adv:
                s.append(
                    f"{n:<25} | {'N/A':<10} | {'N/A':<10} | {'N/A':<10} | {'N/A':<8} | {'N/A':<6}"
                )
            elif "pgd_acc" in adv:
                # å• Îµ æ¨¡å¼
                s.append(
                    f"{n:<25} | {adv.get('clean_acc', 0):<10.2f} | "
                    f"{adv.get('fgsm_acc', 0):<10.2f} | {adv.get('pgd_acc', 0):<10.2f} | "
                    f"{adv.get('eps_255', 0):<8.1f} | {adv.get('pgd_steps', 0):<6}"
                )
            else:
                # å¤š Îµ æ¨¡å¼: æ˜¾ç¤ºæ¯ä¸ª Îµ çš„ç»“æœ
                for eps_key, eps_data in adv.items():
                    s.append(
                        f"{n:<25} | {eps_data.get('clean_acc', 0):<10.2f} | "
                        f"{eps_data.get('fgsm_acc', 0):<10.2f} | {eps_data.get('pgd_acc', 0):<10.2f} | "
                        f"{eps_data.get('eps_255', 0):<8.1f} | {eps_data.get('pgd_steps', 0):<6}"
                    )
        s.append("-" * 100)

        # ===== 2. OOD æ£€æµ‹ =====
        has_ood = any(results[n].get("ood_results") for n in names)
        if has_ood:
            s.append("\nğŸ”® OOD Detection")
            s.append("-" * 100)
            s.append(
                f"{'Experiment':<25} | {'AUROC_MSPâ†‘':<12} | {'AUROC_Entâ†‘':<12} | {'FPR95_MSPâ†“':<12} | {'FPR95_Entâ†“':<12}"
            )
            s.append("-" * 100)

            for n in names:
                ood = results[n].get("ood_results") or {}
                if ood:
                    s.append(
                        f"{n:<25} | {ood.get('ood_auroc_msp', 0):<12.2f} | "
                        f"{ood.get('ood_auroc_entropy', 0):<12.2f} | "
                        f"{ood.get('ood_fpr95_msp', 0):<12.2f} | "
                        f"{ood.get('ood_fpr95_entropy', 0):<12.2f}"
                    )
                else:
                    s.append(
                        f"{n:<25} | {'N/A':<12} | {'N/A':<12} | {'N/A':<12} | {'N/A':<12}"
                    )
            s.append("-" * 100)

        # ===== 3. Corruption é²æ£’æ€§ =====
        has_corr = any(results[n].get("corruption_results") for n in names)
        if has_corr:
            s.append("\nğŸŒªï¸ Corruption Robustness")
            s.append("-" * 100)

            # 3.1 æ€»ä½“å¹³å‡
            s.append(f"{'Experiment':<25} | {'Overallâ†‘':<10}")
            s.append("-" * 50)
            for n in names:
                corr = results[n].get("corruption_results") or {}
                s.append(f"{n:<25} | {corr.get('overall_avg', 0):<10.2f}")
            s.append("")

            # 3.2 æŒ‰ä¸¥é‡ç¨‹åº¦å±•ç¤º
            first_corr = next(
                (
                    results[n].get("corruption_results")
                    for n in names
                    if results[n].get("corruption_results")
                ),
                {},
            )
            severities = sorted(
                list((first_corr.get("by_severity") or {}).keys()), key=lambda x: int(x)
            )

            if severities:
                s.append(
                    f"{'Experiment':<25} | "
                    + " | ".join([f"Sev {str(sev):<4}" for sev in severities])
                )
                s.append("-" * (28 + 10 * len(severities)))
                for n in names:
                    corr = results[n].get("corruption_results") or {}
                    by_sev = corr.get("by_severity") or {}
                    sev_vals = " | ".join(
                        [f"{by_sev.get(sev, 0):<8.2f}" for sev in severities]
                    )
                    s.append(f"{n:<25} | {sev_vals}")
            s.append("")

            # 3.3 æŒ‰ç±»åˆ«å±•ç¤º
            categories = list((first_corr.get("by_category") or {}).keys())
            if categories:
                s.append(
                    f"{'Experiment':<25} | "
                    + " | ".join([f"{c:<10}" for c in categories])
                )
                s.append("-" * (30 + 13 * len(categories)))
                for n in names:
                    corr = results[n].get("corruption_results") or {}
                    by_cat = corr.get("by_category") or {}
                    cat_vals = " | ".join(
                        [f"{by_cat.get(c, 0):<10.2f}" for c in categories]
                    )
                    s.append(f"{n:<25} | {cat_vals}")
            s.append("-" * 100)

        return s

    @classmethod
    def _save_and_print(cls, results: Dict[str, Dict], save_dir: str):
        """ä¿å­˜å¹¶æ‰“å°æŠ¥å‘Š"""
        saver = ResultsSaver(save_dir)
        report_content = cls._generate_report(results)

        # ä¿å­˜ç»“æœ (ç»Ÿä¸€æ ¼å¼)
        saver.save_comparison(results, "comprehensive_results")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (ä¸æ‰“å°åˆ°æ§åˆ¶å°)
        report_path = Path(save_dir) / "detailed_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        get_logger().info(f"\nâœ… Detailed report saved to: {report_path}")
        get_logger().info(f"âœ… All results saved to: {save_dir}")

    @classmethod
    def evaluate_checkpoints(
        cls,
        checkpoint_paths: List[str],
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional["CorruptionDataset"] = None,
        ood_dataset: Optional["OODDataset"] = None,
        run_gradcam: bool = False,
        run_loss_landscape: bool = False,
        run_adversarial: bool = True,
    ):
        """
        ä»ç£ç›˜åŠ è½½ checkpoint å¹¶è¯„ä¼°

        é€‚ç”¨åœºæ™¯: è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œä¸è®­ç»ƒè§£è€¦
        è¿™æ˜¯ evaluation æ¨¡å—çš„ä¸»å…¥å£ï¼Œå®Œå…¨ç‹¬ç«‹äº training æ¨¡å—ã€‚
        """
        get_logger().info(f"\n{'=' * 80}")
        get_logger().info(
            f"ğŸ“Š EVALUATION FROM CHECKPOINTS | Count: {len(checkpoint_paths)}"
        )
        get_logger().info(f"{'=' * 80}")

        output_dir = cfg.evaluation_dir
        ensure_dir(output_dir)
        results = {}
        all_models = {}  # æ”¶é›†æ‰€æœ‰å®éªŒçš„æ¨¡å‹ç”¨äº Loss Landscape
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        for idx, ckpt_path in enumerate(checkpoint_paths, 1):
            get_logger().info(f"\n[{idx}/{len(checkpoint_paths)}] Loading: {ckpt_path}")

            # åŠ è½½æ¨¡å‹
            ctx = CheckpointLoader.load(ckpt_path, cfg)
            exp_name = ctx["name"]
            # context ä¸­çš„ models å·²ç»è¢« CheckpointLoader åˆ†é…åˆ°äº†å„ä¸ª GPU (å¦‚æœå¯ç”¨)
            models = ctx["models"]
            all_models[exp_name] = models  # ä¿å­˜ç”¨äºåç»­åˆ†æ

            # ä½¿ç”¨é€šç”¨è¯„ä¼°æ–¹æ³•
            result = cls._evaluate_models(
                models=models,
                exp_name=exp_name,
                test_loader=test_loader,
                cfg=cfg,
                device=device,
                training_time=ctx["training_time"],
                corruption_dataset=corruption_dataset,
                ood_dataset=ood_dataset,
                run_gradcam=run_gradcam,
                run_adversarial=run_adversarial,
            )
            result["train_config"] = ctx["config"]
            results[exp_name] = result

        # æ¨¡å‹è·ç¦»è®¡ç®—
        if run_loss_landscape and all_models:
            get_logger().info("\nğŸ“ Computing model distances...")
            distance_calc = ModelDistanceCalculator()

            for exp_name, models in all_models.items():
                dist_matrix = distance_calc.compute(models)
                results[exp_name]["distance_matrix"] = dist_matrix

        # ç”Ÿæˆå¹¶ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        cls._save_and_print(results, output_dir)

        get_logger().info(f"\nâœ… Complete! All reports saved to: {output_dir}")
        return results
