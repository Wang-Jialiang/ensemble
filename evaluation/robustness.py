"""
================================================================================
é²æ£’æ€§è¯„ä¼°æ¨¡å—
================================================================================

åŒ…å«: Corruption è¯„ä¼°ã€å¯¹æŠ—æ”»å‡» (FGSM/PGD)ã€åŸŸåç§»è¯„ä¼°
"""

from typing import Any, Dict, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

from ..utils import get_logger
from .core import (
    ENSEMBLE_STRATEGIES,
    MetricsCalculator,
    extract_models,
    get_all_models_logits,
)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Corruption é²æ£’æ€§è¯„ä¼°                                                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def evaluate_corruption(
    trainer_or_models: Any,
    corruption_dataset: "CorruptionDataset",
    batch_size: int = 128,
    num_workers: int = 4,
    logger: Optional[Any] = None,
) -> Dict[str, Any]:
    """
    é€šç”¨ Corruption é²æ£’æ€§è¯„ä¼°
    """
    from tqdm import tqdm

    logger = logger or get_logger()
    dataset_name = corruption_dataset.name
    n_corruptions = len(corruption_dataset.CORRUPTIONS)
    total_evals = 5 * n_corruptions  # 5 severity Ã— N corruptions

    logger.info(f"\nğŸ§ª Running Corruption Evaluation on {dataset_name}")
    logger.info(
        f"   ğŸ“Š {n_corruptions} corruptions Ã— 5 severities = {total_evals} æ¬¡è¯„ä¼°"
    )

    models, device = extract_models(trainer_or_models)
    results = {}
    overall_avg = 0.0

    # åˆ›å»ºæ€»è¿›åº¦æ¡
    pbar = tqdm(total=total_evals, desc="Corruption Eval", leave=False)

    for severity in range(1, 6):
        results[severity] = {}
        severity_accs = []

        for corruption in corruption_dataset.CORRUPTIONS:
            pbar.set_postfix({"severity": severity, "type": corruption[:10]})

            loader = corruption_dataset.get_loader(
                corruption,
                severity=severity,
                batch_size=batch_size,
                num_workers=num_workers,
            )

            # åªéœ€é¢„æµ‹ï¼Œä¸éœ€è¦è¯¦ç»†çš„ Calculator (åªç®— Acc)
            all_logits, targets = get_all_models_logits(models, loader, device)

            ensemble_logits = all_logits.mean(dim=0)
            ensemble_preds = ensemble_logits.argmax(dim=1)
            acc = 100.0 * (ensemble_preds == targets).float().mean().item()

            results[severity][corruption] = acc
            severity_accs.append(acc)
            pbar.update(1)

        avg_acc_sev = np.mean(severity_accs)
        results[severity]["avg"] = avg_acc_sev
        overall_avg += avg_acc_sev

    pbar.close()

    overall_avg /= 5.0
    logger.info(f"   âœ… å®Œæˆ! Overall Avg: {overall_avg:.2f}%")

    results["severity_5_raw"] = results[5]
    results["overall_avg"] = overall_avg
    return results


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Domain Shift (åŸŸåç§») è¯„ä¼°                                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def evaluate_domain_shift(
    trainer_or_models: Any,
    domain_loader: DataLoader,
    domain_name: str = "Domain",
    num_classes: int = 10,
    logger: Optional[Any] = None,
) -> Dict[str, Any]:
    """
    Domain Shift (åŸŸåç§») è¯„ä¼°

    è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§†è§‰åŸŸ/é£æ ¼ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡ã€‚

    Args:
        trainer_or_models: StagedEnsembleTrainer å®ä¾‹æˆ– List[nn.Module]
        domain_loader: Domain Shift æ•°æ®åŠ è½½å™¨ï¼ˆéœ€åŒ…å«æ­£ç¡®çš„æ ‡ç­¾ï¼‰
        domain_name: åŸŸåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        num_classes: ç±»åˆ«æ•°é‡
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        åŒ…å«åŸŸåç§»è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    logger = logger or get_logger()
    logger.info(f"\nğŸŒ Running Domain Shift Evaluation ({domain_name})")

    models, device = extract_models(trainer_or_models)
    calculator = MetricsCalculator(num_classes=num_classes)
    ensemble_fn = ENSEMBLE_STRATEGIES["mean"]

    # è·å–æ‰€æœ‰æ¨¡å‹çš„ logits
    all_logits, targets = get_all_models_logits(models, domain_loader, device)

    if len(all_logits) == 0:
        logger.warning("   âš ï¸ æ— æ•°æ®å¯è¯„ä¼°")
        return {"domain_name": domain_name, "error": "No data"}

    # è®¡ç®—æŒ‡æ ‡
    metrics = calculator.calculate_all_metrics(all_logits, targets, ensemble_fn)

    results = {
        "domain_name": domain_name,
        "num_samples": len(targets),
        "domain_acc": metrics["ensemble_acc"],
        "domain_balanced_acc": metrics["balanced_acc"],
        "domain_worst_class_acc": metrics["worst_class_acc"],
        "domain_avg_individual_acc": metrics["avg_individual_acc"],
        # é›†æˆ vs å•æ¨¡å‹çš„æå‡
        "domain_ensemble_gain": metrics["ensemble_acc"] - metrics["avg_individual_acc"],
    }

    logger.info(f"   âœ… Domain Shift Results ({domain_name}):")
    logger.info(f"      Ensemble Acc: {results['domain_acc']:.2f}%")
    logger.info(f"      Balanced Acc: {results['domain_balanced_acc']:.2f}%")
    logger.info(f"      Ensemble Gain: {results['domain_ensemble_gain']:+.2f}%")

    return results


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å¯¹æŠ—é²æ£’æ€§è¯„ä¼° (FGSM/PGD)                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def fgsm_attack(
    model: nn.Module,
    images: torch.Tensor,
    labels: torch.Tensor,
    eps: float,
    mean: torch.Tensor,
    std: torch.Tensor,
) -> torch.Tensor:
    """FGSM (Fast Gradient Sign Method) å¯¹æŠ—æ”»å‡»

    å•æ­¥æ”»å‡»ï¼Œæ²¿æŸå¤±æ¢¯åº¦ç¬¦å·æ–¹å‘æ·»åŠ æ‰°åŠ¨ã€‚

    Args:
        model: ç›®æ ‡æ¨¡å‹
        images: è¾“å…¥å›¾åƒ (å·²æ ‡å‡†åŒ–)
        labels: çœŸå®æ ‡ç­¾
        eps: æ‰°åŠ¨å¼ºåº¦ Îµ (åœ¨åŸå§‹åƒç´ ç©ºé—´, å¦‚ 8/255)
        mean: æ ‡å‡†åŒ–å‡å€¼
        std: æ ‡å‡†åŒ–æ ‡å‡†å·®

    Returns:
        å¯¹æŠ—æ ·æœ¬ (å·²æ ‡å‡†åŒ–)
    """
    images = images.clone().detach().requires_grad_(True)
    outputs = model(images)
    loss = F.cross_entropy(outputs, labels)
    loss.backward()

    # åœ¨åŸå§‹åƒç´ ç©ºé—´è®¡ç®—æ‰°åŠ¨ï¼Œç„¶åè½¬æ¢å›æ ‡å‡†åŒ–ç©ºé—´
    eps_normalized = eps / std

    perturbation = eps_normalized * images.grad.sign()
    adv_images = images + perturbation

    # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
    lower_bound = (0 - mean) / std
    upper_bound = (1 - mean) / std
    adv_images = torch.max(torch.min(adv_images, upper_bound), lower_bound)

    return adv_images.detach()


def pgd_attack(
    model: nn.Module,
    images: torch.Tensor,
    labels: torch.Tensor,
    eps: float,
    alpha: float,
    steps: int,
    mean: torch.Tensor,
    std: torch.Tensor,
) -> torch.Tensor:
    """PGD (Projected Gradient Descent) å¯¹æŠ—æ”»å‡»

    å¤šæ­¥è¿­ä»£æ”»å‡»ï¼Œæ˜¯ FGSM çš„å¢å¼ºç‰ˆã€‚

    Args:
        model: ç›®æ ‡æ¨¡å‹
        images: è¾“å…¥å›¾åƒ (å·²æ ‡å‡†åŒ–)
        labels: çœŸå®æ ‡ç­¾
        eps: æœ€å¤§æ‰°åŠ¨å¼ºåº¦ Îµ (åœ¨åŸå§‹åƒç´ ç©ºé—´)
        alpha: æ¯æ­¥æ‰°åŠ¨å¤§å° Î± (åœ¨åŸå§‹åƒç´ ç©ºé—´)
        steps: è¿­ä»£æ­¥æ•°
        mean: æ ‡å‡†åŒ–å‡å€¼
        std: æ ‡å‡†åŒ–æ ‡å‡†å·®

    Returns:
        å¯¹æŠ—æ ·æœ¬ (å·²æ ‡å‡†åŒ–)
    """
    # è½¬æ¢åˆ°æ ‡å‡†åŒ–ç©ºé—´
    eps_normalized = eps / std
    alpha_normalized = alpha / std

    # æœ‰æ•ˆèŒƒå›´
    lower_bound = (0 - mean) / std
    upper_bound = (1 - mean) / std

    # éšæœºåˆå§‹åŒ–æ‰°åŠ¨
    adv_images = images.clone().detach()
    random_noise = torch.empty_like(adv_images).uniform_(-1, 1) * eps_normalized
    adv_images = adv_images + random_noise
    adv_images = torch.max(torch.min(adv_images, upper_bound), lower_bound)

    for _ in range(steps):
        adv_images.requires_grad_(True)
        outputs = model(adv_images)
        loss = F.cross_entropy(outputs, labels)

        model.zero_grad()
        loss.backward()

        # æ²¿æ¢¯åº¦æ–¹å‘æ›´æ–°
        grad_sign = adv_images.grad.sign()
        adv_images = adv_images.detach() + alpha_normalized * grad_sign

        # æŠ•å½±åˆ° Îµ-çƒå†…
        delta = adv_images - images
        delta = torch.max(torch.min(delta, eps_normalized), -eps_normalized)
        adv_images = images + delta

        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        adv_images = torch.max(torch.min(adv_images, upper_bound), lower_bound)

    return adv_images.detach()


def evaluate_adversarial(
    trainer_or_models: Any,
    test_loader: DataLoader,
    eps: float = 8 / 255,
    alpha: float = 2 / 255,
    pgd_steps: int = 10,
    dataset_name: str = "cifar10",
    logger: Optional[Any] = None,
) -> Dict[str, Any]:
    """å¯¹æŠ—é²æ£’æ€§è¯„ä¼° (FGSM/PGD å®æ—¶æ”»å‡»)

    ä½¿ç”¨ FGSM å’Œ PGD æ”»å‡»è¯„ä¼°é›†æˆæ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ã€‚
    æ”»å‡»é’ˆå¯¹é›†æˆæ¨¡å‹çš„å¹³å‡ logits è¿›è¡Œã€‚

    Args:
        trainer_or_models: StagedEnsembleTrainer å®ä¾‹æˆ– List[nn.Module]
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        eps: æ‰°åŠ¨å¼ºåº¦ Îµ (é»˜è®¤ 8/255 â‰ˆ 0.031)
        alpha: PGD æ­¥é•¿ Î± (é»˜è®¤ 2/255 â‰ˆ 0.008)
        pgd_steps: PGD è¿­ä»£æ­¥æ•° (é»˜è®¤ 10)
        dataset_name: æ•°æ®é›†åç§° (ç”¨äºè·å–æ ‡å‡†åŒ–å‚æ•°)
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        åŒ…å«å¯¹æŠ—é²æ£’æ€§æŒ‡æ ‡çš„å­—å…¸
    """
    from tqdm import tqdm

    from ..datasets import DATASET_REGISTRY

    logger = logger or get_logger()
    logger.info("\nğŸ—¡ï¸ Running Adversarial Robustness Evaluation")
    logger.info(f"   Îµ = {eps:.4f} ({eps * 255:.1f}/255)")
    logger.info(f"   PGD: Î± = {alpha:.4f}, steps = {pgd_steps}")

    models, device = extract_models(trainer_or_models)

    # è·å–æ•°æ®é›†çš„æ ‡å‡†åŒ–å‚æ•°
    if dataset_name.lower() in DATASET_REGISTRY:
        DatasetClass = DATASET_REGISTRY[dataset_name.lower()]
        mean = torch.tensor(DatasetClass.MEAN).view(1, 3, 1, 1).to(device)
        std = torch.tensor(DatasetClass.STD).view(1, 3, 1, 1).to(device)
    else:
        # é»˜è®¤ä½¿ç”¨ ImageNet æ ‡å‡†åŒ–å‚æ•°
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)

    # åˆ›å»ºä¸€ä¸ªåŒ…è£…æ¨¡å‹ï¼Œè¾“å‡ºé›†æˆçš„å¹³å‡ logits
    class EnsembleWrapper(nn.Module):
        def __init__(self, models_list):
            super().__init__()
            self.models = nn.ModuleList(models_list)

        def forward(self, x):
            logits_list = [m(x) for m in self.models]
            return torch.stack(logits_list).mean(dim=0)

    ensemble_model = EnsembleWrapper(models).to(device)
    ensemble_model.eval()

    # ç»Ÿè®¡å˜é‡
    clean_correct = 0
    fgsm_correct = 0
    pgd_correct = 0
    total = 0

    pbar = tqdm(test_loader, desc="Adversarial Eval", leave=False)

    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)
        batch_size = images.size(0)
        total += batch_size

        # å¹²å‡€æ ·æœ¬é¢„æµ‹
        with torch.no_grad():
            clean_outputs = ensemble_model(images)
            clean_preds = clean_outputs.argmax(dim=1)
            clean_correct += (clean_preds == labels).sum().item()

        # FGSM æ”»å‡»
        ensemble_model.train()  # éœ€è¦æ¢¯åº¦
        for m in ensemble_model.models:
            m.eval()  # ä½† BN ä¿æŒ eval æ¨¡å¼

        fgsm_images = fgsm_attack(ensemble_model, images, labels, eps, mean, std)

        with torch.no_grad():
            fgsm_outputs = ensemble_model(fgsm_images)
            fgsm_preds = fgsm_outputs.argmax(dim=1)
            fgsm_correct += (fgsm_preds == labels).sum().item()

        # PGD æ”»å‡»
        pgd_images = pgd_attack(
            ensemble_model, images, labels, eps, alpha, pgd_steps, mean, std
        )

        with torch.no_grad():
            pgd_outputs = ensemble_model(pgd_images)
            pgd_preds = pgd_outputs.argmax(dim=1)
            pgd_correct += (pgd_preds == labels).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix(
            {
                "clean": f"{100 * clean_correct / total:.1f}%",
                "fgsm": f"{100 * fgsm_correct / total:.1f}%",
                "pgd": f"{100 * pgd_correct / total:.1f}%",
            }
        )

    # æ¢å¤ eval æ¨¡å¼
    ensemble_model.eval()

    # è®¡ç®—æŒ‡æ ‡
    clean_acc = 100.0 * clean_correct / total
    fgsm_acc = 100.0 * fgsm_correct / total
    pgd_acc = 100.0 * pgd_correct / total

    results = {
        "clean_acc": clean_acc,
        "fgsm_acc": fgsm_acc,
        "pgd_acc": pgd_acc,
        "fgsm_attack_success_rate": 100.0 - fgsm_acc,
        "pgd_attack_success_rate": 100.0 - pgd_acc,
        "fgsm_robustness_drop": clean_acc - fgsm_acc,
        "pgd_robustness_drop": clean_acc - pgd_acc,
        "eps": eps,
        "eps_255": eps * 255,
        "alpha": alpha,
        "pgd_steps": pgd_steps,
        "num_samples": total,
    }

    logger.info("   âœ… Adversarial Robustness Results:")
    logger.info(f"      Clean Accuracy: {clean_acc:.2f}%")
    logger.info(f"      FGSM Accuracy (Îµ={eps * 255:.0f}/255): {fgsm_acc:.2f}%")
    logger.info(
        f"      PGD-{pgd_steps} Accuracy (Îµ={eps * 255:.0f}/255): {pgd_acc:.2f}%"
    )
    logger.info(f"      FGSM Robustness Drop: {clean_acc - fgsm_acc:.2f}%")
    logger.info(f"      PGD Robustness Drop: {clean_acc - pgd_acc:.2f}%")

    return results
