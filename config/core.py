"""
================================================================================
é…ç½®æ¨¡å—
================================================================================
"""

import datetime
import json
from dataclasses import asdict, dataclass, field, replace
from pathlib import Path
from typing import List, Optional

import torch

from ..utils import ensure_dir, get_logger


@dataclass
class Config:
    """ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ é›†æˆè®­ç»ƒé…ç½®"""

    # ==========================================================================
    # [å…¨å±€] æ•°æ®é…ç½® - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    data_root: str  # æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
    save_root: str  # æ£€æŸ¥ç‚¹/è¾“å‡ºä¿å­˜æ ¹ç›®å½•
    dataset_name: str  # æ•°æ®é›†åç§°: "cifar10", "cifar100", "eurosat" ç­‰
    val_split: float  # éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹ (0.0-1.0)
    test_split: float  # æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹ï¼Œç”¨äºæ— å®˜æ–¹åˆ’åˆ†çš„æ•°æ®é›†

    # ==========================================================================
    # [å…¨å±€] æ¨¡å‹é…ç½® - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    model_name: str  # æ¨¡å‹åç§°: "resnet18", "resnet50", "vgg16" ç­‰
    num_models_per_gpu: int  # æ¯ä¸ª GPU ä¸Šçš„æ¨¡å‹æ•°é‡
    compile_model: bool  # æ˜¯å¦å¯ç”¨ PyTorch 2.0+ ç¼–è¯‘ä¼˜åŒ– (å¯æå‡10-50%é€Ÿåº¦)

    # ==========================================================================
    # [å…¨å±€] è®­ç»ƒè¶…å‚æ•° - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    batch_size: int  # æ‰¹æ¬¡å¤§å°
    lr: float  # åŸºç¡€å­¦ä¹ ç‡
    weight_decay: float  # æƒé‡è¡°å‡ (L2 æ­£åˆ™åŒ–ç³»æ•°)
    max_grad_norm: float  # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    seed: int  # éšæœºç§å­
    optimizer: str  # ä¼˜åŒ–å™¨: "adamw", "sgd", "adam", "rmsprop"
    scheduler: str  # è°ƒåº¦å™¨: "cosine", "step", "plateau", "none"
    label_smoothing: float  # æ ‡ç­¾å¹³æ»‘ç³»æ•° (0.0=ä¸ä½¿ç”¨, 0.1=å¸¸ç”¨å€¼)

    # ==========================================================================
    # [é˜¶æ®µè®­ç»ƒä¸“ç”¨] ä¸‰é˜¶æ®µä¸ Mask - ä»… StagedEnsembleTrainer ä½¿ç”¨
    # ==========================================================================
    warmup_epochs: int  # Warmup é˜¶æ®µè½®æ•°
    progressive_epochs: int  # Progressive é˜¶æ®µè½®æ•°
    finetune_epochs: int  # Finetune é˜¶æ®µè½®æ•°
    mask_pool_size: int  # é¢„ç”Ÿæˆçš„ Mask æ± å¤§å°
    mask_start_ratio: float  # Progressive é˜¶æ®µèµ·å§‹é®ç½©æ¯”ä¾‹
    mask_end_ratio: float  # Progressive é˜¶æ®µç»“æŸé®ç½©æ¯”ä¾‹
    mask_prob_start: float  # Progressive é˜¶æ®µèµ·å§‹åº”ç”¨æ¦‚ç‡
    mask_prob_end: float  # Progressive é˜¶æ®µç»“æŸåº”ç”¨æ¦‚ç‡
    finetune_mask_ratio: float  # Finetune é˜¶æ®µå›ºå®šé®ç½©æ¯”ä¾‹
    finetune_mask_prob: float  # Finetune é˜¶æ®µå›ºå®šåº”ç”¨æ¦‚ç‡

    # ==========================================================================
    # [é˜¶æ®µè®­ç»ƒä¸“ç”¨] é˜¶æ®µå­¦ä¹ ç‡ç¼©æ”¾ - ä»… StagedEnsembleTrainer ä½¿ç”¨
    # ==========================================================================
    warmup_lr_scale: float  # Warmup é˜¶æ®µå­¦ä¹ ç‡ç¼©æ”¾å› å­ (lr * scale)
    progressive_lr_scale: float  # Progressive é˜¶æ®µå­¦ä¹ ç‡ç¼©æ”¾å› å­
    finetune_lr_scale: float  # Finetune é˜¶æ®µå­¦ä¹ ç‡ç¼©æ”¾å› å­

    # ==========================================================================
    # [å…¨å±€] æ•°æ®åŠ è½½é…ç½® - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    num_workers: int  # DataLoader å·¥ä½œè¿›ç¨‹æ•°
    pin_memory: bool  # æ˜¯å¦ä½¿ç”¨é”é¡µå†…å­˜åŠ é€Ÿ GPU ä¼ è¾“
    persistent_workers: bool  # æ˜¯å¦ä¿æŒå·¥ä½œè¿›ç¨‹å­˜æ´»
    prefetch_factor: int  # æ¯ä¸ª worker é¢„å–çš„æ‰¹æ¬¡æ•°

    # ==========================================================================
    # [å…¨å±€] è®­ç»ƒä¼˜åŒ–é…ç½® - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    use_amp: bool  # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)
    use_tf32: bool  # æ˜¯å¦å¯ç”¨ TF32 åŠ é€Ÿ (ä»… Ampere+ GPU)
    early_stopping_patience: int  # æ—©åœè€å¿ƒå€¼ (éªŒè¯é›†æ— æ”¹å–„çš„è½®æ•°)

    # ==========================================================================
    # [å…¨å±€] ä¿å­˜ä¸æ—¥å¿—é…ç½® - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    save_every_n_epochs: int  # æ¯ N è½®ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    keep_last_n_checkpoints: int  # ä¿ç•™æœ€è¿‘ N ä¸ªæ£€æŸ¥ç‚¹
    use_tensorboard: bool  # æ˜¯å¦å¯ç”¨ TensorBoard æ—¥å¿—
    log_level: str  # æ—¥å¿—çº§åˆ«: "DEBUG", "INFO", "WARNING", "ERROR"

    # ==========================================================================
    # [è¯„ä¼°ä¸“ç”¨] è¯„ä¼°é…ç½® - ä»…è¯„ä¼°æ¨¡å—ä½¿ç”¨
    # ==========================================================================
    ece_n_bins: int  # æ ¡å‡†åº¦ (ECE) è®¡ç®—çš„åˆ†ç®±æ•°é‡
    ensemble_strategy: str  # é›†æˆç­–ç•¥: "mean" (ç­‰æƒå¹³å‡), "voting" (å¤šæ•°æŠ•ç¥¨)
    corruption_dataset: bool  # æ˜¯å¦åŠ è½½ Corruption æ•°æ®é›†è¿›è¡Œè¯„ä¼°
    ood_dataset: bool  # æ˜¯å¦åŠ è½½ OOD æ•°æ®é›†è¿›è¡Œè¯„ä¼°
    domain_dataset: bool  # æ˜¯å¦åŠ è½½ Domain Shift æ•°æ®é›†è¿›è¡Œè¯„ä¼°

    # ==========================================================================
    # [è¯„ä¼°ä¸“ç”¨] å¯¹æŠ—é²æ£’æ€§è¯„ä¼°å‚æ•° - ä»…è¯„ä¼°æ¨¡å—ä½¿ç”¨
    # ==========================================================================
    adv_eps: float  # FGSM/PGD æ‰°åŠ¨å¼ºåº¦ Îµ (å¸¸ç”¨å€¼: 8/255 â‰ˆ 0.031)
    adv_alpha: float  # PGD æ­¥é•¿ Î± (å¸¸ç”¨å€¼: 2/255 â‰ˆ 0.008)
    adv_pgd_steps: int  # PGD è¿­ä»£æ­¥æ•° (å¸¸ç”¨å€¼: 10, 20)

    # ==========================================================================
    # [å…¨å±€] ä¼˜åŒ–å™¨é«˜çº§å‚æ•° - SGD ä¸“ç”¨
    # ==========================================================================
    sgd_momentum: float  # SGD åŠ¨é‡ (é»˜è®¤ 0.9)

    # ==========================================================================
    # [å¢å¼ºä¸“ç”¨] æ•°æ®å¢å¼ºå‚æ•° - Perlin/Cutout ä½¿ç”¨
    # ==========================================================================
    cutout_fill_value: float  # Cutout å¡«å……å€¼ (é»˜è®¤ 0.5)
    perlin_persistence: float  # Perlin å™ªå£°æŒä¹…åº¦ (é»˜è®¤ 0.5)

    # ==========================================================================
    # [è¯„ä¼°ä¸“ç”¨] å¯è§†åŒ–å‚æ•°
    # ==========================================================================
    plot_dpi: int  # å›¾è¡¨ä¿å­˜ DPI (é»˜è®¤ 150)

    # ==========================================================================
    # [å…¨å±€] æ¨¡å‹åˆå§‹åŒ– - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    init_method: str  # åˆå§‹åŒ–æ–¹æ³•: "kaiming", "xavier", "orthogonal", "default"

    # ==========================================================================
    # [å…¨å±€] è¿è¡Œæ§åˆ¶ - è¢« BaseTrainer åŠæ‰€æœ‰å­ç±»ä½¿ç”¨
    # ==========================================================================
    quick_test: bool  # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (å‡å°‘è½®æ•°/æ¨¡å‹æ•°)

    # ==========================================================================
    # [å®éªŒçº§åˆ«] å¢å¼ºä¸è¯¾ç¨‹å­¦ä¹ å‚æ•° - æ¯ä¸ªå®éªŒå¯è¦†ç›–
    # ==========================================================================
    augmentation_method: str  # å¢å¼ºæ–¹æ³•: "perlin", "cutout", "none" ç­‰
    use_curriculum: bool  # æ˜¯å¦ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ 
    fixed_ratio: float  # å›ºå®šé®æŒ¡æ¯”ä¾‹ (ä»… use_curriculum=False æ—¶ç”Ÿæ•ˆ)
    fixed_prob: float  # å›ºå®šé®æŒ¡æ¦‚ç‡ (ä»… use_curriculum=False æ—¶ç”Ÿæ•ˆ)
    share_warmup_backbone: bool  # æ˜¯å¦åœ¨ warmup åå…±äº« backbone

    # è‡ªåŠ¨è®¡ç®—/ç”Ÿæˆå­—æ®µ (æœ‰é»˜è®¤å€¼)
    save_dir: str = ""  # æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½• (ç”± __post_init__ è‡ªåŠ¨ç”Ÿæˆ)
    num_classes: int = 0
    image_size: int = 0
    gpu_ids: List[int] = field(
        default_factory=list, init=False
    )  # ç”± __post_init__ è‡ªåŠ¨è®¾ç½®
    experiment_name: str = ""

    @property
    def total_models(self) -> int:
        return len(self.gpu_ids) * self.num_models_per_gpu

    @property
    def total_epochs(self) -> int:
        return self.warmup_epochs + self.progressive_epochs + self.finetune_epochs

    def copy(self, **kwargs) -> "Config":
        """å…‹éš†é…ç½®å¹¶å¯é€‰åœ°è¦†ç›–å‚æ•°"""
        return replace(self, **kwargs)

    def apply_quick_test(self) -> "Config":
        """åº”ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼"""
        return replace(
            self,
            warmup_epochs=1,
            progressive_epochs=2,
            finetune_epochs=1,
            num_models_per_gpu=1,
        )

    def __post_init__(self) -> None:
        """åˆå§‹åŒ–éªŒè¯ä¸è‡ªåŠ¨é…ç½®"""
        available_gpus = torch.cuda.device_count()
        if available_gpus == 0:
            raise RuntimeError("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨GPU")

        self.gpu_ids = list(range(available_gpus))  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU

        self._auto_configure_for_dataset()

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_dir = str(
            Path(self.save_root) / f"{self.experiment_name or 'exp'}_{timestamp}"
        )
        ensure_dir(self.save_dir)

    def _auto_configure_for_dataset(self) -> None:
        """æ ¹æ®æ•°æ®é›†è‡ªåŠ¨é…ç½® num_classes å’Œ image_size"""
        from ..datasets import DATASET_REGISTRY

        dataset_name = self.dataset_name.lower()

        if dataset_name not in DATASET_REGISTRY:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†: {self.dataset_name}")

        DatasetClass = DATASET_REGISTRY[dataset_name]
        self.num_classes = self.num_classes or getattr(DatasetClass, "NUM_CLASSES", 10)
        self.image_size = self.image_size or getattr(DatasetClass, "IMAGE_SIZE", 32)

        # å¦‚æœéœ€è¦ config_overridesï¼Œå¯ä»¥åœ¨ DatasetClass ä¸­å®šä¹‰å®ƒ
        if hasattr(DatasetClass, "CONFIG_OVERRIDES"):
            for k, v in DatasetClass.CONFIG_OVERRIDES.items():
                setattr(self, k, v)

    def save(self, path: Optional[str] = None) -> None:
        """ä¿å­˜é…ç½®åˆ° JSON æ–‡ä»¶"""
        save_path = Path(path) if path else Path(self.save_dir) / "config.json"
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(asdict(self), f, indent=2, ensure_ascii=False)
        get_logger().info(f"ğŸ’¾ Config saved to: {save_path}")

    @classmethod
    def load_yaml(cls, yaml_path: str) -> tuple["Config", list["Experiment"], list]:
        """ä» YAML åŠ è½½å®Œæ•´ä»»åŠ¡é…ç½® (Config, experiments, eval_checkpoints)

        é…ç½®åˆå¹¶é¡ºåº: constants (ä¸šç•Œæ ‡å‡†) -> base (ç”¨æˆ·è‡ªå®šä¹‰)
        """
        import yaml

        with open(yaml_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)

        # åˆå¹¶ constants å’Œ baseï¼Œbase è¦†ç›– constants
        merged_cfg = {**data.get("constants", {}), **data.get("base", {})}
        base_cfg = cls(**merged_cfg)
        exps = [Experiment(**exp) for exp in data.get("experiments", [])]
        ckpts = data.get("eval_checkpoints", [])  # ä¿æŒç®€å•åˆ—è¡¨æˆ–æŒ‰éœ€åŒ…è£…

        return base_cfg, exps, ckpts


@dataclass
class Experiment:
    """å®éªŒé…ç½®

    å­—æ®µåä¸ Config ä¿æŒä¸€è‡´ï¼Œæ–¹ä¾¿ç›´æ¥ copy è¦†ç›–
    """

    name: str
    desc: str = ""
    # ä¸ Config åŒåçš„å­—æ®µï¼Œå¯ç›´æ¥è¦†ç›–
    augmentation_method: str = "perlin"
    use_curriculum: bool = True
    fixed_ratio: Optional[float] = None
    fixed_prob: Optional[float] = None

    def get_config_overrides(self) -> dict:
        """è·å–æ‰€æœ‰éœ€è¦è¦†ç›–çš„å‚æ•° (è¿‡æ»¤ name/desc å’Œ None å€¼)"""
        exclude = {"name", "desc"}
        return {
            k: v for k, v in asdict(self).items() if k not in exclude and v is not None
        }
