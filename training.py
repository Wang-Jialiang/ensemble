"""
================================================================================
è®­ç»ƒæ¨¡å— - æ•°æ®å¢å¼ºã€GPUWorkerã€ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨
================================================================================
"""

import logging
import random
import shutil
import sys
import time
from pathlib import Path
from typing import Callable, Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.amp import GradScaler, autocast
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from .config import Config
from .models import ModelFactory
from .utils import ensure_dir, format_duration, get_logger

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨å·¥å‚                                                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def create_optimizer(
    model: nn.Module, optimizer_name: str, lr: float, weight_decay: float
) -> optim.Optimizer:
    """
    åˆ›å»ºä¼˜åŒ–å™¨

    Args:
        model: æ¨¡å‹
        optimizer_name: ä¼˜åŒ–å™¨åç§° (adamw, sgd, adam, rmsprop)
        lr: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡

    Returns:
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹
    """
    optimizer_name = optimizer_name.lower()
    params = model.parameters()

    if optimizer_name == "adamw":
        return optim.AdamW(params, lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "adam":
        return optim.Adam(params, lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "sgd":
        return optim.SGD(params, lr=lr, weight_decay=weight_decay, momentum=0.9)
    elif optimizer_name == "rmsprop":
        return optim.RMSprop(params, lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(
            f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}. æ”¯æŒ: adamw, sgd, adam, rmsprop"
        )


def create_scheduler(
    optimizer: optim.Optimizer,
    scheduler_name: str,
    total_epochs: int,
    steps_per_epoch: int = 0,
) -> Optional[optim.lr_scheduler.LRScheduler]:
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨

    Args:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_name: è°ƒåº¦å™¨åç§° (cosine, step, plateau, onecycle, none)
        total_epochs: æ€»è®­ç»ƒè½®æ•°
        steps_per_epoch: æ¯è½®æ­¥æ•° (ç”¨äº OneCycleLR)

    Returns:
        scheduler: è°ƒåº¦å™¨å®ä¾‹ï¼Œnone æ—¶è¿”å› None
    """
    scheduler_name = scheduler_name.lower()

    if scheduler_name == "cosine":
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)
    elif scheduler_name == "step":
        # æ¯ 30% å’Œ 60% çš„ epoch æ—¶é™ä½å­¦ä¹ ç‡
        milestones = [int(total_epochs * 0.3), int(total_epochs * 0.6)]
        return optim.lr_scheduler.MultiStepLR(
            optimizer, milestones=milestones, gamma=0.1
        )
    elif scheduler_name == "plateau":
        return optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="min", factor=0.5, patience=5
        )
    elif scheduler_name == "onecycle":
        if steps_per_epoch <= 0:
            raise ValueError("OneCycleLR éœ€è¦ steps_per_epoch > 0")
        return optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=optimizer.param_groups[0]["lr"] * 10,
            total_steps=total_epochs * steps_per_epoch,
        )
    elif scheduler_name == "none":
        return None
    else:
        raise ValueError(
            f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨: {scheduler_name}. æ”¯æŒ: cosine, step, plateau, onecycle, none"
        )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ—©åœæœºåˆ¶                                                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EarlyStopping:
    """æ—©åœæœºåˆ¶

    ç”¨äºåœ¨éªŒè¯æŒ‡æ ‡ä¸å†æ”¹å–„æ—¶æå‰åœæ­¢è®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

    Args:
        patience: å…è®¸çš„æœ€å¤§æ— æ”¹å–„çš„epochæ•°
        min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
        mode: 'min' æˆ– 'max'ï¼ŒæŒ‡å®šæŒ‡æ ‡æ˜¯è¶Šå°è¶Šå¥½è¿˜æ˜¯è¶Šå¤§è¶Šå¥½
    """

    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = "min"):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0

    def __call__(self, score: float, epoch: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ

        Returns:
            True å¦‚æœåº”è¯¥åœæ­¢è®­ç»ƒï¼Œå¦åˆ™ False
        """
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            return False

        if self.mode == "min":
            improved = score < (self.best_score - self.min_delta)
        else:
            improved = score > (self.best_score + self.min_delta)

        if improved:
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True
            return False


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ äº‘çŠ¶Maskç”Ÿæˆå™¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CloudMaskGenerator:
    """GPUåŠ é€Ÿçš„äº‘çŠ¶Maskç”Ÿæˆå™¨"""

    def __init__(self, height: int, width: int, device: torch.device):
        self.h = height
        self.w = width
        self.device = device
        # base_scale éšå›¾åƒå°ºå¯¸åŠ¨æ€è°ƒæ•´: 32x32 -> 16, 64x64 -> 32
        self.base_scale = min(height, width) / 2.0

    def generate_batch(
        self, num_masks: int, target_ratio: float = 0.3
    ) -> List[torch.Tensor]:
        """æ‰¹é‡ç”ŸæˆPerlinå™ªå£°Mask"""
        masks = []
        for _ in range(num_masks):
            # åŠ¨æ€è°ƒæ•´ octaves å‚æ•°
            scale = self.base_scale * random.uniform(0.8, 1.2)
            octaves = 4 if self.h >= 64 else 3
            persistence = 0.5

            noise = self._generate_perlin_noise(scale, octaves, persistence)
            # ä½¿ç”¨target_ratioä½œä¸ºé˜ˆå€¼
            threshold = torch.quantile(noise, 1.0 - target_ratio)
            mask = (noise < threshold).float()
            mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
            masks.append(mask)

        return masks

    def _generate_perlin_noise(
        self, scale: float, octaves: int = 4, persistence: float = 0.5
    ) -> torch.Tensor:
        """ç”ŸæˆPerlinå™ªå£°"""
        noise = torch.zeros(self.h, self.w, device=self.device)
        amplitude = 1.0
        max_val = 0.0

        for i in range(octaves):
            freq = 2**i
            # ç¡®ä¿é¢‘ç‡ä¸ä¼šå¤ªé«˜å¯¼è‡´å°ºå¯¸ä¸º0
            grid_h = max(2, int(self.h / (scale / freq)))
            grid_w = max(2, int(self.w / (scale / freq)))

            rand_grid = torch.rand(grid_h + 1, grid_w + 1, device=self.device)

            # åŒçº¿æ€§æ’å€¼
            upsampled = F.interpolate(
                rand_grid.unsqueeze(0).unsqueeze(0),
                size=(self.h, self.w),
                mode="bilinear",
                align_corners=True,
            ).squeeze()

            noise += upsampled * amplitude
            max_val += amplitude
            amplitude *= persistence

        return noise / max_val


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ•°æ®å¢å¼ºæ–¹æ³•                                                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class AugmentationMethod:
    """æ•°æ®å¢å¼ºæ–¹æ³•åŸºç±»"""

    def __init__(self, device: torch.device):
        self.device = device

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """åº”ç”¨å¢å¼ºæ–¹æ³•"""
        raise NotImplementedError


class CutoutAugmentation(AugmentationMethod):
    """Cutoutç¡¬é®æŒ¡"""

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if random.random() > prob:
            return images, targets

        B, C, H, W = images.shape
        mask_size = int(H * np.sqrt(ratio))

        augmented = images.clone()
        for i in range(B):
            y = random.randint(0, max(0, H - mask_size))
            x = random.randint(0, max(0, W - mask_size))
            augmented[i, :, y : y + mask_size, x : x + mask_size] = 0.5

        return augmented, targets


class MixupAugmentation(AugmentationMethod):
    """Mixupæ··åˆå¢å¼º"""

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if random.random() > prob:
            return images, targets

        # è¾¹ç•Œæ£€æŸ¥é˜²æ­¢ beta åˆ†å¸ƒå‚æ•°æ— æ•ˆ
        ratio = np.clip(ratio, 0.01, 0.99)
        lam = np.random.beta(ratio * 10, (1 - ratio) * 10)
        lam = max(lam, 1 - lam)

        batch_size = images.size(0)
        index = torch.randperm(batch_size).to(self.device)

        mixed_images = lam * images + (1 - lam) * images[index]
        return mixed_images, targets


class CutMixAugmentation(AugmentationMethod):
    """CutMixå‰ªåˆ‡æ··åˆ"""

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if random.random() > prob:
            return images, targets

        B, C, H, W = images.shape

        lam = np.random.beta(1.0, 1.0)
        cut_rat = np.sqrt(1.0 - lam)
        cut_w = int(W * cut_rat)
        cut_h = int(H * cut_rat)

        cx = random.randint(0, W)
        cy = random.randint(0, H)

        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)

        index = torch.randperm(B).to(self.device)
        mixed_images = images.clone()
        mixed_images[:, :, bby1:bby2, bbx1:bbx2] = images[
            index, :, bby1:bby2, bbx1:bbx2
        ]

        return mixed_images, targets


class DropoutAugmentation(AugmentationMethod):
    """ç‰¹å¾çº§Dropout"""

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if random.random() > prob:
            return images, targets

        mask = torch.rand_like(images) > ratio
        augmented = images * mask.float()
        return augmented, targets


class PerlinMaskAugmentation(AugmentationMethod):
    """Perlinå™ªå£°é®æŒ¡ï¼ˆåŸæ–¹æ³•ï¼‰"""

    def __init__(
        self, device: torch.device, height: int, width: int, pool_size: int = 100
    ):
        super().__init__(device)
        self.mask_generator = CloudMaskGenerator(height, width, device)
        self.masks = []
        self.pool_size = pool_size

    def precompute_masks(self, target_ratio: float):
        """é¢„è®¡ç®—maskæ± """
        self.masks = self.mask_generator.generate_batch(self.pool_size, target_ratio)

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if random.random() > prob or not self.masks:
            return images, targets

        mask = self.masks[random.randint(0, len(self.masks) - 1)]
        if mask.shape[1] == 1:
            mask = mask.expand(1, 3, -1, -1)

        augmented = images * mask
        return augmented, targets


class NoAugmentation(AugmentationMethod):
    """æ— å¢å¼ºï¼ˆBaselineï¼‰"""

    def apply(
        self, images: torch.Tensor, targets: torch.Tensor, ratio: float, prob: float
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        return images, targets


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å¢å¼ºæ–¹æ³•æ³¨å†Œè¡¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AUGMENTATION_REGISTRY = {
    "cutout": lambda device, cfg: CutoutAugmentation(device),
    "mixup": lambda device, cfg: MixupAugmentation(device),
    "cutmix": lambda device, cfg: CutMixAugmentation(device),
    "dropout": lambda device, cfg: DropoutAugmentation(device),
    "perlin": lambda device, cfg: PerlinMaskAugmentation(
        device, cfg.image_size, cfg.image_size, cfg.mask_pool_size
    ),
    "none": lambda device, cfg: NoAugmentation(device),
}


def register_augmentation(name: str, builder: Callable):
    """åŠ¨æ€æ³¨å†Œå¢å¼ºæ–¹æ³•"""
    AUGMENTATION_REGISTRY[name] = builder


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ GPU Worker                                                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GPUWorker:
    """å•GPUæ¨¡å‹ç®¡ç†å™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    ç®¡ç†å•ä¸ªGPUä¸Šçš„å¤šä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¼‚æ­¥è®­ç»ƒä»¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ã€‚
    """

    def __init__(
        self,
        gpu_id: int,
        num_models: int,
        cfg: Config,
        augmentation_method: str = "perlin",
    ):
        self.gpu_id = gpu_id
        self.device = torch.device(f"cuda:{gpu_id}")
        self.cfg = cfg
        self.num_models = num_models

        # åˆ›å»ºæ¨¡å‹
        self.models: List[nn.Module] = []
        self.optimizers: List[optim.Optimizer] = []
        self.schedulers: List[optim.lr_scheduler.LRScheduler] = []

        for _ in range(num_models):
            model = ModelFactory.create_model(
                cfg.model_name,
                num_classes=cfg.num_classes,
                init_method=cfg.init_method,
            )
            model = model.to(self.device)

            if cfg.compile_model and hasattr(torch, "compile"):
                model = torch.compile(model)

            # ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
            optimizer = create_optimizer(model, cfg.optimizer, cfg.lr, cfg.weight_decay)
            scheduler = create_scheduler(optimizer, cfg.scheduler, cfg.total_epochs)

            self.models.append(model)
            self.optimizers.append(optimizer)
            self.schedulers.append(scheduler)

        # åˆ›å»ºå¢å¼ºæ–¹æ³•
        self.augmentation_method = augmentation_method
        self.augmentation = self._create_augmentation(augmentation_method)

        # AMP
        self.scaler = GradScaler("cuda") if cfg.use_amp else None

        # Stream
        self.stream = torch.cuda.Stream(device=self.device)
        self._pending_loss = None

    def _create_augmentation(self, method: str) -> AugmentationMethod:
        """åˆ›å»ºå¢å¼ºæ–¹æ³•"""
        if method not in AUGMENTATION_REGISTRY:
            raise ValueError(
                f"ä¸æ”¯æŒçš„å¢å¼ºæ–¹æ³•: {method}. æ”¯æŒ: {list(AUGMENTATION_REGISTRY.keys())}"
            )
        return AUGMENTATION_REGISTRY[method](self.device, self.cfg)

    def precompute_masks(self, num_masks: int, target_ratio: float):
        """é¢„è®¡ç®—maskï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if hasattr(self.augmentation, "precompute_masks"):
            self.augmentation.precompute_masks(target_ratio)

    def train_batch_async(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        criterion: nn.Module,
        mask_ratio: float,
        mask_prob: float,
        use_mask: bool,
    ):
        """å¼‚æ­¥è®­ç»ƒä¸€ä¸ªbatch"""
        with torch.cuda.stream(self.stream):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            total_loss = 0.0

            for model, optimizer in zip(self.models, self.optimizers):
                model.train()
                optimizer.zero_grad(set_to_none=True)

                # åº”ç”¨å¢å¼º
                if use_mask:
                    aug_inputs, aug_targets = self.augmentation.apply(
                        inputs, targets, mask_ratio, mask_prob
                    )
                else:
                    aug_inputs, aug_targets = inputs, targets

                # å‰å‘ä¼ æ’­
                if self.scaler:
                    with autocast("cuda"):
                        outputs = model(aug_inputs)
                        loss = criterion(outputs, aug_targets)
                    self.scaler.scale(loss).backward()
                    self.scaler.unscale_(optimizer)
                    nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)
                    self.scaler.step(optimizer)
                    self.scaler.update()
                else:
                    outputs = model(aug_inputs)
                    loss = criterion(outputs, aug_targets)
                    loss.backward()
                    nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)
                    optimizer.step()

                total_loss += loss.item()

            self._pending_loss = total_loss / self.num_models

    def synchronize(self) -> float:
        """åŒæ­¥å¹¶è¿”å›å¹³å‡loss"""
        self.stream.synchronize()
        return self._pending_loss if self._pending_loss else 0.0

    def step_schedulers(self, val_loss: Optional[float] = None):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨

        Args:
            val_loss: éªŒè¯æŸå¤± (ç”¨äº ReduceLROnPlateau)
        """
        for scheduler in self.schedulers:
            if scheduler is None:
                continue
            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                if val_loss is not None:
                    scheduler.step(val_loss)
            else:
                scheduler.step()

    def set_lr(self, lr: float):
        """è®¾ç½®æ‰€æœ‰æ¨¡å‹çš„å­¦ä¹ ç‡

        Args:
            lr: æ–°çš„å­¦ä¹ ç‡
        """
        for optimizer in self.optimizers:
            for param_group in optimizer.param_groups:
                param_group["lr"] = lr

    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizers[0].param_groups[0]["lr"] if self.optimizers else 0.0

    def predict_batch(self, inputs: torch.Tensor) -> torch.Tensor:
        """æ‰¹é‡é¢„æµ‹"""
        inputs = inputs.to(self.device)
        all_logits = []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                logits = model(inputs)
            all_logits.append(logits.unsqueeze(0))
        return torch.cat(all_logits, dim=0)

    def save_models(self, save_dir: str, prefix: str):
        """ä¿å­˜æ¨¡å‹"""
        for i, (model, optimizer, scheduler) in enumerate(
            zip(self.models, self.optimizers, self.schedulers)
        ):
            state = {
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict(),
            }
            save_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            torch.save(state, save_path)

    def load_models(self, save_dir: str, prefix: str):
        """åŠ è½½æ¨¡å‹"""
        for i, (model, optimizer, scheduler) in enumerate(
            zip(self.models, self.optimizers, self.schedulers)
        ):
            load_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            if load_path.exists():
                state = torch.load(
                    load_path, map_location=self.device, weights_only=False
                )
                model.load_state_dict(state["model_state_dict"])
                optimizer.load_state_dict(state["optimizer_state_dict"])
                scheduler.load_state_dict(state["scheduler_state_dict"])


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è®­ç»ƒå†å²ä¿å­˜å™¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class HistorySaver:
    """è®­ç»ƒå†å²ä¿å­˜å™¨"""

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)

    def save(self, history: Dict[str, List], filename: str = "history"):
        """ä¿å­˜è®­ç»ƒå†å²ä¸ºJSONå’ŒCSV"""
        import csv
        import json

        json_path = self.save_dir / f"{filename}.json"
        with open(json_path, "w") as f:
            json.dump(history, f, indent=2)

        csv_path = self.save_dir / f"{filename}.csv"
        with open(csv_path, "w", newline="") as f:
            if history:
                writer = csv.DictWriter(f, fieldnames=history.keys())
                writer.writeheader()
                for i in range(len(history[list(history.keys())[0]])):
                    row = {k: v[i] for k, v in history.items()}
                    writer.writerow(row)
        get_logger().info(f"ğŸ’¾ History saved to: {json_path}")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨                                                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class StagedEnsembleTrainer:
    """
    ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    å®ç°ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥è®­ç»ƒæ·±åº¦é›†æˆæ¨¡å‹:

    é˜¶æ®µåˆ’åˆ†:
        1. Warmupé˜¶æ®µ: æ— é®æŒ¡çƒ­èº«è®­ç»ƒï¼Œè®©æ¨¡å‹å­¦ä¹ åŸºç¡€ç‰¹å¾
        2. Progressiveé˜¶æ®µ: æ¸è¿›å¼å¢åŠ é®æŒ¡ï¼ŒåŸ¹å…»æ¨¡å‹å…³æ³¨ä¸åŒåŒºåŸŸ
        3. Finetuneé˜¶æ®µ: å›ºå®šé®æŒ¡æ¯”ä¾‹å¾®è°ƒï¼Œç¨³å®šæ¨¡å‹æ€§èƒ½
    """

    def __init__(
        self,
        method_name: str,
        cfg: Config,
        augmentation_method: str = "perlin",
        use_curriculum: bool = True,
        fixed_ratio: float = 0.25,
        fixed_prob: float = 0.5,
    ):
        self.name = method_name
        self.cfg = cfg
        self.total_training_time = 0.0

        # å¢å¼ºé…ç½®
        self.augmentation_method = augmentation_method
        self.use_curriculum = use_curriculum
        self.fixed_ratio = fixed_ratio
        self.fixed_prob = fixed_prob

        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if cfg.use_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

        get_logger().info(f"\nğŸš€ Initializing {method_name}")
        get_logger().info(f"   Augmentation: {augmentation_method}")
        get_logger().info(f"   Curriculum: {'Yes' if use_curriculum else 'No'}")
        get_logger().info(
            f"   Config: {cfg.total_models} {cfg.model_name} models across {len(cfg.gpu_ids)} GPUs"
        )

        # åˆ›å»ºWorkers
        self.workers: List[GPUWorker] = []
        for gpu_id in cfg.gpu_ids:
            worker = GPUWorker(gpu_id, cfg.num_models_per_gpu, cfg, augmentation_method)
            self.workers.append(worker)

        # æ—¥å¿—ç³»ç»Ÿ
        self.setup_logging()

        # TensorBoard
        self.writer = None
        if cfg.use_tensorboard:
            log_dir = Path(cfg.save_dir) / "tensorboard" / self.name
            self.writer = SummaryWriter(str(log_dir))
            get_logger().info(f"ğŸ“Š TensorBoard logging to: {log_dir}")

        # è®­ç»ƒå†å²
        self.history = {
            "epoch": [],
            "stage": [],
            "train_loss": [],
            "val_loss": [],
            "val_acc": [],
            "mask_ratio": [],
            "mask_prob": [],
            "lr": [],
            "epoch_time": [],
        }

        # æ—©åœ
        self.early_stopping = EarlyStopping(
            patience=cfg.early_stopping_patience, mode="min"
        )

        self.best_val_loss = float("inf")
        self.best_epoch = 0

        # æŒ‡æ ‡è®¡ç®—å™¨å’Œå†å²ä¿å­˜å™¨
        self.history_saver = HistorySaver(cfg.save_dir)

        # ä¿å­˜é…ç½®
        cfg.save()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.cfg.save_dir) / "logs"
        ensure_dir(log_dir)

        logger = logging.getLogger(self.name)
        logger.handlers.clear()
        logger.setLevel(getattr(logging, self.cfg.log_level))

        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

        file_handler = logging.FileHandler(log_dir / f"{self.name}_train.log", mode="w")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        self.logger = logger

    def _get_stage_info(self, epoch: int) -> Tuple[int, str, float, float, bool, float]:
        """è·å–å½“å‰é˜¶æ®µä¿¡æ¯

        Returns:
            Tuple: (stage_num, stage_name, mask_ratio, mask_prob, use_mask, lr_scale)
        """
        cfg = self.cfg

        # æ¨¡å¼1: æ— å¢å¼º (Baseline)
        if self.augmentation_method == "none":
            return 1, "NoAug", 0.0, 0.0, False, 1.0

        # æ¨¡å¼2: å›ºå®šå‚æ•°æ¨¡å¼
        if not self.use_curriculum:
            return 1, "Fixed", self.fixed_ratio, self.fixed_prob, True, 1.0

        # æ¨¡å¼3: è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ (ä¸‰é˜¶æ®µ)
        if epoch < cfg.warmup_epochs:
            return 1, "Warmup", 0.0, 0.0, False, cfg.warmup_lr_scale
        elif epoch < cfg.warmup_epochs + cfg.progressive_epochs:
            progress = (epoch - cfg.warmup_epochs) / cfg.progressive_epochs
            mask_ratio = (
                cfg.mask_start_ratio
                + (cfg.mask_end_ratio - cfg.mask_start_ratio) * progress
            )
            mask_prob = (
                cfg.mask_prob_start
                + (cfg.mask_prob_end - cfg.mask_prob_start) * progress
            )
            return (
                2,
                "Progressive",
                mask_ratio,
                mask_prob,
                True,
                cfg.progressive_lr_scale,
            )
        else:
            return (
                3,
                "Finetune",
                cfg.finetune_mask_ratio,
                cfg.finetune_mask_prob,
                True,
                cfg.finetune_lr_scale,
            )

    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        criterion = nn.CrossEntropyLoss(label_smoothing=self.cfg.label_smoothing)
        stage_num, stage_name, mask_ratio, mask_prob, use_mask, lr_scale = (
            self._get_stage_info(epoch)
        )

        # åº”ç”¨é˜¶æ®µå­¦ä¹ ç‡ç¼©æ”¾
        stage_lr = self.cfg.lr * lr_scale
        for worker in self.workers:
            worker.set_lr(stage_lr)

        # é¢„è®¡ç®—maskï¼ˆå¦‚æœéœ€è¦ï¼‰
        for worker in self.workers:
            worker.precompute_masks(self.cfg.mask_pool_size, mask_ratio)

        total_loss = 0.0
        num_batches = 0
        iterator = tqdm(
            train_loader,
            desc=f"Epoch {epoch + 1}/{self.cfg.total_epochs} [{stage_name}] lr={stage_lr:.6f}",
        )

        for inputs, targets in iterator:
            # å¼‚æ­¥è®­ç»ƒ
            for worker in self.workers:
                worker.train_batch_async(
                    inputs, targets, criterion, mask_ratio, mask_prob, use_mask
                )

            # åŒæ­¥å¹¶ç´¯è®¡loss
            batch_loss = 0.0
            for worker in self.workers:
                batch_loss += worker.synchronize()

            total_loss += batch_loss / len(self.workers)
            num_batches += 1

            iterator.set_postfix({"loss": total_loss / num_batches})

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆschedulerä¼šåŸºäºç¼©æ”¾åçš„lrç»§ç»­è°ƒæ•´ï¼‰
        for worker in self.workers:
            worker.step_schedulers()

        return total_loss / num_batches

    @torch.no_grad()
    def _validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯"""
        criterion = nn.CrossEntropyLoss()
        total_loss = 0.0
        correct = 0
        total = 0

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªworkerçš„è®¾å¤‡ä½œä¸ºä¸»è®¾å¤‡è¿›è¡Œè®¡ç®—
        primary_device = self.workers[0].device

        for inputs, targets in val_loader:
            all_logits = []
            for worker in self.workers:
                worker_logits = worker.predict_batch(inputs)
                all_logits.append(worker_logits.to(primary_device))

            all_logits = torch.cat(all_logits, dim=0)
            ensemble_logits = all_logits.mean(dim=0)

            # ç¡®ä¿targetsä¹Ÿåœ¨ä¸»è®¾å¤‡ä¸Š
            targets = targets.to(primary_device)

            loss = criterion(ensemble_logits, targets)
            total_loss += loss.item()

            preds = ensemble_logits.argmax(dim=1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)

        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒ"""
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸ“ Three-Stage Curriculum Learning: {self.name}")
        self.logger.info("=" * 70)

        current_stage = 0
        training_start_time = time.time()

        try:
            for epoch in range(self.cfg.total_epochs):
                epoch_start_time = time.time()
                stage_num, stage_name, mask_ratio, mask_prob, use_mask, lr_scale = (
                    self._get_stage_info(epoch)
                )

                # é˜¶æ®µåˆ‡æ¢æç¤º
                if stage_num != current_stage:
                    current_stage = stage_num
                    self.logger.info("")
                    self.logger.info("=" * 70)
                    if stage_num == 1:
                        self.logger.info("ğŸ”¥ STAGE 1: WARMUP (No Mask)")
                    elif stage_num == 2:
                        self.logger.info("ğŸ­ STAGE 2: PROGRESSIVE MASKING")
                    else:
                        self.logger.info("ğŸ¯ STAGE 3: FINE-TUNING")
                    self.logger.info("=" * 70)

                # è®­ç»ƒ
                try:
                    train_loss = self._train_epoch(train_loader, epoch)
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        self.logger.error("âŒ GPU Out of Memory!")
                        torch.cuda.empty_cache()
                        raise
                    else:
                        raise

                # éªŒè¯
                val_loss, val_acc = self._validate(val_loader)

                epoch_elapsed = time.time() - epoch_start_time
                current_lr = self.workers[0].optimizers[0].param_groups[0]["lr"]

                # è®°å½•å†å²
                self.history["epoch"].append(epoch + 1)
                self.history["stage"].append(stage_num)
                self.history["train_loss"].append(train_loss)
                self.history["val_loss"].append(val_loss)
                self.history["val_acc"].append(val_acc)
                self.history["mask_ratio"].append(mask_ratio)
                self.history["mask_prob"].append(mask_prob)
                self.history["lr"].append(current_lr)
                self.history["epoch_time"].append(epoch_elapsed)

                # TensorBoard
                if self.writer:
                    self.writer.add_scalar("Loss/train", train_loss, epoch)
                    self.writer.add_scalar("Loss/val", val_loss, epoch)
                    self.writer.add_scalar("Accuracy/val", val_acc, epoch)
                    self.writer.add_scalar("Hyperparameters/lr", current_lr, epoch)
                    self.writer.add_scalar(
                        "Time/epoch_duration_sec", epoch_elapsed, epoch
                    )

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self._save_checkpoint("best")
                    self.logger.info(f"   ğŸ† New best model! Val Loss: {val_loss:.4f}")

                # å®šæœŸä¿å­˜
                if (epoch + 1) % self.cfg.save_every_n_epochs == 0:
                    self._save_checkpoint(f"epoch_{epoch + 1}")
                    self._cleanup_old_checkpoints()

                # æ—¥å¿—
                mask_info = (
                    f"MaskR={mask_ratio:.1%}, MaskP={mask_prob:.1%}"
                    if use_mask
                    else "NoMask"
                )
                self.logger.info(
                    f"Epoch {epoch + 1:3d}/{self.cfg.total_epochs} [{stage_name:11s}] | "
                    f"TrainLoss: {train_loss:.4f} | ValLoss: {val_loss:.4f} | ValAcc: {val_acc:.2f}% | "
                    f"{mask_info} | LR: {current_lr:.6f} | Time: {epoch_elapsed:.1f}s"
                )

                # æ—©åœæ£€æŸ¥
                if self.early_stopping(val_loss, epoch):
                    self.logger.info(
                        f"\nâš ï¸ Early stopping triggered at epoch {epoch + 1}"
                    )
                    break

            self.total_training_time = time.time() - training_start_time
            self.logger.info(
                f"\nâ±ï¸ Total Training Time: {format_duration(self.total_training_time)}"
            )

            self._save_checkpoint("final")
            self.history_saver.save(self.history)
            self.logger.info(f"\nâœ… Training completed: {self.name}")

        except KeyboardInterrupt:
            self.logger.info("\nâš ï¸ Training interrupted by user")
            self.total_training_time = time.time() - training_start_time
            self._save_checkpoint("interrupted")
            self.history_saver.save(self.history)
            raise
        except Exception as e:
            self.logger.error(f"\nâŒ Training failed with error: {e}")
            self._save_checkpoint("error")
            self.history_saver.save(self.history)
            raise
        finally:
            if self.writer:
                self.writer.close()

    def _save_checkpoint(self, tag: str):
        """ä¿å­˜checkpoint"""
        checkpoint_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        ensure_dir(checkpoint_dir)

        for worker in self.workers:
            worker.save_models(str(checkpoint_dir), self.name)

        state = {
            "epoch": len(self.history["epoch"]),
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch,
            "history": self.history,
            "early_stopping_counter": self.early_stopping.counter,
            "total_training_time": self.total_training_time,
            "augmentation_method": self.augmentation_method,
            "use_curriculum": self.use_curriculum,
            "fixed_ratio": self.fixed_ratio,
            "fixed_prob": self.fixed_prob,
        }
        torch.save(state, checkpoint_dir / "trainer_state.pth")
        self.logger.info(f"ğŸ’¾ Saved checkpoint: {tag}")

    def load_checkpoint(self, tag: str = "best") -> bool:
        """åŠ è½½checkpoint"""
        checkpoint_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        if not checkpoint_dir.exists():
            self.logger.warning(f"âš ï¸ Checkpoint not found: {checkpoint_dir}")
            return False

        for worker in self.workers:
            worker.load_models(str(checkpoint_dir), self.name)

        state_path = checkpoint_dir / "trainer_state.pth"
        if state_path.exists():
            state = torch.load(state_path, weights_only=False)
            self.best_val_loss = state["best_val_loss"]
            self.best_epoch = state["best_epoch"]
            self.history = state["history"]
            self.early_stopping.counter = state.get("early_stopping_counter", 0)
            self.total_training_time = state.get("total_training_time", 0.0)
            self.augmentation_method = state.get(
                "augmentation_method", self.augmentation_method
            )
            self.use_curriculum = state.get("use_curriculum", self.use_curriculum)
            self.fixed_ratio = state.get("fixed_ratio", self.fixed_ratio)
            self.fixed_prob = state.get("fixed_prob", self.fixed_prob)
            self.logger.info(f"âœ… Loaded checkpoint: {tag}")
            self.logger.info(
                f"   Augmentation: {self.augmentation_method}, Curriculum: {self.use_curriculum}"
            )
            return True
        return False

    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§checkpoint"""
        checkpoint_base = Path(self.cfg.save_dir) / "checkpoints" / self.name
        if not checkpoint_base.exists():
            return

        epoch_dirs = [
            d for d in checkpoint_base.iterdir() if d.name.startswith("epoch_")
        ]
        epoch_dirs.sort(key=lambda x: int(x.name.split("_")[1]))

        if len(epoch_dirs) > self.cfg.keep_last_n_checkpoints:
            for old_dir in epoch_dirs[: -self.cfg.keep_last_n_checkpoints]:
                shutil.rmtree(old_dir)
                self.logger.info(f"ğŸ—‘ï¸ Removed old checkpoint: {old_dir.name}")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å®éªŒè¿è¡Œå‡½æ•° (ä» evaluation.py ç§»åŠ¨è¿‡æ¥)                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def train_experiment(
    experiment_name: str,
    cfg: Config,
    train_loader: DataLoader,
    val_loader: DataLoader,
    augmentation_method: Optional[str] = None,
    use_curriculum: Optional[bool] = None,
    fixed_ratio: Optional[float] = None,
    fixed_prob: Optional[float] = None,
    resume: Optional[str] = None,
) -> Tuple["StagedEnsembleTrainer", float]:
    """
    ä»…è®­ç»ƒå®éªŒ (ä¸åŒ…å«è¯„ä¼°)

    å‚æ•°:
        experiment_name: å®éªŒåç§°
        cfg: é…ç½®å¯¹è±¡
        train_loader, val_loader: æ•°æ®åŠ è½½å™¨
        augmentation_method: å¢å¼ºæ–¹æ³• (None=ä½¿ç”¨cfgé»˜è®¤)
        use_curriculum: æ˜¯å¦ä½¿ç”¨è¯¾ç¨‹å­¦ä¹  (None=ä½¿ç”¨cfgé»˜è®¤)
        fixed_ratio: å›ºå®šé®æŒ¡æ¯”ä¾‹ (ä»…åœ¨use_curriculum=Falseæ—¶ç”Ÿæ•ˆ)
        fixed_prob: å›ºå®šé®æŒ¡æ¦‚ç‡ (ä»…åœ¨use_curriculum=Falseæ—¶ç”Ÿæ•ˆ)
        resume: æ¢å¤checkpointçš„è·¯å¾„

    è¿”å›:
        (trainer, training_time)
    """
    aug_method = augmentation_method or ("perlin" if cfg.use_perlin_mask else "none")
    curriculum = use_curriculum if use_curriculum is not None else True
    f_ratio = fixed_ratio if fixed_ratio is not None else 0.25
    f_prob = fixed_prob if fixed_prob is not None else 0.5

    trainer = StagedEnsembleTrainer(
        experiment_name,
        cfg,
        augmentation_method=aug_method,
        use_curriculum=curriculum,
        fixed_ratio=f_ratio,
        fixed_prob=f_prob,
    )

    # æ¢å¤è®­ç»ƒ
    if resume:
        trainer.load_checkpoint(resume)

    # è®­ç»ƒ
    trainer.train(train_loader, val_loader)
    training_time = trainer.total_training_time

    # åŠ è½½æœ€ä½³æ¨¡å‹
    trainer.load_checkpoint("best")
    trainer.total_training_time = training_time

    get_logger().info(f"\nâœ… Training completed: {experiment_name}")
    get_logger().info(
        f"   Checkpoint saved to: {Path(cfg.save_dir) / 'checkpoints' / experiment_name}"
    )

    return trainer, training_time
