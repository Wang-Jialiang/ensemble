"""
================================================================================
æ•°æ®é›†æ¨¡å—
================================================================================
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ SSLè¯ä¹¦éªŒè¯ä¿®å¤ (å¯é€šè¿‡ç¯å¢ƒå˜é‡ DISABLE_SSL_VERIFY=1 å¯ç”¨)
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import os
import ssl
import tarfile
import time
import urllib.request
from pathlib import Path
from typing import List

import numpy as np
import torch
import torchvision
from tenacity import retry, stop_after_attempt, wait_fixed
from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset

from .utils import DEFAULT_DATA_ROOT, ensure_dir, get_logger

if os.environ.get("DISABLE_SSL_VERIFY", "0") == "1":
    ssl._create_default_https_context = ssl._create_unverified_context


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å…¨å±€å¸¸é‡å®šä¹‰
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 15ç§æ ‡å‡†Corruptionç±»å‹ (ä¸ImageNet-Cä¸€è‡´)
CORRUPTIONS = [
    "gaussian_noise",
    "shot_noise",
    "impulse_noise",
    "defocus_blur",
    "glass_blur",
    "motion_blur",
    "zoom_blur",
    "snow",
    "frost",
    "fog",
    "brightness",
    "contrast",
    "elastic_transform",
    "pixelate",
    "jpeg_compression",
]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ•°æ®é›†åŸºç±»
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class BasePreloadedDataset(Dataset):
    """å†…å­˜é¢„åŠ è½½æ•°æ®é›†çš„åŸºç±»

    å­ç±»éœ€è¦å®ç°:
        - _load_data(): åŠ è½½æ•°æ®åˆ° self.images å’Œ self.targets
        - _get_dataset_name(): è¿”å›æ•°æ®é›†åç§° (ç”¨äºæ—¥å¿—)

    å­ç±»åº”å½“è¦†ç›–ä»¥ä¸‹ç±»å±æ€§:
        - MEAN: æ ‡å‡†åŒ–å‡å€¼
        - STD: æ ‡å‡†åŒ–æ ‡å‡†å·®
        - IMAGE_SIZE: å›¾åƒå°ºå¯¸
        - NUM_CLASSES: ç±»åˆ«æ•°é‡
        - NAME: æ•°æ®é›†æ˜¾ç¤ºåç§°
    """

    # é»˜è®¤å…ƒæ•°æ® (å­ç±»éœ€è¦†ç›–)
    MEAN = [0.485, 0.456, 0.406]
    STD = [0.229, 0.224, 0.225]
    IMAGE_SIZE = 224
    NUM_CLASSES = 1000
    NAME = "Base"

    def __init__(self, root: str, train: bool):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        å‚æ•°:
            root: æ•°æ®é›†æ ¹ç›®å½•
            train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
        """
        self.root = root
        self.train = train
        self.images: torch.Tensor = None
        self.targets: torch.Tensor = None

        # é¢„è®¡ç®—æ ‡å‡†åŒ–å‚æ•°
        self._mean = torch.tensor(self.MEAN).view(3, 1, 1)
        self._std = torch.tensor(self.STD).view(3, 1, 1)

        # ä¸‹è½½å¹¶åŠ è½½æ•°æ®
        self._load_data()

    def _load_data(self):
        """åŠ è½½æ•°æ®åˆ° self.images å’Œ self.targets (å­ç±»å®ç°)"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° _load_data æ–¹æ³•")

    def _get_dataset_name(self) -> str:
        """è¿”å›æ•°æ®é›†åç§°"""
        return self.NAME

    def _log_loaded(self, elapsed: float):
        """æ‰“å°åŠ è½½å®Œæˆæ—¥å¿—"""
        mem_mb = self.images.numel() * self.images.element_size() / 1024 / 1024
        dataset_name = self._get_dataset_name()
        get_logger().info(
            f"âœ… Loaded {len(self)} {dataset_name} samples ({mem_mb:.1f} MB) in {elapsed:.2f}s"
        )

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        """è·å–å·²æ ‡å‡†åŒ–çš„å›¾åƒå’Œæ ‡ç­¾"""
        img = self.images[idx].float() / 255.0  # uint8 -> float [0-1]
        img = (img - self._mean) / self._std  # æ ‡å‡†åŒ–
        return img, self.targets[idx]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å…·ä½“æ•°æ®é›†å®ç°                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class PreloadedCIFAR10(BasePreloadedDataset):
    """å†…å­˜é¢„åŠ è½½çš„CIFAR-10æ•°æ®é›†"""

    MEAN = [0.4914, 0.4822, 0.4465]
    STD = [0.2023, 0.1994, 0.2010]
    IMAGE_SIZE = 32
    NUM_CLASSES = 10
    NAME = "CIFAR-10"

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(5), reraise=True)
    def _load_data(self):
        """åŠ è½½æ•°æ® (å¸¦é‡è¯•)"""
        try:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
            cifar_dir = Path(self.root) / "cifar-10-batches-py"
            should_download = not cifar_dir.exists()
            if should_download:
                get_logger().info("ğŸ“¥ CIFAR-10æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
            else:
                get_logger().info("âœ… CIFAR-10æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")

            base_dataset = torchvision.datasets.CIFAR10(
                root=self.root, train=self.train, download=should_download
            )
        except Exception as e:
            get_logger().error(f"âŒ CIFAR-10åŠ è½½å¤±è´¥: {e}")
            raise

        get_logger().info(
            f"ğŸ“¦ Preloading {'train' if self.train else 'test'} data to RAM..."
        )
        start = time.time()

        self.images = torch.from_numpy(base_dataset.data)
        self.images = self.images.permute(0, 3, 1, 2)
        self.targets = torch.tensor(base_dataset.targets, dtype=torch.long)

        self._log_loaded(time.time() - start)


class PreloadedEuroSAT(BasePreloadedDataset):
    """å†…å­˜é¢„åŠ è½½çš„EuroSATé¥æ„Ÿæ•°æ®é›†"""

    MEAN = [0.485, 0.456, 0.406]  # ImageNetæ ‡å‡†åŒ–
    STD = [0.229, 0.224, 0.225]
    IMAGE_SIZE = 64
    NUM_CLASSES = 10
    NAME = "EuroSAT"
    HAS_OFFICIAL_SPLIT = False  # æ²¡æœ‰å®˜æ–¹åˆ’åˆ†ï¼Œéœ€è¦æ‰‹åŠ¨åˆ’åˆ†

    def __init__(
        self,
        root: str,
        train: bool,
        test_split: float = 0.2,
        seed: int = 42,
    ):
        """
        åˆå§‹åŒ–EuroSATæ•°æ®é›†

        å‚æ•°:
            root: æ•°æ®é›†æ ¹ç›®å½•
            train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
            test_split: è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†æ¯”ä¾‹ (EuroSATæ²¡æœ‰å®˜æ–¹åˆ’åˆ†)
            seed: éšæœºç§å­
        """
        self.test_split = test_split
        self.seed = seed
        super().__init__(root, train)

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(5), reraise=True)
    def _load_data(self):
        """åŠ è½½æ•°æ® (å¸¦é‡è¯•)"""
        try:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
            eurosat_dir = Path(self.root) / "eurosat" / "2750"
            should_download = not eurosat_dir.exists()
            if should_download:
                get_logger().info("ğŸ“¥ EuroSATæ•°æ®é›†ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
            else:
                get_logger().info("âœ… EuroSATæ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")

            full_dataset = torchvision.datasets.EuroSAT(
                root=self.root, download=should_download
            )
        except Exception as e:
            get_logger().error(f"âŒ EuroSATåŠ è½½å¤±è´¥: {e}")
            raise

        get_logger().info(
            f"ğŸ“¡ Preloading {'train' if self.train else 'test'} data to RAM..."
        )
        start = time.time()

        # è·å–æ‰€æœ‰æ•°æ®
        all_images = []
        all_targets = []
        for img, target in full_dataset:
            # EuroSATå›¾åƒæ˜¯PIL Imageï¼Œè½¬æ¢ä¸ºnumpyå†è½¬tensor
            img_np = np.array(img)
            all_images.append(img_np)
            all_targets.append(target)

        all_images = np.stack(all_images, axis=0)  # (N, 64, 64, 3)
        all_targets = np.array(all_targets)

        # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†: ä½¿ç”¨éš”ç¦»çš„ RNG ä¿è¯å¯é‡å¤æ€§ä¸”ä¸å½±å“å…¨å±€çŠ¶æ€
        total_samples = len(all_images)
        rng = np.random.default_rng(self.seed)
        indices = rng.permutation(total_samples)

        test_size = int(total_samples * self.test_split)
        train_size = total_samples - test_size

        if self.train:
            selected_indices = indices[:train_size]
        else:
            selected_indices = indices[train_size:]

        # è½¬æ¢ä¸ºtensor
        self.images = torch.from_numpy(all_images[selected_indices])
        self.images = self.images.permute(0, 3, 1, 2)  # (N, 3, 64, 64)
        self.targets = torch.tensor(all_targets[selected_indices], dtype=torch.long)

        self._log_loaded(time.time() - start)


DATASET_REGISTRY = {
    "cifar10": PreloadedCIFAR10,
    "eurosat": PreloadedEuroSAT,
}


def register_dataset(name: str, dataset_class: type):
    """åŠ¨æ€æ³¨å†Œæ–°æ•°æ®é›†"""
    DATASET_REGISTRY[name] = dataset_class


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Corruptionæ•°æ®é›†
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CorruptionDataset:
    """Corruption è¯„ä¼°æ•°æ®é›† (ä»…æ”¯æŒé¢„ç”Ÿæˆæ¨¡å¼)

    ä»é¢„ç”Ÿæˆçš„ .npy æ–‡ä»¶åŠ è½½ corruption æ•°æ®ã€‚
    ä½¿ç”¨ `python -m my.generate_corruption` é¢„ç”Ÿæˆæ•°æ®ã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        >>> dataset = CorruptionDataset.from_name("cifar10", "./data")
        >>> dataset = CorruptionDataset.from_name("eurosat", "./data")
    """

    # å¼•ç”¨æ¨¡å—çº§å¸¸é‡
    CORRUPTIONS = CORRUPTIONS

    def __init__(self, name: str, data_dir: Path, mean: List[float], std: List[float]):
        """ç›´æ¥æ„é€ å‡½æ•°ï¼Œæ¨èä½¿ç”¨ from_name()"""
        labels_path = data_dir / "labels.npy"
        if not labels_path.exists():
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°é¢„ç”Ÿæˆæ•°æ®: {labels_path}\n"
                f"è¯·å…ˆè¿è¡Œ: python -m my.generate_corruption --dataset <name>"
            )

        self.name = name
        self.data_dir = data_dir
        self.labels = torch.from_numpy(np.load(str(labels_path))).long()
        self.mean = torch.tensor(mean).view(1, 3, 1, 1)
        self.std = torch.tensor(std).view(1, 3, 1, 1)
        self._cache = {}

    @property
    def num_samples(self) -> int:
        return len(self.labels)

    @classmethod
    def from_name(
        cls, dataset_name: str, root: str = DEFAULT_DATA_ROOT
    ) -> "CorruptionDataset":
        """ä» DATASET_REGISTRY è‡ªåŠ¨æ´¾ç”Ÿé…ç½®"""
        if dataset_name not in DATASET_REGISTRY:
            raise ValueError(
                f"æœªçŸ¥æ•°æ®é›†: {dataset_name}. å¯ç”¨: {list(DATASET_REGISTRY.keys())}"
            )

        DatasetClass = DATASET_REGISTRY[dataset_name]
        data_dir = Path(root) / f"{DatasetClass.NAME}-C"

        # CIFAR-10-C ç‰¹æ®Šå¤„ç†ï¼šå®˜æ–¹ä¸‹è½½
        if dataset_name == "cifar10" and not data_dir.exists():
            get_logger().info("ğŸ“¥ CIFAR-10-C ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
            cls._download_cifar10c(root)

        return cls(
            name=f"{DatasetClass.NAME}-C",
            data_dir=data_dir,
            mean=DatasetClass.MEAN,
            std=DatasetClass.STD,
        )

    def get_loader(
        self,
        corruption_type: str,
        severity: int = 5,
        batch_size: int = 128,
        num_workers: int = 4,
    ) -> DataLoader:
        """è·å–ç‰¹å®šæŸåç±»å‹å’Œä¸¥é‡ç¨‹åº¦çš„æ•°æ®åŠ è½½å™¨"""
        cache_key = (corruption_type, severity)

        if cache_key not in self._cache:
            self._cache[cache_key] = self._load_corruption(corruption_type, severity)

        dataset = TensorDataset(self._cache[cache_key], self.labels)
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
        )

    def _load_corruption(self, corruption_type: str, severity: int) -> torch.Tensor:
        """ä»é¢„ç”Ÿæˆæ–‡ä»¶åŠ è½½"""
        file_path = self.data_dir / f"{corruption_type}.npy"
        if not file_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ° corruption æ–‡ä»¶: {file_path}")

        data = np.load(str(file_path))
        n_samples = len(self.labels)
        images = data[(severity - 1) * n_samples : severity * n_samples]

        images_tensor = torch.from_numpy(images).permute(0, 3, 1, 2).float() / 255.0
        return (images_tensor - self.mean) / self.std

    @staticmethod
    def _download_cifar10c(root: str):
        """ä¸‹è½½ CIFAR-10-C æ•°æ®é›†"""
        url = "https://zenodo.org/record/2535967/files/CIFAR-10-C.tar"
        tar_path = Path(root) / "CIFAR-10-C.tar"
        ensure_dir(root)

        get_logger().info(f"ğŸ“¥ Downloading CIFAR-10-C from {url}...")
        urllib.request.urlretrieve(url, str(tar_path))

        get_logger().info(f"ğŸ“¦ Extracting to {root}...")
        with tarfile.open(str(tar_path), "r") as tar:
            tar.extractall(str(root))

        tar_path.unlink()
        get_logger().info("âœ… CIFAR-10-C download complete!")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ•°æ®é›†åŠ è½½å‡½æ•°                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_dataset(cfg):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†

    å‚æ•°:
        cfg: é…ç½®å¯¹è±¡

    è¿”å›:
        train_loader, val_loader, test_loader, corruption_dataset
    """
    dataset_name = cfg.dataset_name.lower()

    if dataset_name not in DATASET_REGISTRY:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}. æ”¯æŒ: {list(DATASET_REGISTRY.keys())}"
        )

    DatasetClass = DATASET_REGISTRY[dataset_name]

    # ä¸ºæ²¡æœ‰å®˜æ–¹åˆ’åˆ†çš„æ•°æ®é›†ä¼ é€’é¢å¤–å‚æ•°
    extra_kwargs = {}
    if not getattr(DatasetClass, "HAS_OFFICIAL_SPLIT", True):
        extra_kwargs["test_split"] = cfg.test_split

    # åˆ›å»ºå®Œæ•´è®­ç»ƒé›† (ç”¨äºåˆ’åˆ†)
    train_full = DatasetClass(root=cfg.data_root, train=True, **extra_kwargs)
    test_dataset = DatasetClass(root=cfg.data_root, train=False, **extra_kwargs)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_train = len(train_full)
    val_size = int(total_train * cfg.val_split)
    train_size = total_train - val_size

    generator = torch.Generator().manual_seed(cfg.seed)
    indices = torch.randperm(total_train, generator=generator)
    train_indices = indices[:train_size].tolist()
    val_indices = indices[train_size:].tolist()

    # ä½¿ç”¨ PyTorch å†…ç½® Subset
    train_subset = Subset(train_full, train_indices)
    val_subset = Subset(train_full, val_indices)

    # åˆ›å»ºDataLoader
    common_loader_kwargs = {
        "num_workers": cfg.num_workers,
        "pin_memory": cfg.pin_memory,
        "persistent_workers": cfg.persistent_workers and cfg.num_workers > 0,
        "prefetch_factor": cfg.prefetch_factor if cfg.num_workers > 0 else None,
    }

    train_loader = DataLoader(
        train_subset, batch_size=cfg.batch_size, shuffle=True, **common_loader_kwargs
    )
    val_loader = DataLoader(
        val_subset, batch_size=cfg.batch_size * 2, shuffle=False, **common_loader_kwargs
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=cfg.batch_size * 2,
        shuffle=False,
        **common_loader_kwargs,
    )

    get_logger().info(f"ğŸ“Š æ•°æ®é›†: {dataset_name.upper()}")
    get_logger().info(
        f"   è®­ç»ƒé›†: {len(train_subset)} | éªŒè¯é›†: {len(val_subset)} | æµ‹è¯•é›†: {len(test_dataset)}"
    )

    # åŠ è½½Corruptionæ•°æ®é›† (ä»»ä½•åœ¨ DATASET_REGISTRY ä¸­çš„æ•°æ®é›†éƒ½æ”¯æŒ)
    corruption_dataset = None
    try:
        corruption_dataset = CorruptionDataset.from_name(dataset_name, cfg.data_root)
        get_logger().info(f"   Corruptionæ•°æ®é›†: {corruption_dataset.name}")
    except FileNotFoundError as e:
        get_logger().warning(f"   âš ï¸ Corruptionæ•°æ®é›†æœªæ‰¾åˆ°: {e}")

    return train_loader, val_loader, test_loader, corruption_dataset
