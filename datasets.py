"""
================================================================================
æ•°æ®é›†æ¨¡å—
================================================================================
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ SSLè¯ä¹¦éªŒè¯ä¿®å¤ (å¯é€šè¿‡ç¯å¢ƒå˜é‡ DISABLE_SSL_VERIFY=1 å¯ç”¨)
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import os
import ssl
import tarfile
import time
import urllib.request
from pathlib import Path
from typing import List

import numpy as np
import torch
import torchvision
from tenacity import retry, stop_after_attempt, wait_fixed
from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset

from .utils import DEFAULT_DATA_ROOT, ensure_dir, get_logger

if os.environ.get("DISABLE_SSL_VERIFY", "0") == "1":
    ssl._create_default_https_context = ssl._create_unverified_context


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å…¨å±€å¸¸é‡å®šä¹‰
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 15ç§æ ‡å‡†Corruptionç±»å‹ (ä¸ImageNet-Cä¸€è‡´)
CORRUPTIONS = [
    "gaussian_noise",
    "shot_noise",
    "impulse_noise",
    "defocus_blur",
    "glass_blur",
    "motion_blur",
    "zoom_blur",
    "snow",
    "frost",
    "fog",
    "brightness",
    "contrast",
    "elastic_transform",
    "pixelate",
    "jpeg_compression",
]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ•°æ®é›†åŸºç±»
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class BasePreloadedDataset(Dataset):
    """å†…å­˜é¢„åŠ è½½æ•°æ®é›†çš„åŸºç±»

    å­ç±»éœ€è¦å®ç°:
        - _load_data(): åŠ è½½æ•°æ®åˆ° self.images å’Œ self.targets
        - _get_dataset_name(): è¿”å›æ•°æ®é›†åç§° (ç”¨äºæ—¥å¿—)

    å­ç±»åº”å½“è¦†ç›–ä»¥ä¸‹ç±»å±æ€§:
        - MEAN: æ ‡å‡†åŒ–å‡å€¼
        - STD: æ ‡å‡†åŒ–æ ‡å‡†å·®
        - IMAGE_SIZE: å›¾åƒå°ºå¯¸
        - NUM_CLASSES: ç±»åˆ«æ•°é‡
        - NAME: æ•°æ®é›†æ˜¾ç¤ºåç§°
    """

    # é»˜è®¤å…ƒæ•°æ® (å­ç±»éœ€è¦†ç›–)
    MEAN = [0.485, 0.456, 0.406]
    STD = [0.229, 0.224, 0.225]
    IMAGE_SIZE = 224
    NUM_CLASSES = 1000
    NAME = "Base"

    def __init__(self, root: str, train: bool):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        å‚æ•°:
            root: æ•°æ®é›†æ ¹ç›®å½•
            train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
        """
        self.root = root
        self.train = train
        self.images: torch.Tensor = None
        self.targets: torch.Tensor = None

        # é¢„è®¡ç®—æ ‡å‡†åŒ–å‚æ•°
        self._mean = torch.tensor(self.MEAN).view(3, 1, 1)
        self._std = torch.tensor(self.STD).view(3, 1, 1)

        # ä¸‹è½½å¹¶åŠ è½½æ•°æ®
        self._load_data()

    def _load_data(self):
        """åŠ è½½æ•°æ®åˆ° self.images å’Œ self.targets (å­ç±»å®ç°)"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° _load_data æ–¹æ³•")

    def _get_dataset_name(self) -> str:
        """è¿”å›æ•°æ®é›†åç§°"""
        return self.NAME

    def _log_loaded(self, elapsed: float):
        """æ‰“å°åŠ è½½å®Œæˆæ—¥å¿—"""
        mem_mb = self.images.numel() * self.images.element_size() / 1024 / 1024
        dataset_name = self._get_dataset_name()
        get_logger().info(
            f"âœ… Loaded {len(self)} {dataset_name} samples ({mem_mb:.1f} MB) in {elapsed:.2f}s"
        )

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        """è·å–å·²æ ‡å‡†åŒ–çš„å›¾åƒå’Œæ ‡ç­¾"""
        img = self.images[idx].float() / 255.0  # uint8 -> float [0-1]
        img = (img - self._mean) / self._std  # æ ‡å‡†åŒ–
        return img, self.targets[idx]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å…·ä½“æ•°æ®é›†å®ç°                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class PreloadedCIFAR10(BasePreloadedDataset):
    """å†…å­˜é¢„åŠ è½½çš„CIFAR-10æ•°æ®é›†"""

    MEAN = [0.4914, 0.4822, 0.4465]
    STD = [0.2023, 0.1994, 0.2010]
    IMAGE_SIZE = 32
    NUM_CLASSES = 10
    NAME = "CIFAR-10"

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(5), reraise=True)
    def _load_data(self):
        """åŠ è½½æ•°æ® (å¸¦é‡è¯•)"""
        try:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
            cifar_dir = Path(self.root) / "cifar-10-batches-py"
            should_download = not cifar_dir.exists()
            if should_download:
                get_logger().info("ğŸ“¥ CIFAR-10æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
            else:
                get_logger().info("âœ… CIFAR-10æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")

            base_dataset = torchvision.datasets.CIFAR10(
                root=self.root, train=self.train, download=should_download
            )
        except Exception as e:
            get_logger().error(f"âŒ CIFAR-10åŠ è½½å¤±è´¥: {e}")
            raise

        get_logger().info(
            f"ğŸ“¦ Preloading {'train' if self.train else 'test'} data to RAM..."
        )
        start = time.time()

        self.images = torch.from_numpy(base_dataset.data)
        self.images = self.images.permute(0, 3, 1, 2)
        self.targets = torch.tensor(base_dataset.targets, dtype=torch.long)

        self._log_loaded(time.time() - start)


class PreloadedEuroSAT(BasePreloadedDataset):
    """å†…å­˜é¢„åŠ è½½çš„EuroSATé¥æ„Ÿæ•°æ®é›†"""

    MEAN = [0.485, 0.456, 0.406]  # ImageNetæ ‡å‡†åŒ–
    STD = [0.229, 0.224, 0.225]
    IMAGE_SIZE = 64
    NUM_CLASSES = 10
    NAME = "EuroSAT"
    HAS_OFFICIAL_SPLIT = False  # æ²¡æœ‰å®˜æ–¹åˆ’åˆ†ï¼Œéœ€è¦æ‰‹åŠ¨åˆ’åˆ†

    def __init__(
        self,
        root: str,
        train: bool,
        test_split: float = 0.2,
        seed: int = 42,
    ):
        """
        åˆå§‹åŒ–EuroSATæ•°æ®é›†

        å‚æ•°:
            root: æ•°æ®é›†æ ¹ç›®å½•
            train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
            test_split: è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†æ¯”ä¾‹ (EuroSATæ²¡æœ‰å®˜æ–¹åˆ’åˆ†)
            seed: éšæœºç§å­
        """
        self.test_split = test_split
        self.seed = seed
        super().__init__(root, train)

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(5), reraise=True)
    def _load_data(self):
        """åŠ è½½æ•°æ® (å¸¦é‡è¯•)"""
        try:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
            eurosat_dir = Path(self.root) / "eurosat" / "2750"
            should_download = not eurosat_dir.exists()
            if should_download:
                get_logger().info("ğŸ“¥ EuroSATæ•°æ®é›†ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
            else:
                get_logger().info("âœ… EuroSATæ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")

            full_dataset = torchvision.datasets.EuroSAT(
                root=self.root, download=should_download
            )
        except Exception as e:
            get_logger().error(f"âŒ EuroSATåŠ è½½å¤±è´¥: {e}")
            raise

        get_logger().info(
            f"ğŸ“¡ Preloading {'train' if self.train else 'test'} data to RAM..."
        )
        start = time.time()

        # è·å–æ‰€æœ‰æ•°æ®
        all_images = []
        all_targets = []
        for img, target in full_dataset:
            # EuroSATå›¾åƒæ˜¯PIL Imageï¼Œè½¬æ¢ä¸ºnumpyå†è½¬tensor
            img_np = np.array(img)
            all_images.append(img_np)
            all_targets.append(target)

        all_images = np.stack(all_images, axis=0)  # (N, 64, 64, 3)
        all_targets = np.array(all_targets)

        # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†: ä½¿ç”¨éš”ç¦»çš„ RNG ä¿è¯å¯é‡å¤æ€§ä¸”ä¸å½±å“å…¨å±€çŠ¶æ€
        total_samples = len(all_images)
        rng = np.random.default_rng(self.seed)
        indices = rng.permutation(total_samples)

        test_size = int(total_samples * self.test_split)
        train_size = total_samples - test_size

        if self.train:
            selected_indices = indices[:train_size]
        else:
            selected_indices = indices[train_size:]

        # è½¬æ¢ä¸ºtensor
        self.images = torch.from_numpy(all_images[selected_indices])
        self.images = self.images.permute(0, 3, 1, 2)  # (N, 3, 64, 64)
        self.targets = torch.tensor(all_targets[selected_indices], dtype=torch.long)

        self._log_loaded(time.time() - start)


DATASET_REGISTRY = {
    "cifar10": PreloadedCIFAR10,
    "eurosat": PreloadedEuroSAT,
}


def register_dataset(name: str, dataset_class: type):
    """åŠ¨æ€æ³¨å†Œæ–°æ•°æ®é›†"""
    DATASET_REGISTRY[name] = dataset_class


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Corruptionæ•°æ®é›†
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CorruptionDataset:
    """Corruption è¯„ä¼°æ•°æ®é›† (ä»…æ”¯æŒé¢„ç”Ÿæˆæ¨¡å¼)

    ä»é¢„ç”Ÿæˆçš„ .npy æ–‡ä»¶åŠ è½½ corruption æ•°æ®ã€‚
    ä½¿ç”¨ `python -m my.generate_corruption` é¢„ç”Ÿæˆæ•°æ®ã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        >>> dataset = CorruptionDataset.from_name("cifar10", "./data")
        >>> dataset = CorruptionDataset.from_name("eurosat", "./data")
    """

    # å¼•ç”¨æ¨¡å—çº§å¸¸é‡
    CORRUPTIONS = CORRUPTIONS

    def __init__(self, name: str, data_dir: Path, mean: List[float], std: List[float]):
        """ç›´æ¥æ„é€ å‡½æ•°ï¼Œæ¨èä½¿ç”¨ from_name()"""
        labels_path = data_dir / "labels.npy"
        if not labels_path.exists():
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°é¢„ç”Ÿæˆæ•°æ®: {labels_path}\n"
                f"è¯·å…ˆè¿è¡Œ: python -m my.generate_corruption --dataset <name>"
            )

        self.name = name
        self.data_dir = data_dir
        self.labels = torch.from_numpy(np.load(str(labels_path))).long()
        self.mean = torch.tensor(mean).view(1, 3, 1, 1)
        self.std = torch.tensor(std).view(1, 3, 1, 1)
        self._cache = {}

    @property
    def num_samples(self) -> int:
        return len(self.labels)

    @classmethod
    def from_name(
        cls, dataset_name: str, root: str = DEFAULT_DATA_ROOT
    ) -> "CorruptionDataset":
        """ä» DATASET_REGISTRY è‡ªåŠ¨æ´¾ç”Ÿé…ç½®"""
        if dataset_name not in DATASET_REGISTRY:
            raise ValueError(
                f"æœªçŸ¥æ•°æ®é›†: {dataset_name}. å¯ç”¨: {list(DATASET_REGISTRY.keys())}"
            )

        DatasetClass = DATASET_REGISTRY[dataset_name]
        data_dir = Path(root) / f"{DatasetClass.NAME}-C"

        # CIFAR-10-C ç‰¹æ®Šå¤„ç†ï¼šå®˜æ–¹ä¸‹è½½
        if dataset_name == "cifar10" and not data_dir.exists():
            get_logger().info("ğŸ“¥ CIFAR-10-C ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
            cls._download_cifar10c(root)

        return cls(
            name=f"{DatasetClass.NAME}-C",
            data_dir=data_dir,
            mean=DatasetClass.MEAN,
            std=DatasetClass.STD,
        )

    def get_loader(
        self,
        corruption_type: str,
        severity: int = 5,
        batch_size: int = 128,
        num_workers: int = 4,
    ) -> DataLoader:
        """è·å–ç‰¹å®šæŸåç±»å‹å’Œä¸¥é‡ç¨‹åº¦çš„æ•°æ®åŠ è½½å™¨"""
        cache_key = (corruption_type, severity)

        if cache_key not in self._cache:
            self._cache[cache_key] = self._load_corruption(corruption_type, severity)

        dataset = TensorDataset(self._cache[cache_key], self.labels)
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
        )

    def _load_corruption(self, corruption_type: str, severity: int) -> torch.Tensor:
        """ä»é¢„ç”Ÿæˆæ–‡ä»¶åŠ è½½"""
        file_path = self.data_dir / f"{corruption_type}.npy"
        if not file_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ° corruption æ–‡ä»¶: {file_path}")

        data = np.load(str(file_path))
        n_samples = len(self.labels)
        images = data[(severity - 1) * n_samples : severity * n_samples]

        images_tensor = torch.from_numpy(images).permute(0, 3, 1, 2).float() / 255.0
        return (images_tensor - self.mean) / self.std

    @staticmethod
    def _download_cifar10c(root: str):
        """ä¸‹è½½ CIFAR-10-C æ•°æ®é›†"""
        url = "https://zenodo.org/record/2535967/files/CIFAR-10-C.tar"
        tar_path = Path(root) / "CIFAR-10-C.tar"
        ensure_dir(root)

        get_logger().info(f"ğŸ“¥ Downloading CIFAR-10-C from {url}...")
        urllib.request.urlretrieve(url, str(tar_path))

        get_logger().info(f"ğŸ“¦ Extracting to {root}...")
        with tarfile.open(str(tar_path), "r") as tar:
            tar.extractall(str(root))

        tar_path.unlink()
        get_logger().info("âœ… CIFAR-10-C download complete!")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ OOD æ•°æ®é›†                                                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class OODDataset:
    """OOD (Out-of-Distribution) è¯„ä¼°æ•°æ®é›†

    ç”¨äºè¯„ä¼°æ¨¡å‹çš„ OOD æ£€æµ‹èƒ½åŠ›ï¼Œæ”¯æŒå¤šç§ OOD æ•°æ®é›†ã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        >>> ood_dataset = OODDataset.from_name("svhn", id_dataset="cifar10", root="./data")
        >>> loader = ood_dataset.get_loader(batch_size=128)
    """

    # é¢„å®šä¹‰çš„ OOD æ•°æ®é›†é…ç½®
    OOD_CONFIGS = {
        "svhn": {
            "name": "SVHN",
            "loader": lambda root: torchvision.datasets.SVHN(
                root=root, split="test", download=True
            ),
            "image_size": 32,
            "compatible_with": ["cifar10"],  # é€‚åˆä½œä¸ºå“ªäº›IDæ•°æ®é›†çš„OOD
        },
        "textures": {
            "name": "Textures (DTD)",
            "loader": lambda root: torchvision.datasets.DTD(
                root=root, split="test", download=True
            ),
            "image_size": None,  # éœ€è¦resize
            "compatible_with": ["cifar10", "eurosat"],
        },
    }

    def __init__(
        self,
        name: str,
        images: torch.Tensor,
        mean: List[float],
        std: List[float],
    ):
        """ç›´æ¥æ„é€ å‡½æ•°ï¼Œæ¨èä½¿ç”¨ from_name()"""
        self.name = name
        self.images = images  # [N, C, H, W], uint8
        self._mean = torch.tensor(mean).view(1, 3, 1, 1)
        self._std = torch.tensor(std).view(1, 3, 1, 1)

    @property
    def num_samples(self) -> int:
        return len(self.images)

    @classmethod
    def from_name(
        cls,
        ood_name: str,
        id_dataset: str,
        root: str = DEFAULT_DATA_ROOT,
    ) -> "OODDataset":
        """æ ¹æ®åç§°åŠ è½½ OOD æ•°æ®é›†

        Args:
            ood_name: OOD æ•°æ®é›†åç§° (svhn, textures ç­‰)
            id_dataset: ID æ•°æ®é›†åç§° (cifar10, eurosat)ï¼Œç”¨äºç¡®å®šæ ‡å‡†åŒ–å‚æ•°
            root: æ•°æ®æ ¹ç›®å½•

        Returns:
            OODDataset å®ä¾‹
        """
        if ood_name not in cls.OOD_CONFIGS:
            raise ValueError(
                f"æœªçŸ¥ OOD æ•°æ®é›†: {ood_name}. å¯ç”¨: {list(cls.OOD_CONFIGS.keys())}"
            )

        if id_dataset not in DATASET_REGISTRY:
            raise ValueError(
                f"æœªçŸ¥ ID æ•°æ®é›†: {id_dataset}. å¯ç”¨: {list(DATASET_REGISTRY.keys())}"
            )

        ood_config = cls.OOD_CONFIGS[ood_name]
        id_class = DATASET_REGISTRY[id_dataset]

        get_logger().info(f"ğŸ“¥ åŠ è½½ OOD æ•°æ®é›†: {ood_config['name']}...")

        # åŠ è½½ OOD æ•°æ®é›†
        try:
            ood_dataset = ood_config["loader"](root)
        except Exception as e:
            get_logger().error(f"âŒ OOD æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            raise

        # è½¬æ¢ä¸ºå¼ é‡
        images_list = []
        target_size = id_class.IMAGE_SIZE

        for i in range(len(ood_dataset)):
            img, _ = ood_dataset[i]

            # å¤„ç†ä¸åŒæ ¼å¼çš„å›¾åƒ
            if hasattr(img, "numpy"):
                img_np = np.array(img)
            else:
                img_np = np.array(img)

            # ç¡®ä¿æ˜¯ RGB
            if len(img_np.shape) == 2:
                img_np = np.stack([img_np] * 3, axis=-1)
            elif img_np.shape[-1] == 4:
                img_np = img_np[:, :, :3]

            # Resize åˆ° ID æ•°æ®é›†çš„å°ºå¯¸
            if img_np.shape[0] != target_size or img_np.shape[1] != target_size:
                from PIL import Image

                img_pil = Image.fromarray(img_np)
                img_pil = img_pil.resize((target_size, target_size), Image.BILINEAR)
                img_np = np.array(img_pil)

            images_list.append(img_np)

        images = np.stack(images_list, axis=0)  # [N, H, W, C]
        images_tensor = torch.from_numpy(images).permute(0, 3, 1, 2)  # [N, C, H, W]

        get_logger().info(
            f"âœ… åŠ è½½äº† {len(images_tensor)} ä¸ª OOD æ ·æœ¬ (å°ºå¯¸: {target_size}x{target_size})"
        )

        return cls(
            name=ood_config["name"],
            images=images_tensor,
            mean=id_class.MEAN,
            std=id_class.STD,
        )

    def get_loader(
        self,
        batch_size: int = 128,
        num_workers: int = 4,
    ) -> DataLoader:
        """è·å– OOD æ•°æ®åŠ è½½å™¨"""
        # æ ‡å‡†åŒ–
        images_float = self.images.float() / 255.0
        images_normalized = (images_float - self._mean) / self._std

        # ä½¿ç”¨ -1 ä½œä¸º OOD æ ‡ç­¾
        labels = torch.full((len(self.images),), -1, dtype=torch.long)

        dataset = TensorDataset(images_normalized, labels)
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
        )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Domain Shift æ•°æ®é›†                                                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DomainShiftDataset:
    """Domain Shift (åŸŸåç§») è¯„ä¼°æ•°æ®é›†

    ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§†è§‰åŸŸ/é£æ ¼ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚
    ä¸ OOD ä¸åŒçš„æ˜¯ï¼ŒDomain Shift æ•°æ®é›†æœ‰ç›¸åŒçš„ç±»åˆ«ï¼Œåªæ˜¯é£æ ¼ä¸åŒã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        # è‡ªå®šä¹‰æ•°æ®é›†
        >>> ds = DomainShiftDataset.from_folder("./data/sketches", id_dataset="cifar10")
        >>> loader = ds.get_loader(batch_size=128)
    """

    def __init__(
        self,
        name: str,
        images: torch.Tensor,
        labels: torch.Tensor,
        mean: List[float],
        std: List[float],
    ):
        """ç›´æ¥æ„é€ å‡½æ•°"""
        self.name = name
        self.images = images  # [N, C, H, W], uint8
        self.labels = labels  # [N], long
        self._mean = torch.tensor(mean).view(1, 3, 1, 1)
        self._std = torch.tensor(std).view(1, 3, 1, 1)

    @property
    def num_samples(self) -> int:
        return len(self.images)

    @classmethod
    def from_folder(
        cls,
        folder_path: str,
        id_dataset: str,
        class_names: List[str] = None,
    ) -> "DomainShiftDataset":
        """ä»æ–‡ä»¶å¤¹åŠ è½½ Domain Shift æ•°æ®é›†

        æ–‡ä»¶å¤¹ç»“æ„åº”ä¸º:
        folder_path/
            class_0/
                img1.jpg
                img2.jpg
            class_1/
                img1.jpg
            ...

        Args:
            folder_path: æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„
            id_dataset: ID æ•°æ®é›†åç§°ï¼ˆç”¨äºç¡®å®šæ ‡å‡†åŒ–å‚æ•°å’Œå›¾åƒå°ºå¯¸ï¼‰
            class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ–‡ä»¶å¤¹åï¼‰

        Returns:
            DomainShiftDataset å®ä¾‹
        """
        from pathlib import Path

        from PIL import Image

        if id_dataset not in DATASET_REGISTRY:
            raise ValueError(
                f"æœªçŸ¥ ID æ•°æ®é›†: {id_dataset}. å¯ç”¨: {list(DATASET_REGISTRY.keys())}"
            )

        id_class = DATASET_REGISTRY[id_dataset]
        folder = Path(folder_path)

        if not folder.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶å¤¹: {folder_path}")

        # è·å–ç±»åˆ«
        class_folders = sorted([d for d in folder.iterdir() if d.is_dir()])
        if not class_folders:
            raise ValueError(f"æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°å­ç›®å½•: {folder_path}")

        get_logger().info(f"ğŸ“¥ åŠ è½½ Domain Shift æ•°æ®é›†: {folder.name}...")

        images_list = []
        labels_list = []
        target_size = id_class.IMAGE_SIZE

        for class_idx, class_folder in enumerate(class_folders):
            image_files = list(class_folder.glob("*.[jJ][pP][gG]")) + list(
                class_folder.glob("*.[pP][nN][gG]")
            )

            for img_path in image_files:
                try:
                    img = Image.open(img_path).convert("RGB")
                    img = img.resize((target_size, target_size), Image.BILINEAR)
                    img_np = np.array(img)
                    images_list.append(img_np)
                    labels_list.append(class_idx)
                except Exception as e:
                    get_logger().warning(f"è·³è¿‡æ— æ•ˆå›¾åƒ {img_path}: {e}")

        if not images_list:
            raise ValueError(f"æœªæ‰¾åˆ°æœ‰æ•ˆå›¾åƒ: {folder_path}")

        images = np.stack(images_list, axis=0)
        images_tensor = torch.from_numpy(images).permute(0, 3, 1, 2)
        labels_tensor = torch.tensor(labels_list, dtype=torch.long)

        get_logger().info(
            f"âœ… åŠ è½½äº† {len(images_tensor)} ä¸ªæ ·æœ¬, {len(class_folders)} ä¸ªç±»åˆ«"
        )

        return cls(
            name=folder.name,
            images=images_tensor,
            labels=labels_tensor,
            mean=id_class.MEAN,
            std=id_class.STD,
        )

    def get_loader(
        self,
        batch_size: int = 128,
        num_workers: int = 4,
    ) -> DataLoader:
        """è·å–æ•°æ®åŠ è½½å™¨"""
        images_float = self.images.float() / 255.0
        images_normalized = (images_float - self._mean) / self._std

        dataset = TensorDataset(images_normalized, self.labels)
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
        )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æ•°æ®é›†åŠ è½½å‡½æ•°                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_dataset(cfg):
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†

    å‚æ•°:
        cfg: é…ç½®å¯¹è±¡

    è¿”å›:
        train_loader, val_loader, test_loader, corruption_dataset
    """
    dataset_name = cfg.dataset_name.lower()

    if dataset_name not in DATASET_REGISTRY:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}. æ”¯æŒ: {list(DATASET_REGISTRY.keys())}"
        )

    DatasetClass = DATASET_REGISTRY[dataset_name]

    # ä¸ºæ²¡æœ‰å®˜æ–¹åˆ’åˆ†çš„æ•°æ®é›†ä¼ é€’é¢å¤–å‚æ•°
    extra_kwargs = {}
    if not getattr(DatasetClass, "HAS_OFFICIAL_SPLIT", True):
        extra_kwargs["test_split"] = cfg.test_split

    # åˆ›å»ºå®Œæ•´è®­ç»ƒé›† (ç”¨äºåˆ’åˆ†)
    train_full = DatasetClass(root=cfg.data_root, train=True, **extra_kwargs)
    test_dataset = DatasetClass(root=cfg.data_root, train=False, **extra_kwargs)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_train = len(train_full)
    val_size = int(total_train * cfg.val_split)
    train_size = total_train - val_size

    generator = torch.Generator().manual_seed(cfg.seed)
    indices = torch.randperm(total_train, generator=generator)
    train_indices = indices[:train_size].tolist()
    val_indices = indices[train_size:].tolist()

    # ä½¿ç”¨ PyTorch å†…ç½® Subset
    train_subset = Subset(train_full, train_indices)
    val_subset = Subset(train_full, val_indices)

    # åˆ›å»ºDataLoader
    common_loader_kwargs = {
        "num_workers": cfg.num_workers,
        "pin_memory": cfg.pin_memory,
        "persistent_workers": cfg.persistent_workers and cfg.num_workers > 0,
        "prefetch_factor": cfg.prefetch_factor if cfg.num_workers > 0 else None,
    }

    train_loader = DataLoader(
        train_subset, batch_size=cfg.batch_size, shuffle=True, **common_loader_kwargs
    )
    val_loader = DataLoader(
        val_subset, batch_size=cfg.batch_size * 2, shuffle=False, **common_loader_kwargs
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=cfg.batch_size * 2,
        shuffle=False,
        **common_loader_kwargs,
    )

    get_logger().info(f"ğŸ“Š æ•°æ®é›†: {dataset_name.upper()}")
    get_logger().info(
        f"   è®­ç»ƒé›†: {len(train_subset)} | éªŒè¯é›†: {len(val_subset)} | æµ‹è¯•é›†: {len(test_dataset)}"
    )

    # åŠ è½½Corruptionæ•°æ®é›† (ä»»ä½•åœ¨ DATASET_REGISTRY ä¸­çš„æ•°æ®é›†éƒ½æ”¯æŒ)
    corruption_dataset = None
    try:
        corruption_dataset = CorruptionDataset.from_name(dataset_name, cfg.data_root)
        get_logger().info(f"   Corruptionæ•°æ®é›†: {corruption_dataset.name}")
    except FileNotFoundError as e:
        get_logger().warning(f"   âš ï¸ Corruptionæ•°æ®é›†æœªæ‰¾åˆ°: {e}")

    return train_loader, val_loader, test_loader, corruption_dataset
