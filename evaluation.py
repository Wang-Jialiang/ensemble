"""
================================================================================
è¯„ä¼°æ¨¡å—
================================================================================
"""

import json
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from .config import Config
from .datasets import CorruptionDataset
from .models import ModelFactory
from .utils import ensure_dir, format_duration, get_logger

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ é›†æˆç­–ç•¥                                                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EnsembleFn = Callable[[torch.Tensor], torch.Tensor]


def _voting_fn(all_logits: torch.Tensor) -> torch.Tensor:
    """å¤šæ•°æŠ•ç¥¨ (å°†æŠ•ç¥¨ç»“æœè½¬æ¢ä¸º logits)"""
    preds = all_logits.argmax(dim=2)  # [N, Samples]
    num_classes = all_logits.shape[2]
    votes = torch.zeros(preds.shape[1], num_classes, device=all_logits.device)
    for i in range(preds.shape[0]):
        votes.scatter_add_(
            1,
            preds[i].unsqueeze(1),
            torch.ones_like(preds[i].unsqueeze(1), dtype=votes.dtype),
        )
    return votes


ENSEMBLE_STRATEGIES: Dict[str, EnsembleFn] = {
    "mean": lambda logits: logits.mean(dim=0),
    "voting": _voting_fn,
}


def get_ensemble_fn(cfg: "Config") -> EnsembleFn:
    """ä»é…ç½®è·å–é›†æˆå‡½æ•°"""
    strategy = getattr(cfg, "ensemble_strategy", "mean")
    return ENSEMBLE_STRATEGIES.get(strategy, ENSEMBLE_STRATEGIES["mean"])


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŒ‡æ ‡è®¡ç®—å™¨                                                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class MetricsCalculator:
    """é›†æˆæ¨¡å‹æŒ‡æ ‡è®¡ç®—å™¨

    è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡ï¼š
    - å‡†ç¡®ç‡ï¼šé›†æˆ/ä¸ªä½“/Oracle/Top-5
    - æ ¡å‡†ï¼šECEã€NLL
    - å¤šæ ·æ€§ï¼šåˆ†æ­§åº¦ã€å¤šæ ·æ€§
    - å…¬å¹³æ€§ï¼šå¹³è¡¡å‡†ç¡®ç‡ã€åŸºå°¼ç³»æ•°
    """

    def __init__(self, num_classes: int, ece_n_bins: int = 15):
        from torchmetrics.classification import CalibrationError

        self.num_classes = num_classes
        self.ece_metric = CalibrationError(
            task="multiclass", num_classes=num_classes, n_bins=ece_n_bins, norm="l1"
        )

    def calculate_all_metrics(
        self,
        all_logits: torch.Tensor,
        targets: torch.Tensor,
        ensemble_fn: EnsembleFn = None,
    ) -> dict:
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡

        Args:
            all_logits: [num_models, num_samples, num_classes]
            targets: [num_samples]
            ensemble_fn: é›†æˆå‡½æ•°ï¼Œé»˜è®¤ä¸ºç­‰æƒå¹³å‡
        """
        import torch
        import torch.nn.functional as F

        with torch.no_grad():
            metrics = {}
            if ensemble_fn is None:
                ensemble_fn = ENSEMBLE_STRATEGIES["mean"]
            ensemble_logits = ensemble_fn(all_logits)
            ensemble_preds = ensemble_logits.argmax(dim=1)

            # å‡†ç¡®ç‡å’Œæ ¡å‡†
            metrics["ensemble_acc"] = (
                100.0 * (ensemble_preds == targets).float().mean().item()
            )
            metrics["nll"] = F.cross_entropy(ensemble_logits, targets).item()
            metrics["ece"] = self.ece_metric(ensemble_logits, targets).item()

            # ä¸ªä½“æ¨¡å‹å‡†ç¡®ç‡
            all_preds = all_logits.argmax(dim=2)
            correct_per_model = all_preds == targets.unsqueeze(0)
            individual_accs = correct_per_model.float().mean(dim=1) * 100.0
            metrics["avg_individual_acc"] = individual_accs.mean().item()
            metrics["min_individual_acc"] = individual_accs.min().item()
            metrics["max_individual_acc"] = individual_accs.max().item()
            metrics["std_individual_acc"] = individual_accs.std().item()

            # Oracleå‡†ç¡®ç‡
            metrics["oracle_acc"] = (
                100.0 * correct_per_model.any(dim=0).float().mean().item()
            )

            # åˆ†æ­§åº¦
            num_models = all_preds.shape[0]
            disagreement_sum = sum(
                (all_preds[i] != all_preds[j]).float().mean().item()
                for i in range(num_models)
                for j in range(i + 1, num_models)
            )
            pair_count = num_models * (num_models - 1) // 2
            metrics["disagreement"] = 100.0 * (
                disagreement_sum / pair_count if pair_count > 0 else 0.0
            )

            # å¤šæ ·æ€§
            probs = F.softmax(all_logits, dim=2)
            metrics["diversity"] = (
                ((probs - probs.mean(dim=0, keepdim=True)) ** 2).mean().item()
            )

            # JSæ•£åº¦ (è½¯ä¸ä¸€è‡´æ€§)
            js_sum = 0.0
            for i in range(num_models):
                for j in range(i + 1, num_models):
                    p = probs[i]  # [num_samples, num_classes]
                    q = probs[j]
                    m = (p + q) / 2
                    # KL(P||M) + KL(Q||M), ä½¿ç”¨ log2 ä½¿ç»“æœåœ¨ [0, 1]
                    kl_pm = (p * (torch.log2(p + 1e-10) - torch.log2(m + 1e-10))).sum(
                        dim=1
                    )
                    kl_qm = (q * (torch.log2(q + 1e-10) - torch.log2(m + 1e-10))).sum(
                        dim=1
                    )
                    js = 0.5 * (kl_pm + kl_qm)
                    js_sum += js.mean().item()
            metrics["js_divergence"] = js_sum / pair_count if pair_count > 0 else 0.0

            # Top-5å‡†ç¡®ç‡
            if self.num_classes >= 5:
                top5 = ensemble_logits.topk(5, dim=1)[1]
                metrics["top5_acc"] = (
                    100.0
                    * (top5 == targets.unsqueeze(1)).any(dim=1).float().mean().item()
                )

            # ç½®ä¿¡åº¦
            ensemble_probs = F.softmax(ensemble_logits, dim=1)
            max_probs = ensemble_probs.max(dim=1)[0]
            metrics["avg_confidence"] = max_probs.mean().item()
            metrics["avg_correct_confidence"] = (
                max_probs[ensemble_preds == targets].mean().item()
            )
            incorrect_mask = ensemble_preds != targets
            metrics["avg_incorrect_confidence"] = (
                max_probs[incorrect_mask].mean().item() if incorrect_mask.any() else 0.0
            )

            # å…¬å¹³æ€§æŒ‡æ ‡ (å†…è”è®¡ç®—)
            per_class_acc = []
            per_class_count = []
            for c in range(self.num_classes):
                mask = targets == c
                count = mask.sum().item()
                if count > 0:
                    acc = (
                        100.0
                        * ((ensemble_preds == targets) & mask).sum().item()
                        / count
                    )
                else:
                    acc = 0.0
                per_class_acc.append(acc)
                per_class_count.append(count)
                metrics[f"class_{c}_acc"] = acc

            valid_mask = torch.tensor(per_class_count) > 0
            valid_accs = torch.tensor(per_class_acc)[valid_mask]

            if len(valid_accs) > 0:
                metrics["balanced_acc"] = valid_accs.mean().item()
                metrics["acc_disparity"] = (valid_accs.max() - valid_accs.min()).item()
                metrics["worst_class_acc"] = valid_accs.min().item()
                metrics["best_class_acc"] = valid_accs.max().item()
                metrics["per_class_acc_std"] = (
                    valid_accs.std().item() if len(valid_accs) > 1 else 0.0
                )
                # åŸºå°¼ç³»æ•°è®¡ç®— (å†…è”)
                if len(valid_accs) <= 1:
                    gini = 0.0
                else:
                    sorted_vals = torch.sort(valid_accs)[0]
                    n = len(sorted_vals)
                    total = sorted_vals.sum()
                    if total == 0:
                        gini = 0.0
                    else:
                        indices = torch.arange(1, n + 1, dtype=torch.float32)
                        gini = max(
                            0.0,
                            (
                                (2.0 * (indices * sorted_vals).sum() / (n * total))
                                - (n + 1.0) / n
                            ).item(),
                        )
                metrics["acc_gini_coef"] = gini
                metrics["fairness_score"] = max(0.0, 100.0 - metrics["acc_disparity"])
            else:
                metrics.update(
                    {
                        "balanced_acc": 0.0,
                        "acc_disparity": 0.0,
                        "worst_class_acc": 0.0,
                        "best_class_acc": 0.0,
                        "per_class_acc_std": 0.0,
                        "acc_gini_coef": 0.0,
                        "fairness_score": 100.0,
                    }
                )

            return metrics


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è¯„ä¼°å‡½æ•°
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def extract_models(trainer_or_models: Any) -> Tuple[List[nn.Module], torch.device]:
    """
    ä» Trainer æˆ–æ¨¡å‹åˆ—è¡¨ä¸­æå–æ¨¡å‹å’Œè®¾å¤‡

    Args:
        trainer_or_models: StagedEnsembleTrainer å®ä¾‹æˆ– List[nn.Module]

    Returns:
        (models, device): æ¨¡å‹åˆ—è¡¨å’Œè®¡ç®—è®¾å¤‡
    """
    if hasattr(trainer_or_models, "workers"):  # æ˜¯ Trainer
        models = [
            model for worker in trainer_or_models.workers for model in worker.models
        ]
        device = trainer_or_models.workers[0].device
    else:  # æ˜¯æ¨¡å‹åˆ—è¡¨
        models = trainer_or_models
        try:
            device = next(models[0].parameters()).device
        except StopIteration:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return models, device


def get_all_models_logits(
    models: List[nn.Module], loader: DataLoader, device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    è·å–æ‰€æœ‰æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„ logits

    Args:
        models: æ¨¡å‹åˆ—è¡¨ List[nn.Module]
        loader: æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡

    Returns:
        all_logits: (num_models, num_samples, num_classes)
        targets: (num_samples,)
    """
    from tqdm import tqdm

    all_logits_list = []
    all_targets_list = []

    iterator = tqdm(loader, desc="Evaluating Models", leave=False)

    with torch.no_grad():
        for inputs, targets in iterator:
            inputs = inputs.to(device)
            batch_logits = []

            for model in models:
                model.eval()
                logits = model(inputs)  # (batch_size, num_classes)
                batch_logits.append(logits.unsqueeze(0).cpu())

            # combined: (num_models, batch_size, num_classes)
            if batch_logits:
                combined = torch.cat(batch_logits, dim=0)
                all_logits_list.append(combined)
                all_targets_list.append(targets.cpu())

    if not all_logits_list:
        return torch.tensor([]), torch.tensor([])

    # æ²¿ç€ batch ç»´åº¦ (dim=1) æ‹¼æ¥
    all_logits = torch.cat(all_logits_list, dim=1)
    all_targets = torch.cat(all_targets_list)

    return all_logits, all_targets


def evaluate_corruption(
    trainer_or_models: Any,
    corruption_dataset: CorruptionDataset,
    batch_size: int = 128,
    num_workers: int = 4,
    logger: Optional[Any] = None,
) -> Dict[str, Any]:
    """
    é€šç”¨ Corruption é²æ£’æ€§è¯„ä¼°
    """
    logger = logger or get_logger()
    dataset_name = corruption_dataset.name
    logger.info(f"\nğŸ§ª Running Corruption Evaluation on {dataset_name}...")

    models, device = extract_models(trainer_or_models)
    results = {}
    overall_avg = 0.0

    for severity in range(1, 6):
        logger.info(f"   Severity {severity}:")
        results[severity] = {}
        severity_accs = []

        for corruption in corruption_dataset.CORRUPTIONS:
            loader = corruption_dataset.get_loader(
                corruption,
                severity=severity,
                batch_size=batch_size,
                num_workers=num_workers,
            )

            # åªéœ€é¢„æµ‹ï¼Œä¸éœ€è¦è¯¦ç»†çš„ Calculator (åªç®— Acc)
            all_logits, targets = get_all_models_logits(models, loader, device)

            ensemble_logits = all_logits.mean(dim=0)
            ensemble_preds = ensemble_logits.argmax(dim=1)
            acc = 100.0 * (ensemble_preds == targets).float().mean().item()

            results[severity][corruption] = acc
            severity_accs.append(acc)

        avg_acc_sev = np.mean(severity_accs)
        logger.info(f"     -> Avg: {avg_acc_sev:.2f}%")
        overall_avg += avg_acc_sev

    overall_avg /= 5.0
    logger.info(f"\n   ğŸ“ˆ Overall Avg: {overall_avg:.2f}%")

    results["severity_5_raw"] = results[5]
    results["overall_avg"] = overall_avg
    return results


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Grad-CAM ç›®æ ‡å±‚è¾…åŠ©å‡½æ•°                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def get_target_layer(model: nn.Module, model_name: str) -> nn.Module:
    """è·å–æ¨¡å‹çš„ç›®æ ‡å±‚ç”¨äºGrad-CAM

    æ ¹æ®æ¨¡å‹æ¶æ„è‡ªåŠ¨ç¡®å®šé€‚åˆç”¨äºGrad-CAMå¯è§†åŒ–çš„ç›®æ ‡å±‚ã€‚

    Args:
        model: PyTorchæ¨¡å‹
        model_name: æ¨¡å‹åç§° (resnet18, vgg16, efficientnet_b0ç­‰)

    Returns:
        ç›®æ ‡å±‚æ¨¡å—

    Raises:
        ValueError: æ— æ³•è‡ªåŠ¨ç¡®å®šç›®æ ‡å±‚æ—¶æŠ›å‡º
    """
    model_name = model_name.lower()
    if "resnet" in model_name:
        return model.layer4[-1]
    elif "vgg" in model_name:
        return model.features[-1]
    elif "efficientnet" in model_name:
        return model.features[-1]
    else:
        # é»˜è®¤å°è¯•layer4
        if hasattr(model, "layer4"):
            return model.layer4[-1]
        raise ValueError(f"æ— æ³•è‡ªåŠ¨ç¡®å®š {model_name} çš„ç›®æ ‡å±‚ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Grad-CAM çƒ­åŠ›å›¾ç”Ÿæˆå™¨                                                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GradCAM:
    """
    Grad-CAM (Gradient-weighted Class Activation Mapping) - åŸºäº pytorch-grad-cam åº“

    ç”¨äºç”Ÿæˆæ¨¡å‹æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼Œå¯è§†åŒ–æ¨¡å‹å…³æ³¨çš„å›¾åƒåŒºåŸŸã€‚

    ä¾èµ–: pip install grad-cam
    """

    def __init__(self, model: nn.Module, target_layer: nn.Module):
        """åˆå§‹åŒ–Grad-CAM

        Args:
            model: PyTorchæ¨¡å‹
            target_layer: ç”¨äºç”ŸæˆCAMçš„ç›®æ ‡å±‚ (ä½¿ç”¨get_target_layerè·å–)
        """
        self.model = model
        self.target_layer = target_layer
        self._cam = None  # å»¶è¿Ÿåˆå§‹åŒ–

    def _get_cam(self):
        """å»¶è¿Ÿåˆå§‹åŒ– CAM å¯¹è±¡"""
        if self._cam is None:
            try:
                from pytorch_grad_cam import GradCAM as LibGradCAM

                self._cam = LibGradCAM(
                    model=self.model, target_layers=[self.target_layer]
                )
            except ImportError:
                raise ImportError("éœ€è¦å®‰è£… grad-cam: pip install grad-cam")
        return self._cam

    def generate_cam(self, input_tensor: torch.Tensor, target_class: int) -> np.ndarray:
        """ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾ (å•å¼ å›¾åƒ)

        Args:
            input_tensor: è¾“å…¥å¼ é‡ï¼Œshape (1, C, H, W)
            target_class: ç›®æ ‡ç±»åˆ«ç´¢å¼•

        Returns:
            CAM çƒ­åŠ›å›¾ï¼Œshape (H, W)ï¼Œå€¼åœ¨ [0, 1]
        """
        cams = self.generate_cam_batch(input_tensor, [target_class])
        return cams[0]

    def generate_cam_batch(
        self, input_tensor: torch.Tensor, target_classes: List[int]
    ) -> np.ndarray:
        """æ‰¹é‡ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾ (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)

        Args:
            input_tensor: è¾“å…¥å¼ é‡ï¼Œshape (N, C, H, W)
            target_classes: ç›®æ ‡ç±»åˆ«åˆ—è¡¨ï¼Œé•¿åº¦ä¸º N

        Returns:
            CAM çƒ­åŠ›å›¾æ•°ç»„ï¼Œshape (N, H, W)ï¼Œå€¼åœ¨ [0, 1]
        """
        try:
            from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£… grad-cam: pip install grad-cam")

        cam_obj = self._get_cam()

        # æ‰¹é‡ç”Ÿæˆ CAM - ä¸€æ¬¡å¤„ç†æ•´ä¸ª batch
        targets = [ClassifierOutputTarget(cls) for cls in target_classes]
        grayscale_cams = cam_obj(input_tensor=input_tensor, targets=targets)

        # è¿”å› shape (N, H, W)
        return grayscale_cams

    def remove_hooks(self):
        """ç§»é™¤hooksï¼Œé‡Šæ”¾èµ„æº"""
        if self._cam is not None:
            del self._cam
            self._cam = None


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Grad-CAMå¤šæ ·æ€§åˆ†æå™¨                                                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GradCAMAnalyzer:
    """
    Grad-CAMå¤šæ ·æ€§è´¨é‡åˆ†æå™¨

    ç”¨äºåˆ†æé›†æˆæ¨¡å‹ä¸­å„æˆå‘˜çš„æ³¨æ„åŠ›åˆ†å¸ƒå¤šæ ·æ€§ã€‚

    åˆ†ææŒ‡æ ‡:
        - avg_cam_entropy: CAMçƒ­åŠ›å›¾çš„ç†µ (è¶Šé«˜è¡¨ç¤ºæ³¨æ„åŠ›è¶Šåˆ†æ•£)
        - avg_cam_similarity: æ¨¡å‹é—´CAMç›¸ä¼¼åº¦ (è¶Šä½è¡¨ç¤ºè¶Šå¤šæ ·)
        - avg_cam_overlap: CAMçƒ­ç‚¹åŒºåŸŸé‡å åº¦ (è¶Šä½è¡¨ç¤ºè¶Šå¤šæ ·)
        - pred_cam_correlation: é¢„æµ‹ä¸CAMçš„ç›¸å…³æ€§
    """

    def __init__(self, cfg: "Config"):
        self.cfg = cfg
        self.model_name = cfg.model_name

    def analyze_ensemble_quality(
        self, workers: List, test_loader, num_samples: int = 50, image_size: int = 32
    ) -> Dict[str, Any]:
        """åˆ†æé›†æˆæ¨¡å‹çš„Grad-CAMå¤šæ ·æ€§

        Returns:
            metrics: åŒ…å«per_modelå’ŒoverallæŒ‡æ ‡çš„å­—å…¸
        """
        # æ”¶é›†æ ·æœ¬
        samples = []
        labels = []
        for inputs, targets in test_loader:
            for i in range(min(len(inputs), num_samples - len(samples))):
                samples.append(inputs[i : i + 1])
                labels.append(targets[i].item())
            if len(samples) >= num_samples:
                break

        if len(samples) == 0:
            get_logger().warning("âš ï¸ No samples for Grad-CAM analysis")
            return {}

        samples = torch.cat(samples, dim=0)

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„CAM
        all_cams = []
        all_preds = []

        for worker in workers:
            for model_idx, model in enumerate(worker.models):
                model.eval()
                device = next(model.parameters()).device
                target_layer = get_target_layer(model, self.model_name)
                gradcam = GradCAM(model, target_layer)

                model_cams = []
                model_preds = []

                samples_device = samples.to(device)
                with torch.no_grad():
                    logits = model(samples_device)
                    preds = logits.argmax(dim=1).cpu().tolist()

                # æ‰¹é‡ç”ŸæˆCAM
                cams = gradcam.generate_cam_batch(samples_device, preds)
                model_cams = list(cams)
                model_preds = preds

                gradcam.remove_hooks()

                all_cams.append(model_cams)
                all_preds.append(model_preds)

        # è®¡ç®—æŒ‡æ ‡
        metrics = self._compute_diversity_metrics(all_cams, all_preds, labels)

        return metrics

    def _compute_diversity_metrics(
        self,
        all_cams: List[List[np.ndarray]],
        all_preds: List[List[int]],
        labels: List[int],
    ) -> Dict[str, Any]:
        """è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡"""
        num_models = len(all_cams)
        num_samples = len(all_cams[0]) if all_cams else 0

        if num_samples == 0:
            return {}

        per_model_metrics = []

        for model_idx in range(num_models):
            model_cams = all_cams[model_idx]
            model_preds = all_preds[model_idx]

            # è®¡ç®—CAMç†µ
            entropies = []
            for cam in model_cams:
                cam_flat = cam.flatten()
                cam_flat = cam_flat / (cam_flat.sum() + 1e-8)
                entropy = -np.sum(cam_flat * np.log(cam_flat + 1e-8))
                entropies.append(entropy)

            # é¢„æµ‹å‡†ç¡®ç‡
            correct = sum(1 for p, l in zip(model_preds, labels) if p == l)
            accuracy = correct / len(labels)

            per_model_metrics.append(
                {
                    "avg_cam_entropy": np.mean(entropies),
                    "accuracy": accuracy * 100,
                }
            )

        # è®¡ç®—æ¨¡å‹é—´ç›¸ä¼¼åº¦
        similarities = []
        overlaps = []

        for i in range(num_models):
            for j in range(i + 1, num_models):
                for s in range(num_samples):
                    cam_i = all_cams[i][s].flatten()
                    cam_j = all_cams[j][s].flatten()

                    # ä½™å¼¦ç›¸ä¼¼åº¦
                    sim = np.dot(cam_i, cam_j) / (
                        np.linalg.norm(cam_i) * np.linalg.norm(cam_j) + 1e-8
                    )
                    similarities.append(sim)

                    # é‡å åº¦ (IoU)
                    threshold = 0.5
                    mask_i = cam_i > threshold * cam_i.max()
                    mask_j = cam_j > threshold * cam_j.max()
                    intersection = np.logical_and(mask_i, mask_j).sum()
                    union = np.logical_or(mask_i, mask_j).sum()
                    iou = intersection / (union + 1e-8)
                    overlaps.append(iou)

        overall_metrics = {
            "avg_cam_entropy": np.mean(
                [m["avg_cam_entropy"] for m in per_model_metrics]
            ),
            "avg_cam_similarity": np.mean(similarities) if similarities else 0,
            "avg_cam_overlap": np.mean(overlaps) if overlaps else 0,
            "std_cam_entropy": np.std(
                [m["avg_cam_entropy"] for m in per_model_metrics]
            ),
        }

        return {
            "per_model": per_model_metrics,
            "overall": overall_metrics,
        }


class ModelListWrapper:
    """æ¨¡å‹åˆ—è¡¨çš„ç®€æ˜“åŒ…è£…å™¨ï¼Œç”¨äºå…¼å®¹ GradCAMAnalyzer çš„ workers æ¥å£"""

    def __init__(self, models: List[nn.Module]):
        self.models = models


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Loss Landscape å¯è§†åŒ–å™¨                                                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class LossLandscapeVisualizer:
    """Loss Landscape å¯è§†åŒ–å™¨

    ç”¨äºå¯è§†åŒ–é›†æˆæ¨¡å‹ä¸­å„æˆå‘˜åœ¨æŸå¤±åœ°å½¢ä¸Šçš„ä½ç½®åˆ†å¸ƒã€‚

    åŠŸèƒ½:
        - 1D æ’å€¼: åœ¨ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çº¿æ€§æ’å€¼ï¼Œè§‚å¯ŸæŸå¤±å˜åŒ–
        - 2D å¹³é¢: å›´ç»•å•ä¸ªæ¨¡å‹åœ¨éšæœºæ–¹å‘ä¸Šé‡‡æ ·ï¼Œç”Ÿæˆç­‰é«˜çº¿å›¾
        - æ¨¡å‹é—´è·ç¦»: è®¡ç®—æ¨¡å‹åœ¨å‚æ•°ç©ºé—´ä¸­çš„æ¬§æ°è·ç¦»

    ä¾èµ–: pip install loss-landscapes
    """

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)
        self.logger = get_logger()

    def _check_dependency(self):
        """æ£€æŸ¥ loss-landscapes ä¾èµ–"""
        import importlib.util

        if importlib.util.find_spec("loss_landscapes") is None:
            self.logger.warning(
                "âš ï¸ loss-landscapes æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install loss-landscapes"
            )
            return False
        return True

    def _create_metric(
        self, model: nn.Module, dataloader: DataLoader, device: torch.device
    ):
        """åˆ›å»ºæŸå¤±è¯„ä¼°å™¨"""
        import loss_landscapes.metrics as metrics

        criterion = nn.CrossEntropyLoss()

        class LossMetric(metrics.Metric):
            """è‡ªå®šä¹‰æŸå¤±è¯„ä¼°å™¨"""

            def __init__(self, criterion, dataloader, device):
                super().__init__()
                self.criterion = criterion
                self.dataloader = dataloader
                self.device = device

            def __call__(self, model):
                model.eval()
                total_loss = 0.0
                total_samples = 0
                with torch.no_grad():
                    for inputs, targets in self.dataloader:
                        inputs = inputs.to(self.device)
                        targets = targets.to(self.device)
                        outputs = model(inputs)
                        loss = self.criterion(outputs, targets)
                        total_loss += loss.item() * inputs.size(0)
                        total_samples += inputs.size(0)
                return total_loss / total_samples if total_samples > 0 else 0.0

        return LossMetric(criterion, dataloader, device)

    def plot_1d_interpolation(
        self,
        model1: nn.Module,
        model2: nn.Module,
        dataloader: DataLoader,
        device: torch.device,
        steps: int = 50,
        filename: str = "loss_landscape_1d.png",
        label1: str = "Model 1",
        label2: str = "Model 2",
    ) -> Optional[np.ndarray]:
        """ç»˜åˆ¶ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„1DæŸå¤±æ’å€¼æ›²çº¿

        Args:
            model1: èµ·å§‹æ¨¡å‹
            model2: ç»ˆæ­¢æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡
            steps: æ’å€¼æ­¥æ•°
            filename: ä¿å­˜æ–‡ä»¶å
            label1: æ¨¡å‹1æ ‡ç­¾
            label2: æ¨¡å‹2æ ‡ç­¾

        Returns:
            loss_data: æŸå¤±å€¼æ•°ç»„ï¼Œé•¿åº¦ä¸º steps
        """
        if not self._check_dependency():
            return None

        import loss_landscapes
        import matplotlib.pyplot as plt

        self.logger.info(f"ğŸ“ˆ æ­£åœ¨è®¡ç®— 1D Loss Landscape ({label1} â†’ {label2})...")

        model1 = model1.to(device)
        model2 = model2.to(device)
        metric = self._create_metric(model1, dataloader, device)

        # çº¿æ€§æ’å€¼
        loss_data = loss_landscapes.linear_interpolation(
            model1, model2, metric, steps=steps
        )

        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        x = np.linspace(0, 1, steps)
        ax.plot(x, loss_data, "b-", linewidth=2)
        ax.scatter([0, 1], [loss_data[0], loss_data[-1]], c="red", s=100, zorder=5)
        ax.annotate(
            label1,
            (0, loss_data[0]),
            textcoords="offset points",
            xytext=(10, 10),
            fontsize=10,
        )
        ax.annotate(
            label2,
            (1, loss_data[-1]),
            textcoords="offset points",
            xytext=(10, 10),
            fontsize=10,
        )

        ax.set_xlabel("Interpolation (Î±)")
        ax.set_ylabel("Loss")
        ax.set_title(f"Loss Landscape: {label1} â†’ {label2}")
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()

        self.logger.info(f"ğŸ“Š Saved: {filename}")
        return loss_data

    def plot_2d_plane(
        self,
        model: nn.Module,
        dataloader: DataLoader,
        device: torch.device,
        distance: float = 1.0,
        steps: int = 40,
        filename: str = "loss_landscape_2d.png",
        model_name: str = "Model",
    ) -> Optional[np.ndarray]:
        """ç»˜åˆ¶æ¨¡å‹å‘¨å›´çš„2DæŸå¤±åœ°å½¢ç­‰é«˜çº¿å›¾

        Args:
            model: ç›®æ ‡æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡
            distance: é‡‡æ ·è·ç¦»ï¼ˆå‚æ•°ç©ºé—´ä¸­çš„èŒƒå›´ï¼‰
            steps: æ¯ä¸ªæ–¹å‘çš„é‡‡æ ·æ­¥æ•°
            filename: ä¿å­˜æ–‡ä»¶å
            model_name: æ¨¡å‹åç§°

        Returns:
            loss_data: 2DæŸå¤±å€¼æ•°ç»„ï¼Œshape (steps, steps)
        """
        if not self._check_dependency():
            return None

        import loss_landscapes
        import matplotlib.pyplot as plt

        self.logger.info(f"ğŸ“ˆ æ­£åœ¨è®¡ç®— 2D Loss Landscape ({model_name})...")
        self.logger.info(
            f"   â³ é¢„è®¡ {steps}Ã—{steps}={steps * steps} æ¬¡å‰å‘ä¼ æ’­ï¼Œè¯·è€å¿ƒç­‰å¾…..."
        )

        model = model.to(device)
        metric = self._create_metric(model, dataloader, device)

        # éšæœºæ–¹å‘å¹³é¢é‡‡æ ·
        loss_data = loss_landscapes.random_plane(
            model, metric, distance=distance, steps=steps, normalization="filter"
        )
        self.logger.info("   âœ… 2D é‡‡æ ·å®Œæˆ")

        # åˆ›å»ºåæ ‡ç½‘æ ¼
        x = np.linspace(-distance, distance, steps)
        y = np.linspace(-distance, distance, steps)
        X, Y = np.meshgrid(x, y)

        # ç»˜åˆ¶ç­‰é«˜çº¿å›¾
        fig, ax = plt.subplots(figsize=(10, 8))
        contour = ax.contourf(X, Y, loss_data, levels=50, cmap="viridis")
        plt.colorbar(contour, ax=ax, label="Loss")
        ax.scatter([0], [0], c="red", s=100, marker="*", label=model_name, zorder=5)
        ax.legend()
        ax.set_xlabel("Direction 1")
        ax.set_ylabel("Direction 2")
        ax.set_title(f"2D Loss Landscape around {model_name}")
        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        self.logger.info(f"ğŸ“Š Saved: {filename}")

        # ç»˜åˆ¶ 3D è¡¨é¢å›¾ (è£¸çœ¼3Dæ•ˆæœ)
        fig_3d = plt.figure(figsize=(12, 9))
        ax_3d = fig_3d.add_subplot(111, projection="3d")

        # ç»˜åˆ¶è¡¨é¢
        surf = ax_3d.plot_surface(
            X, Y, loss_data, cmap="viridis", edgecolor="none", alpha=0.9
        )
        fig_3d.colorbar(surf, ax=ax_3d, shrink=0.5, aspect=10, label="Loss")

        # æ ‡è®°æ¨¡å‹ä½ç½®
        center_loss = loss_data[steps // 2, steps // 2]
        ax_3d.scatter(
            [0], [0], [center_loss], c="red", s=200, marker="*", label=model_name
        )

        ax_3d.set_xlabel("Direction 1")
        ax_3d.set_ylabel("Direction 2")
        ax_3d.set_zlabel("Loss")
        ax_3d.set_title(f"3D Loss Landscape around {model_name}")
        ax_3d.view_init(elev=30, azim=45)  # è®¾ç½®è§†è§’
        ax_3d.legend()

        filename_3d = filename.replace(".png", "_3d.png")
        plt.tight_layout()
        plt.savefig(self.save_dir / filename_3d, dpi=150)
        plt.close()
        self.logger.info(f"ğŸ“Š Saved: {filename_3d}")

        return loss_data

    def plot_ensemble_interpolations(
        self,
        models: List[nn.Module],
        dataloader: DataLoader,
        device: torch.device,
        steps: int = 50,
        filename: str = "ensemble_loss_landscape.png",
    ) -> Dict[str, np.ndarray]:
        """ç»˜åˆ¶é›†æˆä¸­æ‰€æœ‰æ¨¡å‹å¯¹ä¹‹é—´çš„1DæŸå¤±æ’å€¼æ›²çº¿

        Args:
            models: æ¨¡å‹åˆ—è¡¨
            dataloader: æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡
            steps: æ’å€¼æ­¥æ•°
            filename: ä¿å­˜æ–‡ä»¶å

        Returns:
            results: {(i,j): loss_data} å­—å…¸
        """
        if not self._check_dependency():
            return {}

        import loss_landscapes
        import matplotlib.pyplot as plt

        n_models = len(models)
        if n_models < 2:
            self.logger.warning("âš ï¸ éœ€è¦è‡³å°‘ 2 ä¸ªæ¨¡å‹æ¥è®¡ç®—æ’å€¼")
            return {}

        self.logger.info(
            f"ğŸ“ˆ æ­£åœ¨è®¡ç®—é›†æˆæ¨¡å‹é—´çš„ Loss Landscape ({n_models} ä¸ªæ¨¡å‹)..."
        )

        results = {}
        pairs = [(i, j) for i in range(n_models) for j in range(i + 1, n_models)]

        # è®¡ç®—æ‰€æœ‰æ¨¡å‹å¯¹çš„æ’å€¼ (å¸¦è¿›åº¦æ¡)
        from tqdm import tqdm

        for idx, (i, j) in enumerate(
            tqdm(pairs, desc="Computing Loss Landscape", leave=False)
        ):
            model_i = models[i].to(device)
            model_j = models[j].to(device)
            metric = self._create_metric(model_i, dataloader, device)

            loss_data = loss_landscapes.linear_interpolation(
                model_i, model_j, metric, steps=steps
            )
            results[f"M{i + 1}-M{j + 1}"] = loss_data

        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(12, 6))
        x = np.linspace(0, 1, steps)
        colors = plt.cm.tab10(np.linspace(0, 1, len(results)))

        for (pair_name, loss_data), color in zip(results.items(), colors):
            ax.plot(x, loss_data, label=pair_name, linewidth=1.5, color=color)

        ax.set_xlabel("Interpolation (Î±)")
        ax.set_ylabel("Loss")
        ax.set_title("Loss Landscape: Pairwise Model Interpolations")
        ax.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150, bbox_inches="tight")
        plt.close()

        self.logger.info(f"ğŸ“Š Saved: {filename}")
        return results

    def compute_model_distances(self, models: List[nn.Module]) -> np.ndarray:
        """è®¡ç®—æ¨¡å‹é—´çš„å‚æ•°ç©ºé—´æ¬§æ°è·ç¦»

        Args:
            models: æ¨¡å‹åˆ—è¡¨

        Returns:
            distance_matrix: è·ç¦»çŸ©é˜µï¼Œshape (n_models, n_models)
        """
        n_models = len(models)
        distance_matrix = np.zeros((n_models, n_models))

        # å°†æ‰€æœ‰æ¨¡å‹å‚æ•°å±•å¹³
        flat_params = []
        for model in models:
            params = torch.cat(
                [p.data.view(-1).cpu() for p in model.parameters()]
            ).numpy()
            flat_params.append(params)

        # è®¡ç®—æˆå¯¹è·ç¦»
        for i in range(n_models):
            for j in range(i + 1, n_models):
                dist = np.linalg.norm(flat_params[i] - flat_params[j])
                distance_matrix[i, j] = dist
                distance_matrix[j, i] = dist

        return distance_matrix

    def plot_model_distance_heatmap(
        self,
        models: List[nn.Module],
        filename: str = "model_distances.png",
    ) -> np.ndarray:
        """ç»˜åˆ¶æ¨¡å‹é—´è·ç¦»çƒ­åŠ›å›¾

        Args:
            models: æ¨¡å‹åˆ—è¡¨
            filename: ä¿å­˜æ–‡ä»¶å

        Returns:
            distance_matrix: è·ç¦»çŸ©é˜µ
        """
        import matplotlib.pyplot as plt

        self.logger.info("ğŸ“ˆ æ­£åœ¨è®¡ç®—æ¨¡å‹é—´å‚æ•°è·ç¦»...")

        distance_matrix = self.compute_model_distances(models)
        n_models = len(models)

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(distance_matrix, cmap="YlOrRd")
        plt.colorbar(im, ax=ax, label="Euclidean Distance")

        # è®¾ç½®æ ‡ç­¾
        labels = [f"M{i + 1}" for i in range(n_models)]
        ax.set_xticks(range(n_models))
        ax.set_yticks(range(n_models))
        ax.set_xticklabels(labels)
        ax.set_yticklabels(labels)

        # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
        for i in range(n_models):
            for j in range(n_models):
                ax.text(
                    j,
                    i,
                    f"{distance_matrix[i, j]:.1f}",
                    ha="center",
                    va="center",
                    color="black"
                    if distance_matrix[i, j] < distance_matrix.max() / 2
                    else "white",
                    fontsize=8,
                )

        ax.set_title("Model Parameter Space Distances")
        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()

        self.logger.info(f"ğŸ“Š Saved: {filename}")
        return distance_matrix


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Checkpoint åŠ è½½å™¨                                                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CheckpointLoader:
    """ä» checkpoint åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°

    å®Œå…¨ç‹¬ç«‹äºè®­ç»ƒæ¨¡å—ï¼Œåªéœ€ checkpoint è·¯å¾„å’Œé…ç½®å³å¯åŠ è½½æ¨¡å‹ã€‚
    """

    @staticmethod
    def load(checkpoint_path: str, cfg: Config) -> Dict[str, Any]:
        """
        åŠ è½½ checkpoint å¹¶è¿”å›å¯è¯„ä¼°çš„æ¨¡å‹ä¸Šä¸‹æ–‡

        Args:
            checkpoint_path: checkpoint ç›®å½•è·¯å¾„
            cfg: é…ç½®å¯¹è±¡

        Returns:
            context: {
                'name': å®éªŒåç§°,
                'models': List[nn.Module],
                'training_time': float,
                'config': dict
            }
        """
        checkpoint_dir = Path(checkpoint_path)
        if not checkpoint_dir.exists():
            raise FileNotFoundError(f"Checkpoint ä¸å­˜åœ¨: {checkpoint_path}")

        # æ¨æ–­å®éªŒåç§°
        experiment_name = checkpoint_dir.parent.name

        # è¯»å–è®­ç»ƒçŠ¶æ€
        state_path = checkpoint_dir / "trainer_state.pth"
        training_time = 0.0
        train_config = {}

        if state_path.exists():
            state = torch.load(state_path, weights_only=False)
            training_time = state.get("total_training_time", 0.0)
            train_config = {
                "augmentation_method": state.get("augmentation_method", "unknown"),
                "use_curriculum": state.get("use_curriculum", False),
            }

        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        models = []
        model_files = sorted(checkpoint_dir.glob(f"{experiment_name}_*.pth"))

        for model_file in model_files:
            model = ModelFactory.create_model(
                cfg.model_name, num_classes=cfg.num_classes
            )
            state = torch.load(model_file, weights_only=False)
            model.load_state_dict(state["model_state_dict"])
            model.eval()
            models.append(model)

        if not models:
            raise RuntimeError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {checkpoint_dir}")

        get_logger().info(f"âœ… åŠ è½½ {experiment_name}: {len(models)} ä¸ªæ¨¡å‹")

        return {
            "name": experiment_name,
            "models": models,
            "training_time": training_time,
            "config": train_config,
        }


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŠ¥å‘Šå¯è§†åŒ–å™¨ (matplotlib)                                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ReportVisualizer:
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ (matplotlib)"""

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)

    def plot_accuracy_comparison(
        self, results: Dict[str, Dict], filename: str = "accuracy_comparison.png"
    ):
        """å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
        import matplotlib.pyplot as plt

        names = list(results.keys())
        ensemble_accs = [
            r.get("standard_metrics", {}).get("ensemble_acc", 0)
            for r in results.values()
        ]
        oracle_accs = [
            r.get("standard_metrics", {}).get("oracle_acc", 0) for r in results.values()
        ]

        x = np.arange(len(names))
        width = 0.35

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.bar(
            x - width / 2, ensemble_accs, width, label="Ensemble Acc", color="#2ecc71"
        )
        ax.bar(x + width / 2, oracle_accs, width, label="Oracle Acc", color="#3498db")

        ax.set_ylabel("Accuracy (%)")
        ax.set_title("Accuracy Comparison")
        ax.set_xticks(x)
        ax.set_xticklabels(names, rotation=45, ha="right")
        ax.legend()
        ax.set_ylim(0, 100)
        ax.grid(axis="y", alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        get_logger().info(f"ğŸ“Š Saved: {filename}")

    def plot_calibration_comparison(
        self, results: Dict[str, Dict], filename: str = "calibration.png"
    ):
        """æ ¡å‡†æŒ‡æ ‡å¯¹æ¯”"""
        import matplotlib.pyplot as plt

        names = list(results.keys())
        ece = [
            r.get("standard_metrics", {}).get("ece", 0) * 100 for r in results.values()
        ]
        nll = [r.get("standard_metrics", {}).get("nll", 0) for r in results.values()]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        ax1.bar(names, ece, color="#e74c3c")
        ax1.set_ylabel("ECE (%)")
        ax1.set_title("Expected Calibration Error (â†“ better)")
        ax1.tick_params(axis="x", rotation=45)

        ax2.bar(names, nll, color="#9b59b6")
        ax2.set_ylabel("NLL")
        ax2.set_title("Negative Log Likelihood (â†“ better)")
        ax2.tick_params(axis="x", rotation=45)

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        get_logger().info(f"ğŸ“Š Saved: {filename}")

    def plot_diversity_comparison(
        self, results: Dict[str, Dict], filename: str = "diversity.png"
    ):
        """å¤šæ ·æ€§æŒ‡æ ‡å¯¹æ¯”"""
        import matplotlib.pyplot as plt

        names = list(results.keys())
        disagreement = [
            r.get("standard_metrics", {}).get("disagreement", 0)
            for r in results.values()
        ]
        js_divergence = [
            r.get("standard_metrics", {}).get("js_divergence", 0)
            for r in results.values()
        ]
        diversity = [
            r.get("standard_metrics", {}).get("diversity", 0) * 1000
            for r in results.values()
        ]

        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

        ax1.bar(names, disagreement, color="#f39c12")
        ax1.set_ylabel("Disagreement (%)")
        ax1.set_title("Hard Disagreement (â†‘ more diverse)")
        ax1.tick_params(axis="x", rotation=45)

        ax2.bar(names, js_divergence, color="#e74c3c")
        ax2.set_ylabel("JS Divergence")
        ax2.set_title("Soft Disagreement (â†‘ more diverse)")
        ax2.tick_params(axis="x", rotation=45)

        ax3.bar(names, diversity, color="#1abc9c")
        ax3.set_ylabel("Diversity (Ã—1000)")
        ax3.set_title("Prediction Diversity (â†‘ more diverse)")
        ax3.tick_params(axis="x", rotation=45)

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        get_logger().info(f"ğŸ“Š Saved: {filename}")

    def plot_robustness_heatmap(
        self, results: Dict[str, Dict], filename: str = "robustness.png"
    ):
        """é²æ£’æ€§çƒ­åŠ›å›¾"""
        import matplotlib.pyplot as plt

        # æ”¶é›† corruption ç»“æœ
        exp_names = list(results.keys())
        first_exp = list(results.values())[0]
        corruption_results = first_exp.get("corruption_results", {})

        if not corruption_results:
            get_logger().info("âš ï¸ No corruption results to plot")
            return

        corruption_types = list(corruption_results.keys())

        data = []
        for exp_name in exp_names:
            row = []
            for ctype in corruption_types:
                acc = (
                    results[exp_name]
                    .get("corruption_results", {})
                    .get(ctype, {})
                    .get("ensemble_acc", 0)
                )
                row.append(acc)
            data.append(row)

        data = np.array(data)

        fig, ax = plt.subplots(figsize=(12, max(4, len(exp_names))))
        im = ax.imshow(data, cmap="RdYlGn", aspect="auto", vmin=0, vmax=100)

        ax.set_xticks(np.arange(len(corruption_types)))
        ax.set_yticks(np.arange(len(exp_names)))
        ax.set_xticklabels(corruption_types, rotation=45, ha="right")
        ax.set_yticklabels(exp_names)

        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(len(exp_names)):
            for j in range(len(corruption_types)):
                ax.text(j, i, f"{data[i, j]:.1f}", ha="center", va="center", fontsize=8)

        ax.set_title("Robustness to Corruptions (Accuracy %)")
        plt.colorbar(im, ax=ax, label="Accuracy (%)")

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        get_logger().info(f"ğŸ“Š Saved: {filename}")

    def plot_fairness_radar(
        self, results: Dict[str, Dict], filename: str = "fairness.png"
    ):
        """å…¬å¹³æ€§é›·è¾¾å›¾"""
        import matplotlib.pyplot as plt

        names = list(results.keys())
        metrics = ["balanced_acc", "fairness_score", "worst_class_acc"]
        labels = ["Balanced Acc", "Fairness Score", "Worst Class Acc"]

        fig, ax = plt.subplots(figsize=(10, 6))

        x = np.arange(len(labels))
        width = 0.8 / len(names)

        colors = plt.cm.Set2(np.linspace(0, 1, len(names)))

        for i, (name, result) in enumerate(results.items()):
            std_metrics = result.get("standard_metrics", {})
            values = [std_metrics.get(m, 0) for m in metrics]
            ax.bar(x + i * width, values, width, label=name, color=colors[i])

        ax.set_ylabel("Score")
        ax.set_title("Fairness Metrics Comparison")
        ax.set_xticks(x + width * (len(names) - 1) / 2)
        ax.set_xticklabels(labels)
        ax.legend()
        ax.set_ylim(0, 100)
        ax.grid(axis="y", alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        get_logger().info(f"ğŸ“Š Saved: {filename}")

    def plot_training_time(
        self, results: Dict[str, Dict], filename: str = "training_time.png"
    ):
        """è®­ç»ƒæ—¶é—´å¯¹æ¯”"""
        import matplotlib.pyplot as plt

        names = list(results.keys())
        times = [
            r.get("training_time_seconds", 0) / 60 for r in results.values()
        ]  # è½¬æ¢ä¸ºåˆ†é’Ÿ

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(names, times, color="#34495e")

        ax.set_ylabel("Training Time (minutes)")
        ax.set_title("Training Time Comparison")
        ax.tick_params(axis="x", rotation=45)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, t in zip(bars, times):
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() + 0.5,
                f"{t:.1f}m",
                ha="center",
                va="bottom",
                fontsize=9,
            )

        plt.tight_layout()
        plt.savefig(self.save_dir / filename, dpi=150)
        plt.close()
        get_logger().info(f"ğŸ“Š Saved: {filename}")

    def generate_all(self, results: Dict[str, Dict]):
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        self.plot_accuracy_comparison(results)
        self.plot_calibration_comparison(results)
        self.plot_diversity_comparison(results)
        self.plot_fairness_radar(results)
        self.plot_training_time(results)

        # å¦‚æœæœ‰corruptionç»“æœï¼Œç”Ÿæˆçƒ­åŠ›å›¾
        if any(r.get("corruption_results") for r in results.values()):
            self.plot_robustness_heatmap(results)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è¯„ä¼°ç»“æœä¿å­˜å™¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ResultsSaver:
    """è¯„ä¼°ç»“æœä¿å­˜å™¨

    æ”¯æŒå°†è¯„ä¼°æŒ‡æ ‡ä¿å­˜ä¸º JSON å’Œ CSV æ ¼å¼ã€‚
    """

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)

    def save_metrics(self, metrics: Dict[str, Any], filename: str = "metrics"):
        """ä¿å­˜å•ä¸ªå®éªŒçš„æŒ‡æ ‡"""
        json_path = self.save_dir / f"{filename}.json"
        with open(json_path, "w") as f:
            json.dump(metrics, f, indent=2)

        csv_path = self.save_dir / f"{filename}.csv"
        with open(csv_path, "w", newline="") as f:
            import csv

            writer = csv.writer(f)
            writer.writerow(["Metric", "Value"])
            for key, value in metrics.items():
                writer.writerow([key, value])

        get_logger().info(f"ğŸ’¾ Metrics saved to: {json_path}")

    def save_comparison(self, results: Dict[str, Dict], filename: str = "comparison"):
        """ä¿å­˜å¤šä¸ªå®éªŒçš„å¯¹æ¯”ç»“æœ"""
        json_path = self.save_dir / f"{filename}.json"
        with open(json_path, "w") as f:
            json.dump(
                results,
                f,
                indent=2,
                default=lambda x: x.item() if hasattr(x, "item") else x,
            )

        csv_path = self.save_dir / f"{filename}.csv"
        with open(csv_path, "w", newline="") as f:
            import csv

            if results:
                all_metrics = set()
                for exp_results in results.values():
                    all_metrics.update(exp_results.keys())
                all_metrics = sorted(all_metrics)

                writer = csv.writer(f)
                writer.writerow(["Experiment"] + list(all_metrics))

                for exp_name, exp_metrics in results.items():
                    row = [exp_name] + [exp_metrics.get(m, "") for m in all_metrics]
                    writer.writerow(row)

        get_logger().info(f"ğŸ’¾ Comparison saved to: {json_path}")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ æŠ¥å‘Šç”Ÿæˆå™¨ (è¯„ä¼° + æŠ¥å‘Š)                                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ReportGenerator:
    """å®éªŒè¯„ä¼°ä¸æŠ¥å‘Šç”Ÿæˆå™¨

    ä½¿ç”¨æ–¹å¼:
        ReportGenerator.evaluate_and_report(
            trainers=[trainer1, trainer2],
            test_loader=test_loader,
            cfg=cfg,
            save_dir=cfg.save_dir,
            corruption_dataset=corruption_ds,  # å¯é€‰
            run_gradcam=True,                  # å¯é€‰
        )
    """

    @staticmethod
    def _get_rank_marker(
        value: float, all_values: List[float], higher_is_better: bool
    ) -> str:
        """è·å–æ’åæ ‡è®° (ä»…å¤šå®éªŒæ—¶æ˜¾ç¤º)"""
        if len(all_values) <= 1:
            return ""
        sorted_values = sorted(all_values, reverse=higher_is_better)
        if value == sorted_values[0]:
            return " ğŸ¥‡"
        elif value == sorted_values[1]:
            return " ğŸ¥ˆ"
        return ""

    @staticmethod
    def _evaluate_models(
        models: List[nn.Module],
        exp_name: str,
        test_loader: DataLoader,
        cfg: Config,
        device: torch.device,
        training_time: float = 0.0,
        corruption_dataset: Optional[CorruptionDataset] = None,
        run_gradcam: bool = False,
    ) -> Dict[str, Any]:
        """é€šç”¨æ¨¡å‹è¯„ä¼°æ–¹æ³• - æ ¸å¿ƒè¯„ä¼°é€»è¾‘

        Args:
            models: æ¨¡å‹åˆ—è¡¨
            exp_name: å®éªŒåç§°
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            cfg: é…ç½®å¯¹è±¡ (åŒ…å« ensemble_strategy)
            device: è®¡ç®—è®¾å¤‡
            training_time: è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
            corruption_dataset: Corruption æ•°æ®é›† (å¯é€‰)
            run_gradcam: æ˜¯å¦è¿è¡Œ Grad-CAM åˆ†æ

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        get_logger().info(f"\nğŸ“Š Evaluating: {exp_name}")

        # è·å–é›†æˆç­–ç•¥
        ensemble_fn = get_ensemble_fn(cfg)

        # æ ‡å‡†è¯„ä¼°
        get_logger().info("   ğŸ” Standard evaluation...")
        all_logits, all_targets = get_all_models_logits(models, test_loader, device)
        metrics_calc = MetricsCalculator(cfg.num_classes, cfg.ece_n_bins)
        standard_metrics = metrics_calc.calculate_all_metrics(
            all_logits, all_targets, ensemble_fn=ensemble_fn
        )

        get_logger().info(f"   Ensemble Acc:   {standard_metrics['ensemble_acc']:.2f}%")
        get_logger().info(f"   ECE:            {standard_metrics['ece']:.4f}")

        # Corruption è¯„ä¼°
        corruption_results = None
        if corruption_dataset is not None:
            get_logger().info("   ğŸ” Corruption evaluation...")
            corruption_results = evaluate_corruption(
                models, corruption_dataset, batch_size=cfg.batch_size
            )

        # Grad-CAM åˆ†æ
        gradcam_metrics = None
        if run_gradcam:
            get_logger().info("   ğŸ” Grad-CAM analysis...")
            workers = [ModelListWrapper(models)]
            gradcam_analyzer = GradCAMAnalyzer(cfg)
            gradcam_metrics = gradcam_analyzer.analyze_ensemble_quality(
                workers, test_loader, num_samples=50, image_size=cfg.image_size
            )

        return {
            "experiment_name": exp_name,
            "training_time_seconds": training_time,
            "standard_metrics": standard_metrics,
            "corruption_results": corruption_results,
            "gradcam_metrics": gradcam_metrics,
        }

    @staticmethod
    def _evaluate_trainer(
        trainer: Any,
        test_loader: DataLoader,
        cfg: Config,
        corruption_dataset: Optional[CorruptionDataset] = None,
        run_gradcam: bool = False,
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ª trainer å¹¶è¿”å›ç»“æœå­—å…¸"""
        # ä» trainer æå–æ¨¡å‹
        models, device = extract_models(trainer)
        return ReportGenerator._evaluate_models(
            models=models,
            exp_name=trainer.name,
            test_loader=test_loader,
            cfg=cfg,
            device=device,
            training_time=getattr(trainer, "total_training_time", 0.0),
            corruption_dataset=corruption_dataset,
            run_gradcam=run_gradcam,
        )

    @classmethod
    def _generate_report(cls, results: Dict[str, Any]) -> str:
        """ç”ŸæˆæŠ¥å‘Šå­—ç¬¦ä¸²"""
        lines = []
        log = lambda s="": lines.append(str(s))

        exp_names = list(results.keys())
        is_single = len(exp_names) == 1

        # æ ‡é¢˜
        log("=" * 115)
        if is_single:
            log(f"ğŸ“Š EXPERIMENT RESULTS: {exp_names[0]}")
        else:
            log("ğŸ“Š EXPERIMENTAL RESULTS COMPARISON")
            log(
                "   ğŸ¥‡ = Best, ğŸ¥ˆ = Second Best | â†‘ = Higher is better, â†“ = Lower is better"
            )
        log("=" * 115)

        # è¡¨æ ¼
        log("\nğŸ¯ Performance Metrics")
        log("-" * 115)
        log(
            f"{'Experiment':<25} | {'EnsAccâ†‘':<10} | {'AvgIndâ†‘':<10} | {'Oracleâ†‘':<10} | {'ECEâ†“':<10} | {'NLLâ†“':<10} | {'Time':<12}"
        )
        log("-" * 115)

        acc_vals = [
            results[n].get("standard_metrics", {}).get("ensemble_acc", 0)
            for n in exp_names
        ]

        for name in exp_names:
            m = results[name].get("standard_metrics", {})
            t = format_duration(
                results[name].get(
                    "training_time_seconds", results[name].get("training_time", 0)
                )
            )
            acc = m.get("ensemble_acc", 0)
            mark = cls._get_rank_marker(acc, acc_vals, True)
            log(
                f"{name:<25} | {acc:<7.2f}{mark:<3} | {m.get('avg_individual_acc', 0):<10.2f} | "
                f"{m.get('oracle_acc', 0):<10.2f} | {m.get('ece', 0):<10.4f} | {m.get('nll', 0):<10.4f} | {t:<12}"
            )
        log("-" * 115)

        # è¯¦ç»†æŒ‡æ ‡ (æ¯ä¸ªå®éªŒä¾æ¬¡å±•ç¤º)
        log("\nğŸ“‹ Detailed Metrics")
        log("=" * 115)

        for name in exp_names:
            m = results[name].get("standard_metrics", {})
            log(f"\nğŸ”¹ {name}")
            log("-" * 40)

            # Diversity
            log("   ğŸ”€ Diversity & Confidence")
            log(
                f"      Disagreement: {m.get('disagreement', 0):.2f}%  |  JSæ•£åº¦: {m.get('js_divergence', 0):.4f}  |  Diversity: {m.get('diversity', 0):.6f}"
            )
            log(
                f"      Confidence: avg={m.get('avg_confidence', 0):.4f}, correct={m.get('avg_correct_confidence', 0):.4f}, incorrect={m.get('avg_incorrect_confidence', 0):.4f}"
            )

            # Fairness
            log("\n   âš–ï¸ Fairness")
            log(
                f"      Balanced Acc: {m.get('balanced_acc', 0):.2f}%  |  Disparity: {m.get('acc_disparity', 0):.2f}%  |  Score: {m.get('fairness_score', 0):.2f}"
            )
            log("-" * 40)

        # Corruption
        has_corruption = any(results[n].get("corruption_results") for n in exp_names)
        if has_corruption:
            log("\nğŸ§ª Corruption Robustness")
            log("-" * 60)
            overall_vals = [
                results[n].get("corruption_results", {}).get("overall_avg", 0)
                for n in exp_names
                if results[n].get("corruption_results")
            ]
            for name in exp_names:
                c = results[name].get("corruption_results", {})
                if c and "overall_avg" in c:
                    val = c["overall_avg"]
                    mark = cls._get_rank_marker(val, overall_vals, True)
                    log(f"   {name:<25} | Overall: {val:.2f}%{mark}")
            log("-" * 60)

        log("\n" + "=" * 115)
        return "\n".join(lines)

    @classmethod
    def _save_and_print(cls, results: Dict[str, Dict], save_dir: str):
        """ä¿å­˜å¹¶æ‰“å°æŠ¥å‘Š"""
        saver = ResultsSaver(save_dir)
        report_content = cls._generate_report(results)

        # ä¿å­˜ç»“æœ (ç»Ÿä¸€æ ¼å¼)
        saver.save_comparison(results, "comprehensive_results")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (ä¸æ‰“å°åˆ°æ§åˆ¶å°)
        report_path = Path(save_dir) / "detailed_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        get_logger().info(f"\nâœ… Detailed report saved to: {report_path}")
        get_logger().info(f"âœ… All results saved to: {save_dir}")

    @classmethod
    def evaluate_and_report(
        cls,
        trainers: List["StagedEnsembleTrainer"],
        test_loader: DataLoader,
        cfg: Config,
        save_dir: str,
        corruption_dataset: Optional[CorruptionDataset] = None,
        run_gradcam: bool = False,
    ):
        """è¯„ä¼°å¤šä¸ª trainer å¹¶ç”ŸæˆæŠ¥å‘Š (ä¸€æ­¥å®Œæˆ)

        Args:
            trainers: trainer åˆ—è¡¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            cfg: é…ç½®å¯¹è±¡ (åŒ…å« ensemble_strategy)
            save_dir: æŠ¥å‘Šä¿å­˜ç›®å½•
            corruption_dataset: Corruption æ•°æ®é›† (å¯é€‰)
            run_gradcam: æ˜¯å¦è¿è¡Œ Grad-CAM åˆ†æ
        """
        get_logger().info(
            f"\n{'=' * 80}\nğŸ“Š EVALUATION MODE | Models: {len(trainers)}\n{'=' * 80}"
        )

        # è¯„ä¼°æ‰€æœ‰ trainers
        results = {}
        for idx, trainer in enumerate(trainers, 1):
            get_logger().info(f"\n[{idx}/{len(trainers)}] {trainer.name}")
            result = cls._evaluate_trainer(
                trainer, test_loader, cfg, corruption_dataset, run_gradcam
            )
            results[trainer.name] = result

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        get_logger().info("\nğŸ“Š Generating visualizations...")
        visualizer = ReportVisualizer(save_dir)
        visualizer.generate_all(results)

        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        cls._save_and_print(results, save_dir)

    @classmethod
    def generate_from_checkpoints(
        cls,
        checkpoint_paths: List[str],
        test_loader: DataLoader,
        cfg: Config,
        output_dir: str,
        corruption_dataset: Optional[CorruptionDataset] = None,
        run_gradcam: bool = False,
        run_loss_landscape: bool = False,
    ):
        """
        ä» checkpoint ç›´æ¥è¯„ä¼°å¹¶ç”Ÿæˆå®Œæ•´å¯è§†åŒ–æŠ¥å‘Š

        è¿™æ˜¯ evaluation æ¨¡å—çš„ä¸»å…¥å£ï¼Œå®Œå…¨ç‹¬ç«‹äº training æ¨¡å—ã€‚

        Args:
            checkpoint_paths: checkpoint ç›®å½•è·¯å¾„åˆ—è¡¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            cfg: é…ç½®å¯¹è±¡ (åŒ…å« ensemble_strategy)
            output_dir: è¾“å‡ºç›®å½•
            corruption_dataset: Corruption æ•°æ®é›† (å¯é€‰)
            run_gradcam: æ˜¯å¦è¿è¡Œ Grad-CAM åˆ†æ
            run_loss_landscape: æ˜¯å¦è¿è¡Œ Loss Landscape åˆ†æ

        è¾“å‡º:
            output_dir/
            â”œâ”€â”€ detailed_report.txt      # æ–‡æœ¬æŠ¥å‘Š
            â”œâ”€â”€ accuracy_comparison.png  # å‡†ç¡®ç‡å¯¹æ¯”
            â”œâ”€â”€ calibration.png          # æ ¡å‡†æŒ‡æ ‡
            â”œâ”€â”€ diversity.png            # å¤šæ ·æ€§æŒ‡æ ‡
            â”œâ”€â”€ fairness.png             # å…¬å¹³æ€§æŒ‡æ ‡
            â”œâ”€â”€ training_time.png        # è®­ç»ƒæ—¶é—´
            â”œâ”€â”€ robustness.png           # é²æ£’æ€§çƒ­åŠ›å›¾ (å¦‚æœ‰)
            â”œâ”€â”€ model_distances.png      # æ¨¡å‹å‚æ•°è·ç¦» (å¦‚æœ‰)
            â”œâ”€â”€ ensemble_loss_landscape.png  # Loss Landscape (å¦‚æœ‰)
            â””â”€â”€ final_metrics.json       # æŒ‡æ ‡æ•°æ®
        """
        get_logger().info(f"\n{'=' * 80}")
        get_logger().info(
            f"ğŸ“Š EVALUATION FROM CHECKPOINTS | Count: {len(checkpoint_paths)}"
        )
        get_logger().info(f"{'=' * 80}")

        ensure_dir(output_dir)
        results = {}
        all_models = {}  # æ”¶é›†æ‰€æœ‰å®éªŒçš„æ¨¡å‹ç”¨äº Loss Landscape
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        for idx, ckpt_path in enumerate(checkpoint_paths, 1):
            get_logger().info(f"\n[{idx}/{len(checkpoint_paths)}] Loading: {ckpt_path}")

            # åŠ è½½æ¨¡å‹
            ctx = CheckpointLoader.load(ckpt_path, cfg)
            exp_name = ctx["name"]
            models = [m.to(device) for m in ctx["models"]]
            all_models[exp_name] = models  # ä¿å­˜ç”¨äºåç»­åˆ†æ

            # ä½¿ç”¨é€šç”¨è¯„ä¼°æ–¹æ³•
            result = cls._evaluate_models(
                models=models,
                exp_name=exp_name,
                test_loader=test_loader,
                cfg=cfg,
                device=device,
                training_time=ctx["training_time"],
                corruption_dataset=corruption_dataset,
                run_gradcam=run_gradcam,
            )
            result["train_config"] = ctx["config"]
            results[exp_name] = result

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        get_logger().info("\nğŸ“Š Generating visualizations...")
        visualizer = ReportVisualizer(output_dir)
        visualizer.generate_all(results)

        # Loss Landscape åˆ†æ
        if run_loss_landscape and all_models:
            get_logger().info("\nğŸ”ï¸ Generating Loss Landscape visualizations...")
            landscape_viz = LossLandscapeVisualizer(output_dir)

            for exp_name, models in all_models.items():
                # æ¨¡å‹å‚æ•°è·ç¦»çƒ­åŠ›å›¾ (æ— éœ€ loss-landscapes ä¾èµ–)
                landscape_viz.plot_model_distance_heatmap(
                    models, filename=f"{exp_name}_model_distances.png"
                )

                # Loss Landscape æ’å€¼ (éœ€è¦ loss-landscapes)
                landscape_viz.plot_ensemble_interpolations(
                    models,
                    test_loader,
                    device,
                    filename=f"{exp_name}_loss_landscape.png",
                )

                # 2D/3D è¡¨é¢å›¾ - ä¸ºç¬¬ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆ (è®¡ç®—é‡è¾ƒå¤§)
                if len(models) > 0:
                    landscape_viz.plot_2d_plane(
                        models[0],
                        test_loader,
                        device,
                        distance=1.0,
                        steps=20,  # å‡å°‘æ­¥æ•°ä»¥åŠ å¿«è®¡ç®—
                        filename=f"{exp_name}_landscape_surface.png",
                        model_name=f"{exp_name}_M1",
                    )

        # ç”Ÿæˆå¹¶ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        cls._save_and_print(results, output_dir)

        get_logger().info(f"\nâœ… Complete! All reports saved to: {output_dir}")
        return results
