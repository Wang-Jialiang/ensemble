"""
================================================================================
ä¸»å…¥å£æ¨¡å— - CLIè§£æå’Œç¨‹åºå…¥å£
================================================================================
"""

import sys

sys.dont_write_bytecode = True  # ç¦ç”¨ __pycache__ ç”Ÿæˆ

import argparse
import datetime
from pathlib import Path

from .config import Config
from .datasets import load_dataset
from .evaluation import ReportGenerator
from .training import train_experiment
from .utils import ensure_dir, get_logger, set_seed


def main():
    """NDE ç³»ç»Ÿä¸»å…¥å£: è§£æ -> åˆå§‹åŒ– -> åˆ†å‘"""
    args = _parse_args()
    cfg = _init_config(args)
    set_seed(cfg.seed)

    log = get_logger()
    log.info(f"ğŸš€ NDE System | Model: {cfg.model_name} | Models: {cfg.total_models}")

    if args.eval:
        _run_evaluation(cfg, args)
    else:
        _run_training(cfg)


def _parse_args():
    p = argparse.ArgumentParser(description="NDE è®­ç»ƒç³»ç»Ÿ")
    p.add_argument("--config", type=str, default="config/default.yaml")
    p.add_argument("--eval", action="store_true")
    p.add_argument("--quick-test", action="store_true")
    return p.parse_args()


def _init_config(args):
    path = Path(args.config)
    if not path.exists():
        path = Path(__file__).parent / args.config

    base_cfg, experiments, eval_ckpts = Config.load_yaml(str(path))
    if args.quick_test:
        base_cfg.quick_test = True

    # å°†å®éªŒåˆ—è¡¨æŒ‚è½½åˆ°é…ç½®å¯¹è±¡ä¸Šä¾¿äºåç»­ä¼ é€’ (ä¸´æ—¶)
    base_cfg._experiments = experiments
    base_cfg._eval_ckpts = eval_ckpts
    return base_cfg


def _run_training(cfg):
    log = get_logger()
    log.info(f"ğŸš‚ Training Mode | Experiments: {len(cfg._experiments)}")

    train_loader, val_loader = load_dataset(cfg, mode="train")

    generated_ckpts = []

    # ç”Ÿæˆç»Ÿä¸€çš„è®­ç»ƒæ‰¹æ¬¡ç›®å½• (æ‰€æœ‰å®éªŒå…±äº«)
    batch_ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    batch_dir = Path(cfg.save_root) / "training" / batch_ts
    ensure_dir(batch_dir)
    log.info(f"ğŸ“ Training batch dir: {batch_dir}")

    for exp in cfg._experiments:
        log.info(f"\nğŸ§ª Running: {exp.name}")
        c = cfg.apply_quick_test() if cfg.quick_test else cfg
        c = c.copy(experiment_name=exp.name, **exp.get_config_overrides())

        # æ¯ä¸ªå®éªŒä½œä¸ºå­ç›®å½•
        c.save_dir = str(batch_dir / exp.name)
        ensure_dir(c.save_dir)

        train_experiment(cfg=c, train_loader=train_loader, val_loader=val_loader)

        # æ”¶é›† checkpoint è·¯å¾„
        # å®é™…è·¯å¾„: batch_dir/exp_name/checkpoints/best_acc.pt
        ckpt_path = Path(c.save_dir) / "checkpoints" / "best_acc.pt"
        if ckpt_path.exists():
            generated_ckpts.append(
                {"name": f"{exp.name}", "path": str(ckpt_path), "model": c.model_name}
            )

    return generated_ckpts


def _run_evaluation(cfg, args):
    if not cfg._eval_ckpts:
        get_logger().error("âŒ No checkpoints for evaluation")
        return

    test_loader, c_ds, o_ds = load_dataset(cfg, mode="eval")
    ckpts = [c["path"] for c in cfg._eval_ckpts]

    ReportGenerator.evaluate_checkpoints(
        checkpoint_paths=ckpts,
        test_loader=test_loader,
        cfg=cfg,
        corruption_dataset=c_ds,
        ood_dataset=o_ds,
        run_gradcam=cfg.eval_run_gradcam,
        run_loss_landscape=cfg.eval_run_landscape,
    )


if __name__ == "__main__":
    main()
