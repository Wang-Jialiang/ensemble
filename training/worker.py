"""
================================================================================
GPU Worker æ¨¡å—
================================================================================

GPUWorker (å•GPUæ¨¡å‹ç®¡ç†å™¨)ã€HistorySaver (è®­ç»ƒå†å²ä¿å­˜å™¨)
"""

from pathlib import Path
from typing import Dict, List, Optional

import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import GradScaler, autocast

from ..config import Config
from ..models import ModelFactory
from ..utils import ensure_dir, get_logger
from .augmentation import AUGMENTATION_REGISTRY, AugmentationMethod
from .scheduler import create_optimizer, create_scheduler

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ GPU Worker                                                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GPUWorker:
    """å•GPUæ¨¡å‹ç®¡ç†å™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    ç®¡ç†å•ä¸ªGPUä¸Šçš„å¤šä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¼‚æ­¥è®­ç»ƒä»¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ã€‚
    """

    def __init__(
        self,
        gpu_id: int,
        num_models: int,
        cfg: Config,
        augmentation_method: str = "perlin",
    ):
        self.gpu_id = gpu_id
        self.device = torch.device(f"cuda:{gpu_id}")
        self.cfg = cfg
        self.num_models = num_models

        # åˆ›å»ºæ¨¡å‹
        self.models: List[nn.Module] = []
        self.optimizers: List[optim.Optimizer] = []
        self.schedulers: List[Optional[optim.lr_scheduler.LRScheduler]] = []

        for _ in range(num_models):
            model = ModelFactory.create_model(
                cfg.model_name,
                num_classes=cfg.num_classes,
                init_method=cfg.init_method,
            )
            model = model.to(self.device)

            if cfg.compile_model and hasattr(torch, "compile"):
                model = torch.compile(model)

            # ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
            optimizer = create_optimizer(model, cfg.optimizer, cfg.lr, cfg.weight_decay)
            scheduler = create_scheduler(
                optimizer,
                cfg.scheduler,
                cfg.total_epochs,
                max_lr_factor=getattr(cfg, "onecycle_max_lr_factor", 10.0),
            )

            self.models.append(model)
            self.optimizers.append(optimizer)
            self.schedulers.append(scheduler)

        # åˆ›å»ºå¢å¼ºæ–¹æ³•
        self.augmentation_method = augmentation_method
        self.augmentation = self._create_augmentation(augmentation_method)

        # AMP
        self.scaler = GradScaler("cuda") if cfg.use_amp else None

        # Stream
        self.stream = torch.cuda.Stream(device=self.device)
        self._pending_loss = None

    def _create_augmentation(self, method: str) -> AugmentationMethod:
        """åˆ›å»ºå¢å¼ºæ–¹æ³•"""
        if method not in AUGMENTATION_REGISTRY:
            raise ValueError(
                f"ä¸æ”¯æŒçš„å¢å¼ºæ–¹æ³•: {method}. æ”¯æŒ: {list(AUGMENTATION_REGISTRY.keys())}"
            )
        return AUGMENTATION_REGISTRY[method](self.device, self.cfg)

    def precompute_masks(self, num_masks: int, target_ratio: float):
        """é¢„è®¡ç®—maskï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if hasattr(self.augmentation, "precompute_masks"):
            self.augmentation.precompute_masks(target_ratio)

    def train_batch_async(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        criterion: nn.Module,
        mask_ratio: float,
        mask_prob: float,
        use_mask: bool,
    ):
        """å¼‚æ­¥è®­ç»ƒä¸€ä¸ªbatch"""
        with torch.cuda.stream(self.stream):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            total_loss = 0.0

            for model, optimizer in zip(self.models, self.optimizers):
                model.train()
                optimizer.zero_grad(set_to_none=True)

                # åº”ç”¨å¢å¼º
                if use_mask:
                    aug_inputs, aug_targets = self.augmentation.apply(
                        inputs, targets, mask_ratio, mask_prob
                    )
                else:
                    aug_inputs, aug_targets = inputs, targets

                # å‰å‘ä¼ æ’­
                if self.scaler:
                    with autocast("cuda"):
                        outputs = model(aug_inputs)
                        loss = criterion(outputs, aug_targets)
                    self.scaler.scale(loss).backward()
                    self.scaler.unscale_(optimizer)
                    nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)
                    self.scaler.step(optimizer)
                    self.scaler.update()
                else:
                    outputs = model(aug_inputs)
                    loss = criterion(outputs, aug_targets)
                    loss.backward()
                    nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)
                    optimizer.step()

                total_loss += loss.item()

            self._pending_loss = total_loss / self.num_models

    def synchronize(self) -> float:
        """åŒæ­¥å¹¶è¿”å›å¹³å‡loss"""
        self.stream.synchronize()
        return self._pending_loss if self._pending_loss else 0.0

    def step_schedulers(self, val_loss: Optional[float] = None):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨

        Args:
            val_loss: éªŒè¯æŸå¤± (ç”¨äº ReduceLROnPlateau)
        """
        for scheduler in self.schedulers:
            if scheduler is None:
                continue
            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                if val_loss is not None:
                    scheduler.step(val_loss)
            else:
                scheduler.step()

    def set_lr(self, lr: float):
        """è®¾ç½®æ‰€æœ‰æ¨¡å‹çš„å­¦ä¹ ç‡

        Args:
            lr: æ–°çš„å­¦ä¹ ç‡
        """
        for optimizer in self.optimizers:
            for param_group in optimizer.param_groups:
                param_group["lr"] = lr

    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizers[0].param_groups[0]["lr"] if self.optimizers else 0.0

    def predict_batch(self, inputs: torch.Tensor) -> torch.Tensor:
        """æ‰¹é‡é¢„æµ‹"""
        inputs = inputs.to(self.device)
        all_logits = []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                logits = model(inputs)
            all_logits.append(logits.unsqueeze(0))
        return torch.cat(all_logits, dim=0)

    def save_models(self, save_dir: str, prefix: str):
        """ä¿å­˜æ¨¡å‹"""
        for i, (model, optimizer, scheduler) in enumerate(
            zip(self.models, self.optimizers, self.schedulers)
        ):
            state = {
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
            }
            save_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            torch.save(state, save_path)

    def load_models(self, save_dir: str, prefix: str):
        """åŠ è½½æ¨¡å‹"""
        for i, (model, optimizer, scheduler) in enumerate(
            zip(self.models, self.optimizers, self.schedulers)
        ):
            load_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            if load_path.exists():
                state = torch.load(
                    load_path, map_location=self.device, weights_only=False
                )
                model.load_state_dict(state["model_state_dict"])
                optimizer.load_state_dict(state["optimizer_state_dict"])
                if scheduler and state.get("scheduler_state_dict"):
                    scheduler.load_state_dict(state["scheduler_state_dict"])

    def broadcast_backbone_and_reinit_heads(self, backbone_state_dict: dict):
        """ç”¨å…±äº« backbone åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼Œå¹¶é‡æ–°åˆå§‹åŒ–å„æ¨¡å‹çš„ classifier head

        Args:
            backbone_state_dict: æºæ¨¡å‹çš„ backbone æƒé‡ (ä¸å« fc å±‚)
        """
        for model in self.models:
            # åŠ è½½ backbone æƒé‡ (strict=False å› ä¸ºä¸å« fc å±‚)
            model.load_state_dict(backbone_state_dict, strict=False)
            # é‡æ–°åˆå§‹åŒ– classifier head
            model.reinit_classifier()


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è®­ç»ƒå†å²ä¿å­˜å™¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class HistorySaver:
    """è®­ç»ƒå†å²ä¿å­˜å™¨"""

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)

    def save(self, history: Dict[str, List], filename: str = "history"):
        """ä¿å­˜è®­ç»ƒå†å²ä¸ºJSONå’ŒCSV"""
        import csv
        import json

        json_path = self.save_dir / f"{filename}.json"
        with open(json_path, "w") as f:
            json.dump(history, f, indent=2)

        csv_path = self.save_dir / f"{filename}.csv"
        with open(csv_path, "w", newline="") as f:
            if history:
                writer = csv.DictWriter(f, fieldnames=history.keys())
                writer.writeheader()
                for i in range(len(history[list(history.keys())[0]])):
                    row = {k: v[i] for k, v in history.items()}
                    writer.writerow(row)
        get_logger().info(f"ğŸ’¾ History saved to: {json_path}")
