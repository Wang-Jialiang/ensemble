"""
================================================================================
GPU Worker æ¨¡å—
================================================================================

GPUWorker (å•GPUæ¨¡å‹ç®¡ç†å™¨)ã€HistorySaver (è®­ç»ƒå†å²ä¿å­˜å™¨)
"""

from pathlib import Path
from typing import Dict, List, Tuple

import torch
import torch.nn as nn
from torch.amp import autocast

from ..config import Config
from ..models import ModelFactory
from ..utils import ensure_dir, get_logger
from .augmentation import AUGMENTATION_REGISTRY
from .optimization import create_optimizer, create_scheduler

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ GPU Worker                                                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GPUWorker:
    """å•GPUæ¨¡å‹ç®¡ç†å™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    ç®¡ç†å•ä¸ªGPUä¸Šçš„å¤šä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¼‚æ­¥è®­ç»ƒä»¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ã€‚
    """

    def __init__(
        self,
        gpu_id: int,
        num_models: int,
        cfg: Config,
        augmentation_method: str = "perlin",
    ):
        """GPU Worker æ„é€ å‡½æ•° (å¤§çº²åŒ–)"""
        self.gpu_id = gpu_id
        self.device = torch.device(f"cuda:{gpu_id}")
        self.cfg, self.num_models = cfg, num_models

        # 1. åˆå§‹åŒ–æ·±åº¦å­¦ä¹ ç»„ä»¶ (æ¨¡å‹, ä¼˜åŒ–å™¨, è°ƒåº¦å™¨)
        self.models, self.optimizers, self.schedulers = self._setup_models_and_optim()

        # 2. åˆå§‹åŒ–æ•°æ®å¢å¼ºå¼•æ“
        self._init_augmentation(augmentation_method)

        # 3. åˆå§‹åŒ–å¼‚æ­¥æ‰§è¡Œæµæ°´çº¿ (Stream)
        self.stream = torch.cuda.Stream(device=self.device)
        self._pending_loss = None

    def _setup_models_and_optim(self) -> Tuple[list, list, list]:
        """æ‰¹é‡åˆ›å»ºæ¨¡å‹åŠé…å¥—ä¼˜åŒ–å·¥å…·"""
        ms, os, ss = [], [], []
        for _ in range(self.num_models):
            m = ModelFactory.create_model(
                self.cfg.model_name, self.cfg.num_classes, self.cfg.init_method
            ).to(self.device)
            if self.cfg.compile_model and hasattr(torch, "compile"):
                m = torch.compile(m)

            opt = create_optimizer(
                m,
                self.cfg.optimizer,
                self.cfg.lr,
                self.cfg.weight_decay,
                sgd_momentum=self.cfg.sgd_momentum,
            )
            sch = create_scheduler(
                opt, self.cfg.scheduler, self.cfg.total_epochs, self.cfg.min_lr
            )

            ms.append(m)
            os.append(opt)
            ss.append(sch)
        return ms, os, ss

    def _init_augmentation(self, method):
        """é…ç½®å¢å¼ºå®ä¾‹åŠå…¶å›ºå®šç§å­æ± """
        if method not in AUGMENTATION_REGISTRY:
            raise ValueError(f"ä¸æ”¯æŒçš„å¢å¼ºæ–¹æ³•: {method}")

        self.augmentation = AUGMENTATION_REGISTRY[method](self.device, self.cfg)

    def precompute_masks(self, target_ratio: float):
        """é¢„è®¡ç®— mask æ± 

        æ¯ä¸ª epoch ç”¨å½“å‰ ratio é¢„è®¡ç®—å…±äº« mask æ± ã€‚
        """
        if hasattr(self.augmentation, "precompute_masks"):
            self.augmentation.precompute_masks(target_ratio)

    def train_batch_async(
        self, inputs, targets, criterion, m_ratio, m_prob, use_mask, model_indices=None
    ):
        """æ‰§è¡Œå¼‚æ­¥æ‰¹æ¬¡è®­ç»ƒ (å¤§çº²åŒ–)

        Args:
            model_indices: å¯é€‰ï¼ŒæŒ‡å®šè¦è®­ç»ƒçš„æ¨¡å‹ç´¢å¼•åˆ—è¡¨ã€‚None è¡¨ç¤ºè®­ç»ƒå…¨éƒ¨æ¨¡å‹ã€‚
        """
        with torch.cuda.stream(self.stream):
            # 1. æ¬è¿æ•°æ®è‡³æ˜¾å­˜
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            # 2. ç¡®å®šè¦è®­ç»ƒçš„æ¨¡å‹
            indices = (
                list(model_indices)
                if model_indices is not None
                else list(range(self.num_models))
            )

            # 3. è¿­ä»£æŒ‡å®šçš„æ¨¡å‹
            total_loss = 0.0
            for i in indices:
                m, opt = self.models[i], self.optimizers[i]
                total_loss += self._step_model(
                    i, m, opt, inputs, targets, criterion, m_ratio, m_prob, use_mask
                )

            self._pending_loss = total_loss / len(indices) if len(indices) > 0 else 0.0

    def _step_model(
        self,
        idx,
        model,
        optimizer,
        inputs,
        targets,
        criterion,
        m_ratio,
        m_prob,
        use_mask,
    ):
        """æ‰§è¡Œå•ä¸ªæ¨¡å‹çš„æ¢¯åº¦æ›´æ–°æ­¥"""
        model.train()
        optimizer.zero_grad(set_to_none=True)

        # 1. å‡†å¤‡å¢å¼ºæ•°æ®
        x, y = self._prepare_training_data(
            idx, inputs, targets, m_ratio, m_prob, use_mask
        )

        # 2. æ‰§è¡Œå‰å‘ä¸åå‘ä¼ æ’­
        loss = self._forward_backward(model, x, y, criterion)

        # 3. æ¢¯åº¦è£å‰ªä¸å‚æ•°æ›´æ–°
        nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)
        optimizer.step()

        return loss.item()

    def _prepare_training_data(self, idx, x, y, ratio, prob, use_mask):
        """æ ¹æ®ç­–ç•¥åº”ç”¨æ•°æ®å¢å¼º"""
        if not use_mask:
            return x, y

        return self.augmentation.apply(x, y, ratio, prob)

    def _forward_backward(self, model, x, y, criterion):
        """å†…éƒ¨æ‰§è¡Œè®¡ç®—é“¾è·¯"""
        if self.cfg.use_amp:
            with autocast("cuda", dtype=torch.bfloat16):
                loss = criterion(model(x), y)
        else:
            loss = criterion(model(x), y)

        loss.backward()
        return loss

    def synchronize(self) -> float:
        """åŒæ­¥å¹¶è¿”å›å¹³å‡loss"""
        self.stream.synchronize()
        return self._pending_loss if self._pending_loss else 0.0

    def step_schedulers(self):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        for scheduler in self.schedulers:
            if scheduler is not None:
                scheduler.step()

    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizers[0].param_groups[0]["lr"] if self.optimizers else 0.0

    def predict_batch(self, inputs: torch.Tensor) -> torch.Tensor:
        """æ‰¹é‡é¢„æµ‹"""
        inputs = inputs.to(self.device)
        all_logits = []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                logits = model(inputs)
            all_logits.append(logits.unsqueeze(0))
        return torch.cat(all_logits, dim=0)

    def save_models(self, save_dir: str, prefix: str):
        """ä¿å­˜æ¨¡å‹æƒé‡"""
        for i, model in enumerate(self.models):
            save_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            torch.save(model.state_dict(), save_path)

    def load_models(self, save_dir: str, prefix: str):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        for i, model in enumerate(self.models):
            load_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            if load_path.exists():
                state_dict = torch.load(
                    load_path, map_location=self.device, weights_only=False
                )
                model.load_state_dict(state_dict)

    def broadcast_backbone_and_reinit_heads(self, backbone_state_dict: dict):
        """ç”¨å…±äº« backbone åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼Œå¹¶é‡æ–°åˆå§‹åŒ–å„æ¨¡å‹çš„ classifier head

        Args:
            backbone_state_dict: æºæ¨¡å‹çš„ backbone æƒé‡ (ä¸å« fc å±‚)
        """
        for model in self.models:
            # åŠ è½½ backbone æƒé‡ (strict=False å› ä¸ºä¸å« fc å±‚)
            model.load_state_dict(backbone_state_dict, strict=False)
            # é‡æ–°åˆå§‹åŒ– classifier head
            model.reinit_classifier(init_method=self.cfg.init_method)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è®­ç»ƒå†å²ä¿å­˜å™¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class HistorySaver:
    """è®­ç»ƒå†å² CSV ä¿å­˜å™¨ (å¤§çº²åŒ–)"""

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)

    def save(self, history: Dict[str, List], filename: str = "history"):
        """å°†å†å²å­—å…¸å¯¼å‡ºè‡³ CSV æ–‡ä»¶"""
        import csv

        path = self.save_dir / f"{filename}.csv"

        if not history:
            return

        with open(path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=history.keys())
            writer.writeheader()
            self._write_rows(writer, history)

        get_logger().info(f"ğŸ’¾ History saved: {path}")

    def _write_rows(self, writer, history):
        """éå†å¹¶å†™å…¥è¡Œæ•°æ®"""
        num_entries = len(next(iter(history.values())))
        for i in range(num_entries):
            writer.writerow({k: v[i] for k, v in history.items()})
