"""
================================================================================
GPU Worker æ¨¡å—
================================================================================

GPUWorker (å•GPUæ¨¡å‹ç®¡ç†å™¨)ã€HistorySaver (è®­ç»ƒå†å²ä¿å­˜å™¨)
"""

from pathlib import Path
from typing import Dict, List, Optional

import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import autocast

from ..config import Config
from ..models import ModelFactory
from ..utils import ensure_dir, get_logger
from .augmentation import AUGMENTATION_REGISTRY, AugmentationMethod
from .optimization import create_optimizer, create_scheduler

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ GPU Worker                                                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class GPUWorker:
    """å•GPUæ¨¡å‹ç®¡ç†å™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    ç®¡ç†å•ä¸ªGPUä¸Šçš„å¤šä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¼‚æ­¥è®­ç»ƒä»¥æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ã€‚
    """

    def __init__(self, gpu_id: int, num_models: int, cfg: Config, augmentation_method: str = "perlin"):
        """GPU Worker æ„é€ å‡½æ•° (å¤§çº²åŒ–)"""
        self.gpu_id = gpu_id
        self.device = torch.device(f"cuda:{gpu_id}")
        self.cfg, self.num_models = cfg, num_models

        # 1. åˆå§‹åŒ–æ·±åº¦å­¦ä¹ ç»„ä»¶ (æ¨¡å‹, ä¼˜åŒ–å™¨, è°ƒåº¦å™¨)
        self.models, self.optimizers, self.schedulers = self._setup_models_and_optim()

        # 2. åˆå§‹åŒ–æ•°æ®å¢å¼ºå¼•æ“
        self._init_augmentation(augmentation_method)

        # 3. åˆå§‹åŒ–å¼‚æ­¥æ‰§è¡Œæµæ°´çº¿ (Stream)
        self.stream = torch.cuda.Stream(device=self.device)
        self._pending_loss = None

    def _setup_models_and_optim(self) -> Tuple[list, list, list]:
        """æ‰¹é‡åˆ›å»ºæ¨¡å‹åŠé…å¥—ä¼˜åŒ–å·¥å…·"""
        ms, os, ss = [], [], []
        for _ in range(self.num_models):
            m = ModelFactory.create_model(self.cfg.model_name, self.cfg.num_classes, self.cfg.init_method).to(self.device)
            if self.cfg.compile_model and hasattr(torch, "compile"): m = torch.compile(m)
            
            opt = create_optimizer(m, self.cfg.optimizer, self.cfg.lr, self.cfg.weight_decay, sgd_momentum=self.cfg.sgd_momentum)
            sch = create_scheduler(opt, self.cfg.scheduler, self.cfg.total_epochs)
            
            ms.append(m); os.append(opt); ss.append(sch)
        return ms, os, ss

    def _init_augmentation(self, method):
        """é…ç½®å¢å¼ºå®ä¾‹åŠå…¶å›ºå®šç§å­æ± """
        if method not in AUGMENTATION_REGISTRY:
            raise ValueError(f"ä¸æ”¯æŒçš„å¢å¼ºæ–¹æ³•: {method}")
            
        self.augmentation = AUGMENTATION_REGISTRY[method](self.device, self.cfg)
        self._use_model_level = self.cfg.model_level_augmentation
        
        if self._use_model_level:
            self.augmentation.init_model_seeds(num_models=self.num_models)

    def precompute_masks(self, target_ratio: float):
        """é¢„è®¡ç®—maskï¼ˆä»…æ ·æœ¬çº§å¢å¼ºä½¿ç”¨ï¼‰

        æ ·æœ¬çº§å¢å¼ºï¼šæ¯ä¸ª epoch ç”¨å½“å‰ ratio é¢„è®¡ç®—å…±äº« mask æ± 
        æ¨¡å‹çº§å¢å¼ºï¼šè·³è¿‡ï¼Œå› ä¸ºä½¿ç”¨å›ºå®š seed åœ¨ apply æ—¶åŠ¨æ€ç”Ÿæˆ
        """
        if self._use_model_level:
            return
        if hasattr(self.augmentation, "precompute_masks"):
            self.augmentation.precompute_masks(target_ratio)

    def train_batch_async(self, inputs, targets, criterion, m_ratio, m_prob, use_mask):
        """æ‰§è¡Œå¼‚æ­¥æ‰¹æ¬¡è®­ç»ƒ (å¤§çº²åŒ–)"""
        with torch.cuda.stream(self.stream):
            # 1. æ¬è¿æ•°æ®è‡³æ˜¾å­˜
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            # 2. ä¾æ¬¡è¿­ä»£ç®¡ç†çš„æ‰€æœ‰æ¨¡å‹
            total_loss = 0.0
            for i, (m, opt) in enumerate(zip(self.models, self.optimizers)):
                total_loss += self._step_model(i, m, opt, inputs, targets, criterion, m_ratio, m_prob, use_mask)

            self._pending_loss = total_loss / self.num_models

    def _step_model(self, idx, model, optimizer, inputs, targets, criterion, m_ratio, m_prob, use_mask):
        """æ‰§è¡Œå•ä¸ªæ¨¡å‹çš„æ¢¯åº¦æ›´æ–°æ­¥"""
        model.train()
        optimizer.zero_grad(set_to_none=True)

        # 1. å‡†å¤‡å¢å¼ºæ•°æ®
        x, y = self._prepare_training_data(idx, inputs, targets, m_ratio, m_prob, use_mask)

        # 2. æ‰§è¡Œå‰å‘ä¸åå‘ä¼ æ’­
        loss = self._forward_backward(model, x, y, criterion)

        # 3. æ¢¯åº¦è£å‰ªä¸å‚æ•°æ›´æ–°
        nn.utils.clip_grad_norm_(model.parameters(), self.cfg.max_grad_norm)
        optimizer.step()
        
        return loss.item()

    def _prepare_training_data(self, idx, x, y, ratio, prob, use_mask):
        """æ ¹æ®ç­–ç•¥åº”ç”¨æ•°æ®å¢å¼º"""
        if not use_mask: return x, y
        
        model_idx = idx if self._use_model_level else None
        return self.augmentation.apply(x, y, ratio, prob, model_index=model_idx)

    def _forward_backward(self, model, x, y, criterion):
        """å†…éƒ¨æ‰§è¡Œè®¡ç®—é“¾è·¯"""
        if self.cfg.use_amp:
            with autocast("cuda", dtype=torch.bfloat16):
                loss = criterion(model(x), y)
        else:
            loss = criterion(model(x), y)
            
        loss.backward()
        return loss

    def synchronize(self) -> float:
        """åŒæ­¥å¹¶è¿”å›å¹³å‡loss"""
        self.stream.synchronize()
        return self._pending_loss if self._pending_loss else 0.0

    def step_schedulers(self):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        for scheduler in self.schedulers:
            if scheduler is not None:
                scheduler.step()

    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizers[0].param_groups[0]["lr"] if self.optimizers else 0.0

    def predict_batch(self, inputs: torch.Tensor) -> torch.Tensor:
        """æ‰¹é‡é¢„æµ‹"""
        inputs = inputs.to(self.device)
        all_logits = []
        for model in self.models:
            model.eval()
            with torch.no_grad():
                logits = model(inputs)
            all_logits.append(logits.unsqueeze(0))
        return torch.cat(all_logits, dim=0)

    def save_models(self, save_dir: str, prefix: str):
        """ä¿å­˜æ¨¡å‹"""
        for i, (model, optimizer, scheduler) in enumerate(
            zip(self.models, self.optimizers, self.schedulers)
        ):
            state = {
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
            }
            save_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            torch.save(state, save_path)

    def load_models(self, save_dir: str, prefix: str):
        """åŠ è½½æ¨¡å‹"""
        for i, (model, optimizer, scheduler) in enumerate(
            zip(self.models, self.optimizers, self.schedulers)
        ):
            load_path = Path(save_dir) / f"{prefix}_gpu{self.gpu_id}_model{i}.pth"
            if load_path.exists():
                state = torch.load(
                    load_path, map_location=self.device, weights_only=False
                )
                model.load_state_dict(state["model_state_dict"])
                optimizer.load_state_dict(state["optimizer_state_dict"])
                if scheduler and state.get("scheduler_state_dict"):
                    scheduler.load_state_dict(state["scheduler_state_dict"])

    def broadcast_backbone_and_reinit_heads(self, backbone_state_dict: dict):
        """ç”¨å…±äº« backbone åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼Œå¹¶é‡æ–°åˆå§‹åŒ–å„æ¨¡å‹çš„ classifier head

        Args:
            backbone_state_dict: æºæ¨¡å‹çš„ backbone æƒé‡ (ä¸å« fc å±‚)
        """
        for model in self.models:
            # åŠ è½½ backbone æƒé‡ (strict=False å› ä¸ºä¸å« fc å±‚)
            model.load_state_dict(backbone_state_dict, strict=False)
            # é‡æ–°åˆå§‹åŒ– classifier head
            model.reinit_classifier(init_method=self.cfg.init_method)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ è®­ç»ƒå†å²ä¿å­˜å™¨                                                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class HistorySaver:
    """è®­ç»ƒå†å² CSV ä¿å­˜å™¨ (å¤§çº²åŒ–)"""

    def __init__(self, save_dir: str):
        self.save_dir = Path(save_dir)
        ensure_dir(self.save_dir)

    def save(self, history: Dict[str, List], filename: str = "history"):
        """å°†å†å²å­—å…¸å¯¼å‡ºè‡³ CSV æ–‡ä»¶"""
        import csv
        path = self.save_dir / f"{filename}.csv"
        
        if not history: return

        with open(path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=history.keys())
            writer.writeheader()
            self._write_rows(writer, history)
            
        get_logger().info(f"ğŸ’¾ History saved: {path}")

    def _write_rows(self, writer, history):
        """éå†å¹¶å†™å…¥è¡Œæ•°æ®"""
        num_entries = len(next(iter(history.values())))
        for i in range(num_entries):
            writer.writerow({k: v[i] for k, v in history.items()})
