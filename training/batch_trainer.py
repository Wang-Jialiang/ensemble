"""
================================================================================
Batch Ensemble è®­ç»ƒå™¨
================================================================================

ç‹¬ç«‹çš„ Batch Ensemble è®­ç»ƒå™¨ï¼Œä¸ StagedEnsembleTrainer å®Œå…¨è§£è€¦ã€‚

ç‰¹ç‚¹:
- è®­ç»ƒå•ä¸ª BatchEnsembleResNet æ¨¡å‹ (å†…å«å¤šä¸ªéšå¼æˆå‘˜)
- æ— éœ€å¤š GPU åè°ƒï¼Œå• GPU å³å¯è¿è¡Œ ensemble
- æ”¯æŒä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹  (å¯é€‰)
"""

import time
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

from ..config import Config
from ..models.batch_ensemble import BatchEnsembleResNet
from ..utils import ensure_dir, get_logger
from .base import BaseTrainer
from .scheduler import create_optimizer, create_scheduler


class BatchEnsembleTrainer(BaseTrainer):
    """
    Batch Ensemble è®­ç»ƒå™¨

    è®­ç»ƒå•ä¸ª BatchEnsembleResNet æ¨¡å‹ï¼Œå†…éƒ¨åŒ…å«å¤šä¸ªéšå¼é›†æˆæˆå‘˜ã€‚
    """

    def __init__(
        self,
        method_name: str,
        cfg: Config,
        num_members: int = 4,
        use_curriculum: bool = False,
    ):
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        super().__init__(method_name, cfg)

        self.num_members = num_members
        self.use_curriculum = use_curriculum

        # è®¾å¤‡
        self.device = torch.device(
            f"cuda:{cfg.gpu_ids[0]}" if cfg.gpu_ids else "cuda:0"
        )

        # æ€§èƒ½ä¼˜åŒ–
        if cfg.use_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

        get_logger().info(f"\nğŸš€ Initializing {method_name} (Batch Ensemble)")
        get_logger().info(f"   Members: {num_members} (implicit)")
        get_logger().info(f"   Device: {self.device}")

        # åˆ›å»ºæ¨¡å‹
        self.model = BatchEnsembleResNet(
            layers=[2, 2, 2, 2],  # ResNet-18 é…ç½®
            num_classes=cfg.num_classes,
            num_members=num_members,
        ).to(self.device)

        # å¯é€‰ç¼–è¯‘
        if cfg.compile_model and hasattr(torch, "compile"):
            self.model = torch.compile(self.model)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = create_optimizer(
            self.model, cfg.optimizer, cfg.lr, cfg.weight_decay
        )
        self.scheduler = create_scheduler(
            self.optimizer, cfg.scheduler, cfg.total_epochs
        )

        # è®¾ç½®æ—¥å¿—
        self.setup_logging()

        # ä¿å­˜é…ç½®
        cfg.save()

    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        criterion = nn.CrossEntropyLoss(label_smoothing=self.cfg.label_smoothing)

        total_loss = 0.0
        iterator = tqdm(
            train_loader,
            desc=f"Epoch {epoch + 1}/{self.cfg.total_epochs}",
        )

        for inputs, targets in iterator:
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            self.optimizer.zero_grad()

            # æ¨¡å‹è¾“å‡º: [num_members, batch_size, num_classes]
            logits = self.model(inputs)

            # è®¡ç®—æ¯ä¸ªæˆå‘˜çš„ loss å¹¶å¹³å‡
            # targets éœ€è¦æ‰©å±•: [num_members, batch_size]
            targets_expanded = targets.unsqueeze(0).expand(self.num_members, -1)

            # é‡å¡‘ logits: [num_members * batch_size, num_classes]
            logits_flat = logits.view(-1, logits.size(-1))
            targets_flat = targets_expanded.reshape(-1)

            loss = criterion(logits_flat, targets_flat)

            loss.backward()

            # æ¢¯åº¦è£å‰ª
            if self.cfg.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), self.cfg.max_grad_norm
                )

            self.optimizer.step()

            total_loss += loss.item()
            iterator.set_postfix({"loss": loss.item()})

        # æ›´æ–°è°ƒåº¦å™¨
        if self.scheduler is not None:
            self.scheduler.step()

        return total_loss / len(train_loader)

    @torch.no_grad()
    def _validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯"""
        self.model.eval()
        criterion = nn.CrossEntropyLoss()

        total_loss = 0.0
        correct = 0
        total = 0

        for inputs, targets in val_loader:
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            # æ¨¡å‹è¾“å‡º: [num_members, batch_size, num_classes]
            logits = self.model(inputs)

            # é›†æˆé¢„æµ‹: å¹³å‡æ‰€æœ‰æˆå‘˜çš„ logits
            ensemble_logits = logits.mean(dim=0)  # [batch_size, num_classes]

            loss = criterion(ensemble_logits, targets)
            total_loss += loss.item()

            preds = ensemble_logits.argmax(dim=1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)

        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒ"""
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸ¯ Batch Ensemble Training: {self.name}")
        self.logger.info(f"   Members: {self.num_members}")
        self.logger.info("=" * 70)

        training_start = time.time()

        try:
            for epoch in range(self.cfg.total_epochs):
                epoch_start = time.time()

                # è®­ç»ƒ
                train_loss = self._train_epoch(train_loader, epoch)

                # éªŒè¯
                val_loss, val_acc = self._validate(val_loader)

                epoch_time = time.time() - epoch_start
                current_lr = self.optimizer.param_groups[0]["lr"]

                # ä½¿ç”¨åŸºç±»æ–¹æ³•è®°å½• epoch ä¿¡æ¯
                self._log_epoch(
                    epoch, train_loss, val_loss, val_acc, current_lr, epoch_time
                )

                # ä½¿ç”¨åŸºç±»æ–¹æ³•æ£€æŸ¥æœ€ä½³å¹¶ä¿å­˜
                self._check_best_and_save(val_loss, epoch)

                # æ—©åœ
                if self.early_stopping(val_loss, epoch):
                    self.logger.info(f"\nâš ï¸ Early stopping at epoch {epoch + 1}")
                    break

            # ä½¿ç”¨åŸºç±»æ–¹æ³•å®Œæˆè®­ç»ƒ
            self._finalize_training(training_start)

        except KeyboardInterrupt:
            self._handle_interrupt()
            raise

    def _save_checkpoint(self, tag: str):
        """ä¿å­˜ checkpoint"""
        ckpt_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        ensure_dir(ckpt_dir)

        # ä¿å­˜æ¨¡å‹
        torch.save(self.model.state_dict(), ckpt_dir / "model.pth")

        # ä¿å­˜çŠ¶æ€
        state = {
            "epoch": len(self.history["epoch"]),
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch,
            "history": self.history,
            "num_members": self.num_members,
            "total_training_time": self.total_training_time,
        }
        torch.save(state, ckpt_dir / "trainer_state.pth")
        self.logger.info(f"ğŸ’¾ Saved checkpoint: {tag}")

    def load_checkpoint(self, tag: str = "best") -> bool:
        """åŠ è½½ checkpoint"""
        ckpt_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        if not ckpt_dir.exists():
            self.logger.warning(f"âš ï¸ Checkpoint not found: {ckpt_dir}")
            return False

        self.model.load_state_dict(
            torch.load(ckpt_dir / "model.pth", weights_only=True)
        )

        state_path = ckpt_dir / "trainer_state.pth"
        if state_path.exists():
            state = torch.load(state_path, weights_only=False)
            self.best_val_loss = state["best_val_loss"]
            self.best_epoch = state["best_epoch"]
            self.history = state["history"]
            self.total_training_time = state.get("total_training_time", 0.0)
            self.logger.info(f"âœ… Loaded checkpoint: {tag}")
            return True
        return False

    def get_models(self) -> List[nn.Module]:
        """
        è·å–æ¨¡å‹åˆ—è¡¨ (ç”¨äºè¯„ä¼°å…¼å®¹)

        Batch Ensemble åªæœ‰ä¸€ä¸ªç‰©ç†æ¨¡å‹ï¼Œä½†è¿”å›åˆ—è¡¨ä»¥å…¼å®¹ç°æœ‰è¯„ä¼°ä»£ç 
        """
        return [self.model]


def train_batch_ensemble(
    experiment_name: str,
    cfg: Config,
    train_loader: DataLoader,
    val_loader: DataLoader,
    num_members: int = 4,
) -> Tuple[BatchEnsembleTrainer, float]:
    """
    Batch Ensemble è®­ç»ƒå…¥å£å‡½æ•°

    Args:
        experiment_name: å®éªŒåç§°
        cfg: é…ç½®
        train_loader, val_loader: æ•°æ®åŠ è½½å™¨
        num_members: é›†æˆæˆå‘˜æ•°

    Returns:
        (trainer, training_time)
    """
    trainer = BatchEnsembleTrainer(
        method_name=experiment_name,
        cfg=cfg,
        num_members=num_members,
    )

    trainer.train(train_loader, val_loader)
    training_time = trainer.total_training_time

    # åŠ è½½æœ€ä½³æ¨¡å‹
    trainer.load_checkpoint("best")
    trainer.total_training_time = training_time

    get_logger().info(f"\nâœ… Batch Ensemble training completed: {experiment_name}")

    return trainer, training_time
