"""
================================================================================
çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
================================================================================

Knowledge Distillation Ensemble å®ç° (Hinton et al., 2015)

æ ¸å¿ƒæ€æƒ³: ç”¨é¢„è®­ç»ƒçš„æ•™å¸ˆé›†æˆç”Ÿæˆè½¯æ ‡ç­¾ï¼Œè®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚
- å­¦ç”Ÿæ¨¡å‹å¯ä»¥é€¼è¿‘æ•™å¸ˆé›†æˆçš„æ€§èƒ½
- å•æ¨¡å‹æ¨ç†ï¼Œä½†å…·æœ‰é›†æˆçº§åˆ«çš„çŸ¥è¯†

å‚è€ƒ: https://arxiv.org/abs/1503.02531
"""

import time
from pathlib import Path
from typing import TYPE_CHECKING, List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from ..config import Config
from ..models import ModelFactory
from ..utils import ensure_dir, get_logger
from .base import BaseTrainer
from .scheduler import create_optimizer, create_scheduler

if TYPE_CHECKING:
    from .core import StagedEnsembleTrainer


class DistillationLoss(nn.Module):
    """
    çŸ¥è¯†è’¸é¦æŸå¤±

    Loss = Î± * KL(student_soft || teacher_soft) * T^2 + (1 - Î±) * CE(student_hard, labels)

    å…¶ä¸­:
    - T: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è½¯æ ‡ç­¾çš„å¹³æ»‘ç¨‹åº¦
    - Î±: è½¯æ ‡ç­¾æŸå¤±æƒé‡
    """

    def __init__(self, temperature: float = 4.0, alpha: float = 0.7):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        targets: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º [batch_size, num_classes]
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º [batch_size, num_classes]
            targets: ç¡¬æ ‡ç­¾ [batch_size]

        Returns:
            Combined distillation loss
        """
        # è½¯æ ‡ç­¾æŸå¤± (KL æ•£åº¦)
        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)

        soft_loss = F.kl_div(student_soft, teacher_soft, reduction="batchmean")
        soft_loss = soft_loss * (self.temperature**2)

        # ç¡¬æ ‡ç­¾æŸå¤± (äº¤å‰ç†µ)
        hard_loss = self.ce_loss(student_logits, targets)

        # ç»„åˆæŸå¤±
        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss


class DistillationTrainer(BaseTrainer):
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨

    ä½¿ç”¨å·²è®­ç»ƒçš„æ•™å¸ˆé›†æˆ (å¤šä¸ªæ¨¡å‹æˆ– StagedEnsembleTrainer)
    è®­ç»ƒä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚
    """

    def __init__(
        self,
        method_name: str,
        cfg: Config,
        teacher_models: Union[List[nn.Module], "StagedEnsembleTrainer"],
        temperature: float = 4.0,
        alpha: float = 0.7,
        student_model_name: Optional[str] = None,
    ):
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        super().__init__(method_name, cfg)

        self.temperature = temperature
        self.alpha = alpha

        # è®¾å¤‡
        self.device = torch.device(
            f"cuda:{cfg.gpu_ids[0]}" if cfg.gpu_ids else "cuda:0"
        )

        # æ€§èƒ½ä¼˜åŒ–
        if cfg.use_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

        get_logger().info(f"\nğŸš€ Initializing {method_name} (Knowledge Distillation)")
        get_logger().info(f"   Temperature: {temperature}, Alpha: {alpha}")
        get_logger().info(f"   Device: {self.device}")

        # è®¾ç½®æ•™å¸ˆæ¨¡å‹
        self.teacher_models = self._setup_teachers(teacher_models)
        get_logger().info(f"   Teachers: {len(self.teacher_models)} models")

        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        student_name = student_model_name or cfg.model_name
        self.student = ModelFactory.create_model(
            student_name,
            num_classes=cfg.num_classes,
            init_method=cfg.init_method,
        ).to(self.device)

        # å¯é€‰ç¼–è¯‘
        if cfg.compile_model and hasattr(torch, "compile"):
            self.student = torch.compile(self.student)

        # è’¸é¦æŸå¤±
        self.criterion = DistillationLoss(temperature, alpha)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ (ä½¿ç”¨å…¬å…±å·¥å‚å‡½æ•°)
        self.optimizer = create_optimizer(
            self.student, cfg.optimizer, cfg.lr, cfg.weight_decay
        )
        self.scheduler = create_scheduler(self.optimizer, "cosine", cfg.total_epochs)

        # è®¾ç½®æ—¥å¿—
        self.setup_logging()

        # ä¿å­˜é…ç½®
        cfg.save()

    def _setup_teachers(self, teachers) -> List[nn.Module]:
        """è®¾ç½®æ•™å¸ˆæ¨¡å‹"""
        # å¦‚æœæ˜¯ StagedEnsembleTrainerï¼Œæå–å…¶ workers ä¸­çš„æ¨¡å‹
        if hasattr(teachers, "workers"):
            models = []
            for worker in teachers.workers:
                for model in worker.models:
                    model.eval()
                    models.append(model)
            return models

        # å¦‚æœå·²ç»æ˜¯æ¨¡å‹åˆ—è¡¨
        elif isinstance(teachers, list):
            for m in teachers:
                m.eval()
                m.to(self.device)
            return teachers

        else:
            raise ValueError(f"Unknown teacher type: {type(teachers)}")

    # æ³¨: _create_optimizer å’Œ _create_scheduler å·²ç§»é™¤ï¼Œä½¿ç”¨ scheduler æ¨¡å—çš„å…¬å…±å‡½æ•°æ›¿ä»£

    @torch.no_grad()
    def _get_teacher_logits(self, inputs: torch.Tensor) -> torch.Tensor:
        """è·å–æ•™å¸ˆé›†æˆçš„å¹³å‡ logits"""
        all_logits = []

        for teacher in self.teacher_models:
            # å°†è¾“å…¥ç§»åˆ°æ•™å¸ˆæ¨¡å‹æ‰€åœ¨è®¾å¤‡
            teacher_device = next(teacher.parameters()).device
            inputs_t = inputs.to(teacher_device)
            logits = teacher(inputs_t).to(self.device)
            all_logits.append(logits)

        # è¿”å›å¹³å‡ logits
        return torch.stack(all_logits).mean(dim=0)

    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.student.train()

        total_loss = 0.0
        iterator = tqdm(
            train_loader,
            desc=f"Epoch {epoch + 1}/{self.cfg.total_epochs}",
        )

        for inputs, targets in iterator:
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            # è·å–æ•™å¸ˆè½¯æ ‡ç­¾
            teacher_logits = self._get_teacher_logits(inputs)

            # å­¦ç”Ÿå‰å‘
            self.optimizer.zero_grad()
            student_logits = self.student(inputs)

            # è’¸é¦æŸå¤±
            loss = self.criterion(student_logits, teacher_logits, targets)

            loss.backward()

            if self.cfg.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.student.parameters(), self.cfg.max_grad_norm
                )

            self.optimizer.step()
            total_loss += loss.item()
            iterator.set_postfix({"loss": loss.item()})

        return total_loss / len(train_loader)

    @torch.no_grad()
    def _validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯å­¦ç”Ÿæ¨¡å‹"""
        self.student.eval()
        criterion = nn.CrossEntropyLoss()

        total_loss = 0.0
        correct = 0
        total = 0

        for inputs, targets in val_loader:
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            logits = self.student(inputs)
            loss = criterion(logits, targets)
            total_loss += loss.item()

            preds = logits.argmax(dim=1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)

        return total_loss / len(val_loader), 100.0 * correct / total

    @torch.no_grad()
    def _validate_teacher(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯æ•™å¸ˆé›†æˆ (ç”¨äºå¯¹æ¯”)"""
        criterion = nn.CrossEntropyLoss()
        total_loss = 0.0
        correct = 0
        total = 0

        for inputs, targets in val_loader:
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            ensemble_logits = self._get_teacher_logits(inputs)
            loss = criterion(ensemble_logits, targets)
            total_loss += loss.item()

            preds = ensemble_logits.argmax(dim=1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)

        return total_loss / len(val_loader), 100.0 * correct / total

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒ"""
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸ¯ Knowledge Distillation: {self.name}")
        self.logger.info(
            f"   Teachers: {len(self.teacher_models)}, T={self.temperature}, Î±={self.alpha}"
        )
        self.logger.info("=" * 70)

        # å…ˆè¯„ä¼°æ•™å¸ˆæ€§èƒ½
        teacher_loss, teacher_acc = self._validate_teacher(val_loader)
        self.logger.info(
            f"ğŸ“š Teacher ensemble: Loss={teacher_loss:.4f}, Acc={teacher_acc:.2f}%"
        )

        training_start = time.time()

        try:
            for epoch in range(self.cfg.total_epochs):
                epoch_start = time.time()

                # è®­ç»ƒ
                train_loss = self._train_epoch(train_loader, epoch)

                # æ›´æ–°è°ƒåº¦å™¨
                if self.scheduler:
                    self.scheduler.step()

                # éªŒè¯
                val_loss, val_acc = self._validate(val_loader)

                epoch_time = time.time() - epoch_start
                current_lr = self.optimizer.param_groups[0]["lr"]

                # ä½¿ç”¨åŸºç±»æ–¹æ³•è®°å½• epoch
                self._log_epoch(
                    epoch, train_loss, val_loss, val_acc, current_lr, epoch_time
                )

                # ä½¿ç”¨åŸºç±»æ–¹æ³•æ£€æŸ¥æœ€ä½³å¹¶ä¿å­˜
                self._check_best_and_save(val_loss, epoch)

                # æ—©åœ
                if self.early_stopping(val_loss, epoch):
                    self.logger.info(f"\nâš ï¸ Early stopping at epoch {epoch + 1}")
                    break

            # æœ€ç»ˆå¯¹æ¯”
            final_loss, final_acc = self._validate(val_loader)
            self.logger.info("\nğŸ“Š Final Comparison:")
            self.logger.info(
                f"   Teacher: Loss={teacher_loss:.4f}, Acc={teacher_acc:.2f}%"
            )
            self.logger.info(f"   Student: Loss={final_loss:.4f}, Acc={final_acc:.2f}%")

            # ä½¿ç”¨åŸºç±»æ–¹æ³•å®Œæˆè®­ç»ƒ
            self._finalize_training(training_start)

        except KeyboardInterrupt:
            # ä½¿ç”¨åŸºç±»æ–¹æ³•å¤„ç†ä¸­æ–­
            self._handle_interrupt()
            raise

    def _save_checkpoint(self, tag: str):
        """ä¿å­˜ checkpoint"""
        ckpt_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        ensure_dir(ckpt_dir)

        torch.save(self.student.state_dict(), ckpt_dir / "student.pth")

        state = {
            "epoch": len(self.history["epoch"]),
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch,
            "history": self.history,
            "temperature": self.temperature,
            "alpha": self.alpha,
            "total_training_time": self.total_training_time,
        }
        torch.save(state, ckpt_dir / "trainer_state.pth")
        self.logger.info(f"ğŸ’¾ Saved checkpoint: {tag}")

    def load_checkpoint(self, tag: str = "best") -> bool:
        """åŠ è½½ checkpoint"""
        ckpt_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        if not ckpt_dir.exists():
            self.logger.warning(f"âš ï¸ Checkpoint not found: {ckpt_dir}")
            return False

        self.student.load_state_dict(
            torch.load(ckpt_dir / "student.pth", weights_only=True)
        )

        state_path = ckpt_dir / "trainer_state.pth"
        if state_path.exists():
            state = torch.load(state_path, weights_only=False)
            self.best_val_loss = state["best_val_loss"]
            self.best_epoch = state["best_epoch"]
            self.history = state["history"]
            self.total_training_time = state.get("total_training_time", 0.0)
            self.logger.info(f"âœ… Loaded checkpoint: {tag}")
            return True
        return False

    def get_models(self) -> List[nn.Module]:
        """è·å–æ¨¡å‹åˆ—è¡¨ (è¿”å›å­¦ç”Ÿæ¨¡å‹)"""
        return [self.student]


def train_distillation(
    experiment_name: str,
    cfg: Config,
    train_loader: DataLoader,
    val_loader: DataLoader,
    teacher_models: Union[List[nn.Module], "StagedEnsembleTrainer"],
    temperature: float = 4.0,
    alpha: float = 0.7,
) -> Tuple[DistillationTrainer, float]:
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒå…¥å£å‡½æ•°

    Args:
        experiment_name: å®éªŒåç§°
        cfg: é…ç½®
        train_loader, val_loader: æ•°æ®åŠ è½½å™¨
        teacher_models: æ•™å¸ˆæ¨¡å‹åˆ—è¡¨æˆ– StagedEnsembleTrainer
        temperature: è’¸é¦æ¸©åº¦
        alpha: è½¯æ ‡ç­¾æƒé‡

    Returns:
        (trainer, training_time)
    """
    trainer = DistillationTrainer(
        method_name=experiment_name,
        cfg=cfg,
        teacher_models=teacher_models,
        temperature=temperature,
        alpha=alpha,
    )

    trainer.train(train_loader, val_loader)
    training_time = trainer.total_training_time

    # åŠ è½½æœ€ä½³æ¨¡å‹
    trainer.load_checkpoint("best")
    trainer.total_training_time = training_time

    get_logger().info(f"\nâœ… Distillation completed: {experiment_name}")

    return trainer, training_time
