"""
================================================================================
è®­ç»ƒæ ¸å¿ƒæ¨¡å—
================================================================================

StagedEnsembleTrainer (ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨)ã€train_experiment (å®éªŒå…¥å£å‡½æ•°)
"""

import logging
import shutil
import sys
import time
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from ..config import Config
from ..utils import console, ensure_dir, format_duration, get_logger
from .optimization import EarlyStopping
from .worker import GPUWorker, HistorySaver

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Checkpoint Mixin                                                           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CheckpointMixin:
    """æ£€æŸ¥ç‚¹ç®¡ç† Mixin (å¤§çº²åŒ–)"""

    def _save_checkpoint(self, tag: str):
        """ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€ä¸æ¨¡å‹æƒé‡"""
        path = self._get_checkpoint_dir(tag)
        ensure_dir(path)

        # 1. ä¿å­˜å„ Worker çš„æ¨¡å‹
        for worker in self.workers:
            worker.save_models(str(path), self.name)

        # 2. ä¿å­˜ Trainer çŠ¶æ€å¿«ç…§
        self._write_state_file(path)
        self.logger.info(f"ğŸ’¾ Checkpoint Saved: {tag}")

    def load_checkpoint(self, tag: str = "best") -> bool:
        """ä»æŒ‡å®š tag åŠ è½½æ£€æŸ¥ç‚¹"""
        path = self._get_checkpoint_dir(tag)
        if not path.exists():
            self.logger.warning(f"âš ï¸ Checkpoint ä¸å­˜åœ¨: {path}")
            return False

        # 1. åŠ è½½æ¨¡å‹
        for worker in self.workers:
            worker.load_models(str(path), self.name)

        # 2. æ¢å¤çŠ¶æ€å˜é‡
        self._read_state_file(path)
        return True

    def _get_checkpoint_dir(self, tag: str) -> Path:
        """ç»Ÿä¸€è·¯å¾„ç”Ÿæˆé€»è¾‘ (save_dir å·²åŒ…å«å®éªŒå)"""
        return Path(self.cfg.save_dir) / "checkpoints" / tag

    def _write_state_file(self, path: Path):
        """å†™å…¥ trainer çŠ¶æ€äºŒè¿›åˆ¶æ–‡ä»¶"""
        # è®¡ç®—å½“å‰ç´¯è®¡è®­ç»ƒæ—¶é—´ (ä» train å¼€å§‹åˆ°ç°åœ¨)
        current_training_time = (
            time.time() - self._train_start_time
            if hasattr(self, "_train_start_time")
            else self.total_training_time
        )
        state = {
            "epoch": len(self.history["epoch"]),
            "best_val_loss": self._best_val_loss,
            "best_val_acc": self._best_val_acc,
            "best_epoch": self._best_epoch,
            "history": self.history,
            "early_stopping_counter": self.early_stopping.counter,
            "total_time": current_training_time,
            "aug_method": self.augmentation_method,
            "params": (self.use_curriculum, self.fixed_ratio, self.fixed_prob),
        }
        torch.save(state, path / "trainer_state.pth")

    def _read_state_file(self, path: Path):
        """è§£æå¹¶æ¢å¤çŠ¶æ€"""
        file = path / "trainer_state.pth"
        if not file.exists():
            return

        s = torch.load(file, weights_only=False)
        self._best_val_loss, self._best_val_acc = (
            s["best_val_loss"],
            s.get("best_val_acc", 0.0),
        )
        self._best_epoch = s["best_epoch"]
        self.history = s["history"]
        self.early_stopping.counter = s.get("early_stopping_counter", 0)
        self.total_training_time = s.get("total_time", 0.0)
        self.logger.info(f"âœ… State Restored (Best Loss: {self._best_val_loss:.4f})")

    def _cleanup_old_checkpoints(self):
        """æ¸…ç†å†—ä½™çš„å‘¨æœŸæ€§æ£€æŸ¥ç‚¹"""
        base = Path(self.cfg.save_dir) / "checkpoints"
        if not base.exists():
            return

        dirs = sorted(
            [d for d in base.iterdir() if d.name.startswith("epoch_")],
            key=lambda x: int(x.name.split("_")[1]),
        )

        if len(dirs) > self.cfg.keep_last_n_checkpoints:
            for d in dirs[: -self.cfg.keep_last_n_checkpoints]:
                shutil.rmtree(d)
                self.logger.info(f"ğŸ—‘ï¸ Cleaned: {d.name}")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨                                                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class StagedEnsembleTrainer(CheckpointMixin):
    """
    ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    å®ç°ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥è®­ç»ƒæ·±åº¦é›†æˆæ¨¡å‹:

    é˜¶æ®µåˆ’åˆ†:
        1. Warmupé˜¶æ®µ: æ— é®æŒ¡çƒ­èº«è®­ç»ƒï¼Œè®©æ¨¡å‹å­¦ä¹ åŸºç¡€ç‰¹å¾
        2. Progressiveé˜¶æ®µ: æ¸è¿›å¼å¢åŠ é®æŒ¡ï¼ŒåŸ¹å…»æ¨¡å‹å…³æ³¨ä¸åŒåŒºåŸŸ
        3. Finetuneé˜¶æ®µ: å›ºå®šé®æŒ¡æ¯”ä¾‹å¾®è°ƒï¼Œç¨³å®šæ¨¡å‹æ€§èƒ½
    """

    def __init__(
        self,
        method_name,
        cfg,
        augmentation_method="perlin",
        use_curriculum=True,
        fixed_ratio=0.25,
        fixed_prob=0.5,
        share_warmup_backbone=False,
    ):
        """ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨æ„é€ å‡½æ•° (å¤§çº²åŒ–)"""
        self.name = method_name
        self.cfg = cfg
        self.total_training_time = 0.0

        # 1. åˆå§‹åŒ–å±æ€§ä¸å¢å¼ºç­–ç•¥
        self.augmentation_method = augmentation_method
        self.use_curriculum = use_curriculum
        self.fixed_ratio, self.fixed_prob = fixed_ratio, fixed_prob
        self.share_warmup_backbone = share_warmup_backbone

        # 2. ç¡¬ä»¶ä¸æ—¥å¿—åˆå§‹åŒ–
        self._init_hardware_optimizations()
        self.setup_logging()
        self._init_monitoring_tools()

        # 3. åˆå§‹åŒ–å·¥ä½œèŠ‚ç‚¹ (Parallel Workers)
        self.workers: List[GPUWorker] = [
            GPUWorker(gid, cfg.num_models_per_gpu, cfg, augmentation_method)
            for gid in cfg.gpu_ids
        ]

        # 4. åˆå§‹åŒ–çŠ¶æ€è·Ÿè¸ªå˜é‡
        self._init_tracking_structures()
        self._log_training_config()  # ä½¿ç”¨ Rich å±•ç¤ºé…ç½®æ±‡æ€»
        cfg.save()  # æŒä¹…åŒ–å½“å‰è¿è¡Œé…ç½®

    def _init_hardware_optimizations(self):
        """é…ç½® Cuda åç«¯åŠ é€Ÿé€‰é¡¹"""
        if self.cfg.use_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

    def _init_monitoring_tools(self):
        """åˆå§‹åŒ– wandb è§‚æµ‹å·¥å…·"""

        # Weights & Biases
        self.wandb_run = None
        if getattr(self.cfg, "use_wandb", False):
            try:
                import wandb

                self.wandb_run = wandb.init(
                    project=getattr(self.cfg, "wandb_project", "ensemble"),
                    name=self.name,
                    config=self._get_wandb_config(),
                    reinit="finish_previous",
                )
            except ImportError:
                self.logger.warning("âš ï¸ wandb not installed, skipping")
            except Exception as e:
                self.logger.warning(f"âš ï¸ wandb init failed: {e}")

    def _get_wandb_config(self) -> dict:
        """æå–é…ç½®ç”¨äº wandb"""
        return {
            "model": self.cfg.model_name,
            "dataset": self.cfg.dataset_name,
            "batch_size": self.cfg.batch_size,
            "lr": self.cfg.lr,
            "optimizer": self.cfg.optimizer,
            "augmentation": self.augmentation_method,
            "use_curriculum": self.use_curriculum,
            "total_epochs": self.cfg.total_epochs,
        }

    def _init_tracking_structures(self):
        """åˆå§‹åŒ–è®­ç»ƒå†å²ã€æ—©åœä¸è®°å½•å™¨"""
        self.history = {
            k: []
            for k in [
                "epoch",
                "stage",
                "train_loss",
                "val_loss",
                "val_acc",
                "mask_ratio",
                "mask_prob",
                "lr",
                "time",
            ]
        }
        self.early_stopping = EarlyStopping(
            patience=self.cfg.early_stopping_patience,
            metrics={"val_loss": "min", "val_acc": "max"},
            criteria="any",
        )
        self._best_val_loss, self._best_val_acc, self._best_epoch = float("inf"), 0.0, 0
        self.history_saver = HistorySaver(self.cfg.save_dir)

    def get_models(self) -> List[nn.Module]:
        """è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ (ä¸å…¶ä»– Trainer æ¥å£ä¸€è‡´)"""
        return [model for worker in self.workers for model in worker.models]

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.cfg.save_dir) / "logs"
        ensure_dir(log_dir)

        logger = logging.getLogger(self.name)
        logger.handlers.clear()
        logger.setLevel(getattr(logging, self.cfg.log_level))

        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

        # æ–‡ä»¶è¾“å‡º (æ€»æ˜¯å¼€å¯)
        file_handler = logging.FileHandler(log_dir / f"{self.name}_train.log", mode="w")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        # æ§åˆ¶å°è¾“å‡º (å¯é€šè¿‡é…ç½®å…³é—­)
        if getattr(self.cfg, "log_to_console", True):
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)

        self.logger = logger

    def _get_stage_info(self, epoch: int) -> Tuple[int, str, float, float, bool]:
        """è·å–å½“å‰é˜¶æ®µä¿¡æ¯

        Returns:
            Tuple: (stage_num, stage_name, mask_ratio, mask_prob, use_mask)
        """
        cfg = self.cfg

        # æ¨¡å¼1: æ— å¢å¼º (Baseline)
        if self.augmentation_method == "none":
            return 1, "NoAug", 0.0, 0.0, False

        # æ¨¡å¼2: å›ºå®šå‚æ•°æ¨¡å¼
        if not self.use_curriculum:
            return 1, "Fixed", self.fixed_ratio, self.fixed_prob, True

        # æ¨¡å¼3: è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ (ä¸‰é˜¶æ®µ)
        if epoch < cfg.warmup_epochs:
            return 1, "Warmup", 0.0, 0.0, False
        elif epoch < cfg.warmup_epochs + cfg.progressive_epochs:
            progress = (epoch - cfg.warmup_epochs) / cfg.progressive_epochs
            mask_ratio = (
                cfg.mask_start_ratio
                + (cfg.mask_end_ratio - cfg.mask_start_ratio) * progress
            )
            mask_prob = (
                cfg.mask_prob_start
                + (cfg.mask_prob_end - cfg.mask_prob_start) * progress
            )
            return 2, "Progressive", mask_ratio, mask_prob, True
        else:
            return 3, "Finetune", cfg.finetune_mask_ratio, cfg.finetune_mask_prob, True

    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """è®­ç»ƒå•ä¸ª Epoch (å¤§çº²åŒ–)"""
        # 1. å‡†å¤‡å½“å‰é˜¶æ®µå‚æ•°
        *_, m_ratio, m_prob, use_mask = self._get_stage_info(epoch)
        criterion = nn.CrossEntropyLoss(label_smoothing=self.cfg.label_smoothing)

        # 2. é¢„çƒ­ Workers (å¦‚é¢„è®¡ç®— Mask æ± )
        for w in self.workers:
            w.precompute_masks(m_ratio)

        # 3. æ‰§è¡Œæ‰¹æ¬¡è¿­ä»£
        return self._run_batch_iteration(
            train_loader, epoch, criterion, m_ratio, m_prob, use_mask
        )

    def _run_batch_iteration(self, loader, epoch, criterion, m_ratio, m_prob, use_mask):
        """å…·ä½“æ‰§è¡Œå¼ é‡æµåŠ¨ä¸æ¢¯åº¦æ›´æ–° (ä½¿ç”¨ Rich Progress)"""
        total_loss, n = 0.0, 0
        from rich.progress import (
            BarColumn,
            Progress,
            TaskProgressColumn,
            TextColumn,
            TimeRemainingColumn,
        )

        # åˆ¤æ–­æ˜¯å¦ä¸º Warmup å•æ¨¡å‹è®­ç»ƒæ¨¡å¼
        stage_num = self._get_stage_info(epoch)[0]
        is_warmup_single_model = self.share_warmup_backbone and stage_num == 1

        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TextColumn("| Loss: [bold magenta]{task.fields[loss]}"),
            TimeRemainingColumn(),
            console=console,
        ) as progress:
            desc = f"Epoch {epoch + 1:3d} [LR={self.workers[0].get_lr():.6f}]"
            if is_warmup_single_model:
                desc += " [Warmup-SingleModel]"
            task_id = progress.add_task(desc, total=len(loader), loss="0.0000")

            for inputs, targets in loader:
                if is_warmup_single_model:
                    # Warmup ä¼˜åŒ–ï¼šä»…è®­ç»ƒç¬¬ä¸€ä¸ª Worker çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
                    self.workers[0].train_batch_async(
                        inputs,
                        targets,
                        criterion,
                        m_ratio,
                        m_prob,
                        use_mask,
                        model_indices=[0],
                    )
                    batch_loss = self.workers[0].synchronize()
                else:
                    # æ­£å¸¸æ¨¡å¼ï¼šæ‰€æœ‰ Worker æ‰€æœ‰æ¨¡å‹
                    for w in self.workers:
                        w.train_batch_async(
                            inputs, targets, criterion, m_ratio, m_prob, use_mask
                        )
                    batch_loss = sum(w.synchronize() for w in self.workers) / len(
                        self.workers
                    )

                total_loss += batch_loss
                n += 1
                progress.update(task_id, advance=1, loss=f"{total_loss / n:.4f}")

        # æ­¥è¿›è°ƒåº¦å™¨ (æ‰€æœ‰æ¨¡å‹ï¼Œä¿æŒ LR åŒæ­¥)
        for w in self.workers:
            w.step_schedulers()
        return total_loss / n

    @torch.no_grad()
    def _validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """é›†æˆéªŒè¯è¿‡ç¨‹"""
        criterion = nn.CrossEntropyLoss()
        total_loss, correct, total = 0.0, 0, 0
        device = self.workers[0].device  # ä¸»è®¡ç®—è®¾å¤‡

        for inputs, targets in val_loader:
            # 1. èšåˆæ‰€æœ‰ Worker çš„é¢„æµ‹ Logits
            ensemble_logits = self._collect_ensemble_logits(inputs, device)
            targets = targets.to(device)

            # 2. è®¡ç®—æŒ‡æ ‡
            total_loss += criterion(ensemble_logits, targets).item()
            correct += (ensemble_logits.argmax(1) == targets).sum().item()
            total += targets.size(0)

        return total_loss / len(val_loader), 100.0 * correct / total

    def _collect_ensemble_logits(self, inputs, device):
        """ä»åˆ†å¸ƒå¼ Workers ä¸­æ”¶é›†å¹¶èšåˆé¢„æµ‹ç»“æœ"""
        from ..evaluation.strategies import get_ensemble_fn  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–

        # æ¯ä¸ª worker è¿”å› [num_models, batch, classes]ï¼Œconcat æˆ [total_models, batch, classes]
        logits_list = [w.predict_batch(inputs).to(device) for w in self.workers]
        stacked = torch.cat(logits_list, dim=0)  # [total_models, batch, classes]
        ensemble_fn = get_ensemble_fn(self.cfg)
        return ensemble_fn(stacked)  # [batch, classes]

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """æ‰§è¡Œå…¨ç”Ÿå‘½å‘¨æœŸè®­ç»ƒ (å¤§çº²åŒ–)"""
        self._log_training_start()
        self._train_start_time = time.time()  # ä¿å­˜ä¸ºå®ä¾‹å˜é‡ä¾› checkpoint ä½¿ç”¨
        current_stage = 0

        try:
            for epoch in range(self.cfg.total_epochs):
                # 1. å‘¨æœŸå‡†å¤‡ (é˜¶æ®µåˆ‡æ¢)
                current_stage = self._handle_epoch_prep(epoch, current_stage)

                # 2. æ‰§è¡Œè®­ç»ƒä¸éªŒè¯å¾ªç¯
                stats = self._run_epoch_cycle(train_loader, val_loader, epoch)
                stats["stage"] = current_stage

                # 3. ç”Ÿå‘½å‘¨æœŸé’©å­: è®°å½•ã€æŒä¹…åŒ–ã€æ—©åœ
                self._handle_epoch_post(epoch, stats)
                if self.early_stopping(stats, epoch):
                    break

            self._finalize_training()

        except Exception as e:
            self._handle_training_error(e)
            raise
        finally:
            if self.wandb_run:
                import wandb

                wandb.finish()

    def _handle_epoch_prep(self, epoch, current_stage):
        """å¤„ç† Epoch å¼€å§‹å‰çš„é¢„å¤‡åŠ¨ä½œ (å¦‚é˜¶æ®µåˆ‡æ¢)"""
        s_num, s_name, *_ = self._get_stage_info(epoch)
        if s_num != current_stage:
            # æ‰§è¡Œé˜¶æ®µåˆ‡æ¢é€»è¾‘ (å¦‚ Backbone å¹¿æ’­)
            if s_num == 2 and current_stage == 1 and self.share_warmup_backbone:
                self._broadcast_warmup_backbone()
            self._log_stage_header(s_num)
        return s_num

    def _run_epoch_cycle(self, train_loader, val_loader, epoch):
        """æ‰§è¡Œå•ä¸ª Epoch çš„è®¡ç®—å¾ªç¯å¹¶æ”¶é›†æŒ‡æ ‡"""
        t0 = time.time()
        t_loss = self._train_epoch(train_loader, epoch)
        v_loss, v_acc = self._validate(val_loader)

        # è·å–å½“å‰å…ƒæ•°æ®
        _, _, m_ratio, m_prob, _ = self._get_stage_info(epoch)
        return {
            "train_loss": t_loss,
            "val_loss": v_loss,
            "val_acc": v_acc,
            "mask_ratio": m_ratio,
            "mask_prob": m_prob,
            "lr": self.workers[0].get_lr(),
            "time": time.time() - t0,
        }

    def _handle_epoch_post(self, epoch, stats):
        """å¤„ç† Epoch ç»“æŸåçš„è¾…åŠ©åŠ¨ä½œ (æ—¥å¿—ã€å¿«ç…§ã€æ¸…ç†)"""
        # 1. è®°å½•å†å²ä¸ TensorBoard
        self._record_metrics(epoch, stats)

        # 2. å¤„ç†æœ€ä½³æ¨¡å‹ä¿å­˜
        if stats["val_loss"] < self._best_val_loss:
            self._best_val_loss, self._best_epoch = stats["val_loss"], epoch
            self._save_checkpoint("best")
            self.logger.info(f"   ğŸ† New Best Loss: {stats['val_loss']:.4f}")

        if stats["val_acc"] > self._best_val_acc:
            self._best_val_acc = stats["val_acc"]
            self._save_checkpoint("best_acc")
            self.logger.info(f"   â­ New Best Acc: {stats['val_acc']:.2f}%")

        # 3. å®šæœŸæ£€æŸ¥ç‚¹
        if (epoch + 1) % self.cfg.save_every_n_epochs == 0:
            self._save_checkpoint(f"epoch_{epoch + 1}")
            self._cleanup_old_checkpoints()

        # 4. æ‰“å°æ±‡æ€»æ—¥å¿—
        self._log_epoch_summary(epoch, stats)

    def _log_training_start(self):
        self.logger.info(
            "=" * 70 + f"\nğŸ“ Staged Training Start: {self.name}\n" + "=" * 70
        )

    def _log_stage_header(self, num):
        titles = {
            1: "STAGE 1: WARMUP",
            2: "STAGE 2: PROGRESSIVE",
            3: "STAGE 3: FINETUNE",
        }
        self.logger.info(f"\n{'=' * 20} {titles.get(num, 'UNKNOWN')} {'=' * 20}")

    def _record_metrics(self, epoch, stats):
        """åŒæ­¥å†å²è®°å½•ä¸å¯è§†åŒ–å·¥å…·"""
        for k, v in stats.items():
            self.history[k].append(v)
        self.history["epoch"].append(epoch + 1)

        # wandb
        if self.wandb_run:
            import wandb

            wandb.log(
                {
                    "epoch": epoch + 1,
                    "train_loss": stats["train_loss"],
                    "val_loss": stats["val_loss"],
                    "val_acc": stats["val_acc"],
                    "lr": stats["lr"],
                    "mask_ratio": stats["mask_ratio"],
                    "mask_prob": stats["mask_prob"],
                }
            )

    def _log_epoch_summary(self, epoch, stats):
        _, s_name, _, _, _ = self._get_stage_info(epoch)
        self.logger.info(
            f"Epoch {epoch + 1:3d} [{s_name:11s}] | "
            f"T-Loss: {stats['train_loss']:.4f} | V-Loss: {stats['val_loss']:.4f} | "
            f"V-Acc: {stats['val_acc']:.2f}% | LR: {stats['lr']:.6f} | {stats['time']:.1f}s"
        )

    def _finalize_training(self):
        self.total_training_time = time.time() - self._train_start_time
        self.logger.info(f"\nâ±ï¸ Total Time: {format_duration(self.total_training_time)}")
        self._save_checkpoint("final")
        self.history_saver.save(self.history)

    def _handle_training_error(self, error):
        self.logger.error(f"\nâŒ Training Failed: {error}")
        self.total_training_time = time.time() - self._train_start_time
        self._save_checkpoint("error")
        self.history_saver.save(self.history)

    def _broadcast_warmup_backbone(self):
        """ä»ç¬¬ä¸€ä¸ªæ¨¡å‹è·å– backboneï¼Œå¹¿æ’­åˆ°æ‰€æœ‰å­æ¨¡å‹å¹¶é‡æ–°åˆå§‹åŒ–å„è‡ªçš„ classifier head"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ª worker çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºæº
        source_model = self.workers[0].models[0]
        backbone_state = source_model.get_backbone_state_dict()

        for worker in self.workers:
            worker.broadcast_backbone_and_reinit_heads(backbone_state)

        self.logger.info(
            "ğŸ”„ Shared warmup backbone to all models, re-initialized classifier heads"
        )

    def _log_training_config(self):
        """å±•ç¤ºç²¾ç¾çš„è®­ç»ƒé…ç½®è¡¨æ ¼"""
        if console is None:
            return

        from rich.panel import Panel
        from rich.table import Table

        table = Table(box=None, padding=(0, 2))
        table.add_column("Property", style="bold cyan")
        table.add_column("Value", style="magenta")

        # åŸºç¡€ä¿¡æ¯
        table.add_row("Experiment", self.name)
        table.add_row("Model", self.cfg.model_name)
        table.add_row("Dataset", self.cfg.dataset_name)
        table.add_row("Ensemble Size", str(len(self.get_models())))
        table.add_row("Augmentation", self.augmentation_method)
        table.add_section()

        # æ ¸å¿ƒå‚æ•°
        table.add_row("Batch Size", str(self.cfg.batch_size))
        table.add_row("Learning Rate", f"{self.cfg.lr:.6f}")
        table.add_row("Optimizer", self.cfg.optimizer)
        table.add_row("Scheduler", self.cfg.scheduler)
        table.add_section()

        # ç¡¬ä»¶ä¿¡æ¯
        table.add_row("GPU IDs", str(self.cfg.gpu_ids))
        table.add_row("Mixed Precision", "ON" if self.cfg.use_amp else "OFF")

        console.print(
            Panel(
                table,
                title="[bold green]Training Configuration Summary[/bold green]",
                expand=False,
                border_style="green",
            )
        )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å®éªŒè¿è¡Œå‡½æ•°                                                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def train_experiment(
    cfg: Config,
    train_loader: DataLoader,
    val_loader: DataLoader,
) -> Tuple["StagedEnsembleTrainer", float]:
    """
    ä»…è®­ç»ƒå®éªŒ (ä¸åŒ…å«è¯„ä¼°)

    æ‰€æœ‰å¢å¼ºå‚æ•°ä» cfg è¯»å–:
    - cfg.experiment_name: å®éªŒåç§°
    - cfg.augmentation_method: å¢å¼ºæ–¹æ³•
    - cfg.use_curriculum: æ˜¯å¦ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ 
    - cfg.fixed_ratio, cfg.fixed_prob: å›ºå®šé®æŒ¡å‚æ•°
    - cfg.share_warmup_backbone: æ˜¯å¦å…±äº« backbone

    å‚æ•°:
        cfg: é…ç½®å¯¹è±¡ (åŒ…å«æ‰€æœ‰å®éªŒå‚æ•°)
        train_loader, val_loader: æ•°æ®åŠ è½½å™¨

    è¿”å›:
        (trainer, training_time)
    """
    trainer = StagedEnsembleTrainer(
        cfg.experiment_name,
        cfg,
        augmentation_method=cfg.augmentation_method,
        use_curriculum=cfg.use_curriculum,
        fixed_ratio=cfg.fixed_ratio,
        fixed_prob=cfg.fixed_prob,
        share_warmup_backbone=cfg.share_warmup_backbone,
    )

    # è®­ç»ƒ
    trainer.train(train_loader, val_loader)
    training_time = trainer.total_training_time

    # åŠ è½½æœ€ä½³æ¨¡å‹
    trainer.load_checkpoint("best")
    trainer.total_training_time = training_time

    get_logger().info(f"\nâœ… Training completed: {cfg.experiment_name}")
    get_logger().info(f"   Checkpoint saved to: {Path(cfg.save_dir) / 'checkpoints'}")

    return trainer, training_time
