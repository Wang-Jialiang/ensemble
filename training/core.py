"""
================================================================================
è®­ç»ƒæ ¸å¿ƒæ¨¡å—
================================================================================

StagedEnsembleTrainer (ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨)ã€train_experiment (å®éªŒå…¥å£å‡½æ•°)
"""

import logging
import shutil
import sys
import time
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from ..config import Config
from ..utils import console, ensure_dir, format_duration, get_logger
from .optimization import EarlyStopping
from .worker import GPUWorker, HistorySaver

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Checkpoint Mixin                                                           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CheckpointMixin:
    """æ£€æŸ¥ç‚¹ç®¡ç† Mixin (å¤§çº²åŒ–)"""

    def _save_checkpoint(self, tag: str):
        """ä¿å­˜æ¨¡å‹æƒé‡"""
        path = self._get_checkpoint_dir(tag)
        ensure_dir(path)

        for worker in self.workers:
            worker.save_models(str(path), self.name)
        self.logger.info(f"ğŸ’¾ Checkpoint Saved: {tag}")

    def load_checkpoint(self, tag: str = "best") -> bool:
        """ä»æŒ‡å®š tag åŠ è½½æ¨¡å‹æƒé‡"""
        path = self._get_checkpoint_dir(tag)
        if not path.exists():
            self.logger.warning(f"âš ï¸ Checkpoint ä¸å­˜åœ¨: {path}")
            return False

        for worker in self.workers:
            worker.load_models(str(path), self.name)
        return True

    def _get_checkpoint_dir(self, tag: str) -> Path:
        """ç»Ÿä¸€è·¯å¾„ç”Ÿæˆé€»è¾‘ (save_dir å·²åŒ…å«å®éªŒå)"""
        return Path(self.cfg.save_dir) / "checkpoints" / tag


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨                                                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class StagedEnsembleTrainer(CheckpointMixin):
    """
    ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    å®ç°ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥è®­ç»ƒæ·±åº¦é›†æˆæ¨¡å‹:

    é˜¶æ®µåˆ’åˆ†:
        1. Warmupé˜¶æ®µ: æ— é®æŒ¡çƒ­èº«è®­ç»ƒï¼Œè®©æ¨¡å‹å­¦ä¹ åŸºç¡€ç‰¹å¾
        2. Progressiveé˜¶æ®µ: æ¸è¿›å¼å¢åŠ é®æŒ¡ï¼ŒåŸ¹å…»æ¨¡å‹å…³æ³¨ä¸åŒåŒºåŸŸ
        3. Finetuneé˜¶æ®µ: å›ºå®šé®æŒ¡æ¯”ä¾‹å¾®è°ƒï¼Œç¨³å®šæ¨¡å‹æ€§èƒ½
    """

    def __init__(
        self,
        method_name,
        cfg,
        augmentation_method="perlin",
        use_curriculum=True,
        fixed_ratio=0.25,
        share_warmup_backbone=False,
    ):
        """ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨æ„é€ å‡½æ•° (å¤§çº²åŒ–)"""
        self.name = method_name
        self.cfg = cfg
        self.total_training_time = 0.0

        # 1. åˆå§‹åŒ–å±æ€§ä¸å¢å¼ºç­–ç•¥
        self.augmentation_method = augmentation_method
        self.use_curriculum = use_curriculum
        self.fixed_ratio = fixed_ratio
        self.share_warmup_backbone = share_warmup_backbone

        # 2. ç¡¬ä»¶ä¸æ—¥å¿—åˆå§‹åŒ–
        self._init_hardware_optimizations()
        self.setup_logging()
        self._init_monitoring_tools()

        # 3. åˆå§‹åŒ–å·¥ä½œèŠ‚ç‚¹ (Parallel Workers)
        self.workers: List[GPUWorker] = [
            GPUWorker(gid, cfg.num_models_per_gpu, cfg, augmentation_method)
            for gid in cfg.gpu_ids
        ]

        # 4. åˆå§‹åŒ–çŠ¶æ€è·Ÿè¸ªå˜é‡
        self._init_tracking_structures()

    def _init_hardware_optimizations(self):
        """é…ç½® Cuda åç«¯åŠ é€Ÿé€‰é¡¹"""
        if self.cfg.use_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

    def _init_monitoring_tools(self):
        """åˆå§‹åŒ– wandb è§‚æµ‹å·¥å…·"""

        # Weights & Biases
        self.wandb_run = None
        if getattr(self.cfg, "use_wandb", False):
            try:
                import wandb

                self.wandb_run = wandb.init(
                    project=getattr(self.cfg, "wandb_project", "ensemble"),
                    name=self.name,
                    config=self._get_wandb_config(),
                    mode="online",
                    reinit="finish_previous",
                    save_code=False,
                    settings=wandb.Settings(silent=True),
                )
            except ImportError:
                self.logger.warning("âš ï¸ wandb not installed, skipping")
            except Exception as e:
                self.logger.warning(f"âš ï¸ wandb init failed: {e}")

    def _get_wandb_config(self) -> dict:
        """æå–é…ç½®ç”¨äº wandb"""
        return {
            "model": self.cfg.model_name,
            "dataset": self.cfg.dataset_name,
            "batch_size": self.cfg.batch_size,
            "lr": self.cfg.lr,
            "optimizer": self.cfg.optimizer,
            "augmentation": self.augmentation_method,
            "use_curriculum": self.use_curriculum,
            "total_epochs": self.cfg.total_epochs,
        }

    def _init_tracking_structures(self):
        """åˆå§‹åŒ–è®­ç»ƒå†å²ã€æ—©åœä¸è®°å½•å™¨"""
        self.history = {
            k: []
            for k in [
                "epoch",
                "stage",
                "train_loss",
                "val_loss",
                "val_acc",
                "mask_ratio",
                "mask_prob",
                "lr",
                "time",
            ]
        }
        self.early_stopping = EarlyStopping(
            patience=self.cfg.early_stopping_patience,
            metrics={"val_loss": "min", "val_acc": "max"},
            criteria="any",
        )
        self._best_val_loss = float("inf")
        self.history_saver = HistorySaver(self.cfg.training_base_dir)

    def get_models(self) -> List[nn.Module]:
        """è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ (ä¸å…¶ä»– Trainer æ¥å£ä¸€è‡´)"""
        return [model for worker in self.workers for model in worker.models]

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logger = logging.getLogger(self.name)
        logger.handlers.clear()
        logger.setLevel(getattr(logging, self.cfg.log_level))

        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

        # æ–‡ä»¶è¾“å‡º (æ”¾åœ¨æ—¶é—´æˆ³ç›®å½•ä¸‹ï¼Œæ–‡ä»¶ååŒ…å«å®éªŒå)
        log_path = Path(self.cfg.training_base_dir) / f"{self.name}_train.log"
        file_handler = logging.FileHandler(log_path, mode="w")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        # æ§åˆ¶å°è¾“å‡º (å¯é€šè¿‡é…ç½®å…³é—­)
        if getattr(self.cfg, "log_to_console", False):
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)

        self.logger = logger

    def _get_mask_prob_by_epoch(self, epoch: int) -> float:
        """è®¡ç®—ç»™å®š epoch çš„é®ç½©æ¦‚ç‡ (åŸºäºä¸‰é˜¶æ®µ)

        ç­–ç•¥ (ä¸ warmup/progressive/finetune_epochs å…±ç”¨):
            - Warmup é˜¶æ®µ: mask_prob = 0 (ä¸ä½¿ç”¨é®ç½©)
            - Progressive é˜¶æ®µ: mask_prob ä» mask_start_prob çº¿æ€§å¢åŠ åˆ° mask_end_prob
            - Finetune é˜¶æ®µ: mask_prob = mask_end_prob (å›ºå®š)
        """
        cfg = self.cfg
        start_prob = cfg.mask_start_prob  # 0.0
        end_prob = cfg.mask_end_prob  # 0.8

        if epoch < cfg.warmup_epochs:
            # Warmup é˜¶æ®µ: æ¦‚ç‡ä¸º 0
            return 0.0
        elif epoch < cfg.warmup_epochs + cfg.progressive_epochs:
            # Progressive é˜¶æ®µ: æ¦‚ç‡ä» start_prob çº¿æ€§å¢åŠ åˆ° end_prob
            prog_epoch = epoch - cfg.warmup_epochs
            progress = prog_epoch / max(cfg.progressive_epochs - 1, 1)
            return start_prob + (end_prob - start_prob) * progress
        else:
            # Finetune é˜¶æ®µ: æ¦‚ç‡ä¿æŒ end_prob
            return end_prob

    def _get_stage_info(self, epoch: int) -> Tuple[int, str, float, float, bool]:
        """è·å–å½“å‰é˜¶æ®µä¿¡æ¯

        Returns:
            Tuple: (stage_num, stage_name, mask_ratio, mask_prob, use_mask)
        """
        cfg = self.cfg

        # æ¨¡å¼1: æ— å¢å¼º (Baseline)
        if self.augmentation_method == "none":
            return 1, "NoAug", 0.0, 0.0, False

        # è®¡ç®—ç»Ÿä¸€æ¦‚ç‡ (æ‰€æœ‰æ¨¡å¼å…±ç”¨)
        mask_prob = self._get_mask_prob_by_epoch(epoch)

        # æ¨¡å¼2: å›ºå®šå‚æ•°æ¨¡å¼
        if not self.use_curriculum:
            return 1, "Fixed", self.fixed_ratio, mask_prob, True

        # æ¨¡å¼3: è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ (ä¸‰é˜¶æ®µ)
        if epoch < cfg.warmup_epochs:
            # Warmup é˜¶æ®µ: ä¸ä½¿ç”¨é®ç½©
            return 1, "Warmup", 0.0, 0.0, False
        elif epoch < cfg.warmup_epochs + cfg.progressive_epochs:
            # Progressive é˜¶æ®µ: ratio çº¿æ€§å¢é•¿ï¼Œprob ä½¿ç”¨ç»Ÿä¸€ç­–ç•¥
            progress = (epoch - cfg.warmup_epochs) / max(cfg.progressive_epochs - 1, 1)
            mask_ratio = (
                cfg.mask_start_ratio
                + (cfg.mask_end_ratio - cfg.mask_start_ratio) * progress
            )
            return 2, "Progressive", mask_ratio, mask_prob, True
        else:
            # Finetune é˜¶æ®µ: ratio å›ºå®šï¼Œprob ä½¿ç”¨ç»Ÿä¸€ç­–ç•¥
            return 3, "Finetune", cfg.finetune_mask_ratio, mask_prob, True

    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> Tuple[float, float]:
        """è®­ç»ƒå•ä¸ª Epoch (å¤§çº²åŒ–)

        Returns:
            Tuple[float, float]: (train_loss, current_lr) - æœ¬ epoch çš„æŸå¤±å’Œä½¿ç”¨çš„å­¦ä¹ ç‡
        """
        # 1. å‡†å¤‡å½“å‰é˜¶æ®µå‚æ•°
        *_, m_ratio, m_prob, use_mask = self._get_stage_info(epoch)
        criterion = nn.CrossEntropyLoss(label_smoothing=self.cfg.label_smoothing)

        # 2. è®°å½•æœ¬ epoch ä½¿ç”¨çš„ LRï¼ˆåœ¨ step ä¹‹å‰ï¼‰
        current_lr = self.workers[0].get_lr()

        # 3. é¢„çƒ­ Workers (å¦‚é¢„è®¡ç®— Mask æ± )
        for w in self.workers:
            w.precompute_masks(m_ratio)

        # 4. æ‰§è¡Œæ‰¹æ¬¡è¿­ä»£
        train_loss = self._run_batch_iteration(
            train_loader, epoch, criterion, m_ratio, m_prob, use_mask
        )
        return train_loss, current_lr

    def _run_batch_iteration(self, loader, epoch, criterion, m_ratio, m_prob, use_mask):
        """å…·ä½“æ‰§è¡Œå¼ é‡æµåŠ¨ä¸æ¢¯åº¦æ›´æ–° (ä½¿ç”¨ Rich Progress)"""
        total_loss, n = 0.0, 0
        from rich.progress import (
            BarColumn,
            Progress,
            TaskProgressColumn,
            TextColumn,
            TimeRemainingColumn,
        )

        # åˆ¤æ–­æ˜¯å¦ä¸º Warmup å•æ¨¡å‹è®­ç»ƒæ¨¡å¼
        stage_num = self._get_stage_info(epoch)[0]
        is_warmup_single_model = self.share_warmup_backbone and stage_num == 1

        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeRemainingColumn(),
            console=console,
            transient=True,  # å®Œæˆåè‡ªåŠ¨æ¶ˆå¤±
        ) as progress:
            desc = f"Epoch {epoch + 1:3d}"
            if is_warmup_single_model:
                desc += " [Warmup-SingleModel]"
            task_id = progress.add_task(desc, total=len(loader))

            for inputs, targets in loader:
                if is_warmup_single_model:
                    # Warmup ä¼˜åŒ–ï¼šä»…è®­ç»ƒç¬¬ä¸€ä¸ª Worker çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
                    self.workers[0].train_batch_async(
                        inputs,
                        targets,
                        criterion,
                        m_ratio,
                        m_prob,
                        use_mask,
                        model_indices=[0],
                    )
                    batch_loss = self.workers[0].synchronize()
                else:
                    # æ­£å¸¸æ¨¡å¼ï¼šæ‰€æœ‰ Worker æ‰€æœ‰æ¨¡å‹
                    for w in self.workers:
                        w.train_batch_async(
                            inputs, targets, criterion, m_ratio, m_prob, use_mask
                        )
                    batch_loss = sum(w.synchronize() for w in self.workers) / len(
                        self.workers
                    )

                total_loss += batch_loss
                n += 1
                progress.update(task_id, advance=1)

        # æ­¥è¿›è°ƒåº¦å™¨ (æ‰€æœ‰æ¨¡å‹ï¼Œä¿æŒ LR åŒæ­¥)
        for w in self.workers:
            w.step_schedulers()
        return total_loss / n

    @torch.no_grad()
    def _validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """é›†æˆéªŒè¯è¿‡ç¨‹"""
        criterion = nn.CrossEntropyLoss()
        total_loss, correct, total = 0.0, 0, 0
        device = self.workers[0].device  # ä¸»è®¡ç®—è®¾å¤‡

        for inputs, targets in val_loader:
            # 1. èšåˆæ‰€æœ‰ Worker çš„é¢„æµ‹ Logits
            ensemble_logits = self._collect_ensemble_logits(inputs, device)
            targets = targets.to(device)

            # 2. è®¡ç®—æŒ‡æ ‡
            total_loss += criterion(ensemble_logits, targets).item()
            correct += (ensemble_logits.argmax(1) == targets).sum().item()
            total += targets.size(0)

        return total_loss / len(val_loader), 100.0 * correct / total

    def _collect_ensemble_logits(self, inputs, device):
        """ä»åˆ†å¸ƒå¼ Workers ä¸­æ”¶é›†å¹¶èšåˆé¢„æµ‹ç»“æœ"""
        from ..evaluation.strategies import get_ensemble_fn  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–

        # æ¯ä¸ª worker è¿”å› [num_models, batch, classes]ï¼Œconcat æˆ [total_models, batch, classes]
        logits_list = [w.predict_batch(inputs).to(device) for w in self.workers]
        stacked = torch.cat(logits_list, dim=0)  # [total_models, batch, classes]
        ensemble_fn = get_ensemble_fn(self.cfg)
        return ensemble_fn(stacked)  # [batch, classes]

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """æ‰§è¡Œå…¨ç”Ÿå‘½å‘¨æœŸè®­ç»ƒ (å¤§çº²åŒ–)"""
        self._log_training_start()
        self._train_start_time = time.time()  # ä¿å­˜ä¸ºå®ä¾‹å˜é‡ä¾› checkpoint ä½¿ç”¨
        current_stage = 0

        try:
            for epoch in range(self.cfg.total_epochs):
                # 1. å‘¨æœŸå‡†å¤‡ (é˜¶æ®µåˆ‡æ¢)
                current_stage = self._handle_epoch_prep(epoch, current_stage)

                # 2. æ‰§è¡Œè®­ç»ƒä¸éªŒè¯å¾ªç¯
                stats = self._run_epoch_cycle(train_loader, val_loader, epoch)
                stats["stage"] = current_stage

                # 3. ç”Ÿå‘½å‘¨æœŸé’©å­: è®°å½•ã€æŒä¹…åŒ–ã€æ—©åœ
                self._handle_epoch_post(epoch, stats)
                # æ—©åœåœ¨ warmup + progressive é˜¶æ®µåç”Ÿæ•ˆ (æ‰€æœ‰æ¨¡å¼ç»Ÿä¸€)
                finetune_start = self.cfg.warmup_epochs + self.cfg.progressive_epochs
                if epoch >= finetune_start and self.early_stopping(stats):
                    break

            self._finalize_training()

        except Exception as e:
            self._handle_training_error(e)
            raise
        finally:
            if self.wandb_run:
                import wandb

                wandb.finish()
                # æ¸…ç† wandb æœ¬åœ°ç¼“å­˜ç›®å½•
                wandb_dir = Path.cwd() / "wandb"
                if wandb_dir.exists():
                    shutil.rmtree(wandb_dir, ignore_errors=False)

    def _handle_epoch_prep(self, epoch, current_stage):
        """å¤„ç† Epoch å¼€å§‹å‰çš„é¢„å¤‡åŠ¨ä½œ (å¦‚é˜¶æ®µåˆ‡æ¢)"""
        s_num, s_name, *_ = self._get_stage_info(epoch)

        # ç»Ÿä¸€ backbone å…±äº«é€»è¾‘ï¼šåœ¨ warmup_epochs ç»“æŸåçš„ç¬¬ä¸€ä¸ª epoch è§¦å‘
        # æ— è®ºæ˜¯è¯¾ç¨‹å­¦ä¹ æ¨¡å¼è¿˜æ˜¯ Fixed æ¨¡å¼éƒ½é€‚ç”¨
        if epoch == self.cfg.warmup_epochs and self.share_warmup_backbone:
            self._broadcast_warmup_backbone()

        # é˜¶æ®µåˆ‡æ¢æ—¥å¿—
        if s_num != current_stage:
            self._log_stage_header(s_num)
        return s_num

    def _run_epoch_cycle(self, train_loader, val_loader, epoch):
        """æ‰§è¡Œå•ä¸ª Epoch çš„è®¡ç®—å¾ªç¯å¹¶æ”¶é›†æŒ‡æ ‡"""
        t0 = time.time()
        t_loss, current_lr = self._train_epoch(train_loader, epoch)
        v_loss, v_acc = self._validate(val_loader)

        # è·å–å½“å‰å…ƒæ•°æ®
        _, _, m_ratio, m_prob, _ = self._get_stage_info(epoch)
        return {
            "train_loss": t_loss,
            "val_loss": v_loss,
            "val_acc": v_acc,
            "mask_ratio": m_ratio,
            "mask_prob": m_prob,
            "lr": current_lr,  # ä½¿ç”¨æœ¬ epoch å®é™…ä½¿ç”¨çš„ LR
            "time": time.time() - t0,
        }

    def _handle_epoch_post(self, epoch, stats):
        """å¤„ç† Epoch ç»“æŸåçš„è¾…åŠ©åŠ¨ä½œ (æ—¥å¿—ã€å¿«ç…§)"""
        # 1. è®°å½•å†å²
        self._record_metrics(epoch, stats)

        # 2. ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäº loss)
        if stats["val_loss"] < self._best_val_loss:
            self._best_val_loss = stats["val_loss"]
            self._save_checkpoint("best")

        # 3. æ‰“å°æ±‡æ€»æ—¥å¿—
        self._log_epoch_summary(epoch, stats)

    def _log_training_start(self):
        self.logger.info(
            "=" * 70 + f"\nğŸ“ Staged Training Start: {self.name}\n" + "=" * 70
        )

    def _log_stage_header(self, num):
        titles = {
            1: "STAGE 1: WARMUP",
            2: "STAGE 2: PROGRESSIVE",
            3: "STAGE 3: FINETUNE",
        }
        self.logger.info(f"\n{'=' * 20} {titles.get(num, 'UNKNOWN')} {'=' * 20}")

    def _record_metrics(self, epoch, stats):
        """åŒæ­¥å†å²è®°å½•ä¸å¯è§†åŒ–å·¥å…·"""
        for k, v in stats.items():
            self.history[k].append(v)
        self.history["epoch"].append(epoch + 1)

        # wandb
        if self.wandb_run:
            import wandb

            wandb.log(
                {
                    "train_loss": stats["train_loss"],
                    "val_loss": stats["val_loss"],
                    "val_acc": stats["val_acc"],
                    "epoch_time": stats["time"],
                },
                step=epoch + 1,  # x è½´ä» 1 å¼€å§‹
            )

    def _log_epoch_summary(self, epoch, stats):
        self.logger.info(
            f"Epoch {epoch + 1:3d} | "
            f"T-Loss: {stats['train_loss']:.4f} | V-Loss: {stats['val_loss']:.4f} | "
            f"V-Acc: {stats['val_acc']:.2f}% | {stats['time']:.1f}s"
        )

    def _finalize_training(self):
        self.total_training_time = time.time() - self._train_start_time
        self.logger.info(f"\nâ±ï¸ Total Time: {format_duration(self.total_training_time)}")
        self.history_saver.save(self.history, filename=f"{self.name}_history")

    def _handle_training_error(self, error):
        self.logger.error(f"\nâŒ Training Failed: {error}")
        self.total_training_time = time.time() - self._train_start_time
        self._save_checkpoint("error")
        self.history_saver.save(self.history, filename=f"{self.name}_history")

    def _broadcast_warmup_backbone(self):
        """ä»ç¬¬ä¸€ä¸ªæ¨¡å‹è·å– backboneï¼Œå¹¿æ’­åˆ°æ‰€æœ‰å­æ¨¡å‹å¹¶é‡æ–°åˆå§‹åŒ–å„è‡ªçš„ classifier head"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ª worker çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºæº
        source_model = self.workers[0].models[0]
        backbone_state = source_model.get_backbone_state_dict()

        for worker in self.workers:
            worker.broadcast_backbone_and_reinit_heads(backbone_state)

        self.logger.info(
            "ğŸ”„ Shared warmup backbone to all models, re-initialized classifier heads"
        )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å®éªŒè¿è¡Œå‡½æ•°                                                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def train_experiment(
    cfg: Config,
    train_loader: DataLoader,
    val_loader: DataLoader,
) -> Tuple["StagedEnsembleTrainer", float]:
    """
    ä»…è®­ç»ƒå®éªŒ (ä¸åŒ…å«è¯„ä¼°)

    ä»€æ‰€æœ‰å¢å¼ºå‚æ•°ä» cfg è¯»å–:
    - cfg.experiment_name: å®éªŒåç§°
    - cfg.augmentation_method: å¢å¼ºæ–¹æ³•
    - cfg.use_curriculum: æ˜¯å¦ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ 
    - cfg.fixed_ratio: å›ºå®šé®æŒ¡æ¯”ä¾‹
    - cfg.mask_start_prob, cfg.mask_end_prob: ç»Ÿä¸€æ¦‚ç‡å¢é•¿å‚æ•° (ä¸ä¸‰é˜¶æ®µå…±ç”¨)
    - cfg.share_warmup_backbone: æ˜¯å¦å…±äº« backbone

    å‚æ•°:
        cfg: é…ç½®å¯¹è±¡ (åŒ…å«æ‰€æœ‰å®éªŒå‚æ•°)
        train_loader, val_loader: æ•°æ®åŠ è½½å™¨

    è¿”å›:
        (trainer, training_time)
    """
    trainer = StagedEnsembleTrainer(
        cfg.experiment_name,
        cfg,
        augmentation_method=cfg.augmentation_method,
        use_curriculum=cfg.use_curriculum,
        fixed_ratio=cfg.fixed_ratio,
        share_warmup_backbone=cfg.share_warmup_backbone,
    )

    # è®­ç»ƒ
    trainer.train(train_loader, val_loader)

    # åŠ è½½æœ€ä½³æ¨¡å‹
    trainer.load_checkpoint("best")

    # è¾“å‡º best checkpoint å¯¹åº”çš„ val acc
    if trainer.history["val_loss"]:
        best_idx = trainer.history["val_loss"].index(min(trainer.history["val_loss"]))
        best_val_acc = trainer.history["val_acc"][best_idx]
        best_epoch = trainer.history["epoch"][best_idx]
        get_logger().info(
            f"ğŸ“Š Best Checkpoint (Epoch {best_epoch}): Val Acc = {best_val_acc:.2f}%"
        )

    get_logger().info(f"âœ… Training completed: {cfg.experiment_name}")

    return trainer, trainer.total_training_time
