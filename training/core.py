"""
================================================================================
è®­ç»ƒæ ¸å¿ƒæ¨¡å—
================================================================================

StagedEnsembleTrainer (ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨)ã€train_experiment (å®éªŒå…¥å£å‡½æ•°)
"""

import logging
import shutil
import sys
import time
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from ..config import Config
from ..utils import ensure_dir, format_duration, get_logger
from .optimization import EarlyStopping
from .worker import GPUWorker, HistorySaver

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ Checkpoint Mixin                                                           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class CheckpointMixin:
    """æ£€æŸ¥ç‚¹ç®¡ç† Mixin

    æä¾› checkpoint ä¿å­˜ã€åŠ è½½ã€æ¸…ç†åŠŸèƒ½ã€‚
    éœ€è¦å­ç±»æä¾›: cfg, name, workers, history, best_val_loss, best_epoch,
                  early_stopping, total_training_time, augmentation_method,
                  use_curriculum, fixed_ratio, fixed_prob, logger
    """

    def _save_checkpoint(self, tag: str):
        """ä¿å­˜checkpoint"""
        checkpoint_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        ensure_dir(checkpoint_dir)

        for worker in self.workers:
            worker.save_models(str(checkpoint_dir), self.name)

        state = {
            "epoch": len(self.history["epoch"]),
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch,
            "history": self.history,
            "early_stopping_counter": self.early_stopping.counter,
            "total_training_time": self.total_training_time,
            "augmentation_method": self.augmentation_method,
            "use_curriculum": self.use_curriculum,
            "fixed_ratio": self.fixed_ratio,
            "fixed_prob": self.fixed_prob,
        }
        torch.save(state, checkpoint_dir / "trainer_state.pth")
        self.logger.info(f"ğŸ’¾ Saved checkpoint: {tag}")

    def load_checkpoint(self, tag: str = "best") -> bool:
        """åŠ è½½checkpoint"""
        checkpoint_dir = Path(self.cfg.save_dir) / "checkpoints" / self.name / tag
        if not checkpoint_dir.exists():
            self.logger.warning(f"âš ï¸ Checkpoint not found: {checkpoint_dir}")
            return False

        for worker in self.workers:
            worker.load_models(str(checkpoint_dir), self.name)

        state_path = checkpoint_dir / "trainer_state.pth"
        if state_path.exists():
            state = torch.load(state_path, weights_only=False)
            self.best_val_loss = state["best_val_loss"]
            self.best_epoch = state["best_epoch"]
            self.history = state["history"]
            self.early_stopping.counter = state.get("early_stopping_counter", 0)
            self.total_training_time = state.get("total_training_time", 0.0)
            self.augmentation_method = state.get(
                "augmentation_method", self.augmentation_method
            )
            self.use_curriculum = state.get("use_curriculum", self.use_curriculum)
            self.fixed_ratio = state.get("fixed_ratio", self.fixed_ratio)
            self.fixed_prob = state.get("fixed_prob", self.fixed_prob)
            self.logger.info(f"âœ… Loaded checkpoint: {tag}")
            self.logger.info(
                f"   Augmentation: {self.augmentation_method}, Curriculum: {self.use_curriculum}"
            )
            return True
        return False

    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§checkpoint"""
        checkpoint_base = Path(self.cfg.save_dir) / "checkpoints" / self.name
        if not checkpoint_base.exists():
            return

        epoch_dirs = [
            d for d in checkpoint_base.iterdir() if d.name.startswith("epoch_")
        ]
        epoch_dirs.sort(key=lambda x: int(x.name.split("_")[1]))

        if len(epoch_dirs) > self.cfg.keep_last_n_checkpoints:
            for old_dir in epoch_dirs[: -self.cfg.keep_last_n_checkpoints]:
                shutil.rmtree(old_dir)
                self.logger.info(f"ğŸ—‘ï¸ Removed old checkpoint: {old_dir.name}")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨                                                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class StagedEnsembleTrainer(CheckpointMixin):
    """
    ä¸‰é˜¶æ®µé›†æˆè®­ç»ƒå™¨ (æ”¯æŒå¤šç§æ•°æ®å¢å¼ºæ–¹æ³•)

    å®ç°ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥è®­ç»ƒæ·±åº¦é›†æˆæ¨¡å‹:

    é˜¶æ®µåˆ’åˆ†:
        1. Warmupé˜¶æ®µ: æ— é®æŒ¡çƒ­èº«è®­ç»ƒï¼Œè®©æ¨¡å‹å­¦ä¹ åŸºç¡€ç‰¹å¾
        2. Progressiveé˜¶æ®µ: æ¸è¿›å¼å¢åŠ é®æŒ¡ï¼ŒåŸ¹å…»æ¨¡å‹å…³æ³¨ä¸åŒåŒºåŸŸ
        3. Finetuneé˜¶æ®µ: å›ºå®šé®æŒ¡æ¯”ä¾‹å¾®è°ƒï¼Œç¨³å®šæ¨¡å‹æ€§èƒ½
    """

    def __init__(
        self,
        method_name: str,
        cfg: Config,
        augmentation_method: str = "perlin",
        use_curriculum: bool = True,
        fixed_ratio: float = 0.25,
        fixed_prob: float = 0.5,
        share_warmup_backbone: bool = False,
    ):
        self.name = method_name
        self.cfg = cfg
        self.total_training_time = 0.0

        # å¢å¼ºé…ç½®
        self.augmentation_method = augmentation_method
        self.use_curriculum = use_curriculum
        self.fixed_ratio = fixed_ratio
        self.fixed_prob = fixed_prob
        self.share_warmup_backbone = share_warmup_backbone

        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if cfg.use_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

        get_logger().info(f"\nğŸš€ Initializing {method_name}")
        get_logger().info(f"   Augmentation: {augmentation_method}")
        get_logger().info(f"   Curriculum: {'Yes' if use_curriculum else 'No'}")
        get_logger().info(
            f"   Config: {cfg.total_models} {cfg.model_name} models across {len(cfg.gpu_ids)} GPUs"
        )

        # åˆ›å»ºWorkers
        self.workers: List[GPUWorker] = []
        for gpu_id in cfg.gpu_ids:
            worker = GPUWorker(gpu_id, cfg.num_models_per_gpu, cfg, augmentation_method)
            self.workers.append(worker)

        # æ—¥å¿—ç³»ç»Ÿ
        self.setup_logging()

        # TensorBoard
        self.writer = None
        if cfg.use_tensorboard:
            log_dir = Path(cfg.save_dir) / "tensorboard" / self.name
            self.writer = SummaryWriter(str(log_dir))
            get_logger().info(f"ğŸ“Š TensorBoard logging to: {log_dir}")

        # è®­ç»ƒå†å²
        self.history = {
            "epoch": [],
            "stage": [],
            "train_loss": [],
            "val_loss": [],
            "val_acc": [],
            "mask_ratio": [],
            "mask_prob": [],
            "lr": [],
            "epoch_time": [],
        }

        # æ—©åœ
        self.early_stopping = EarlyStopping(
            patience=cfg.early_stopping_patience, mode="min"
        )

        self.best_val_loss = float("inf")
        self.best_epoch = 0

        # æŒ‡æ ‡è®¡ç®—å™¨å’Œå†å²ä¿å­˜å™¨
        self.history_saver = HistorySaver(cfg.save_dir)

        # ä¿å­˜é…ç½®
        cfg.save()

    def get_models(self) -> List[nn.Module]:
        """è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ (ä¸å…¶ä»– Trainer æ¥å£ä¸€è‡´)"""
        return [model for worker in self.workers for model in worker.models]

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.cfg.save_dir) / "logs"
        ensure_dir(log_dir)

        logger = logging.getLogger(self.name)
        logger.handlers.clear()
        logger.setLevel(getattr(logging, self.cfg.log_level))

        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

        file_handler = logging.FileHandler(log_dir / f"{self.name}_train.log", mode="w")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        self.logger = logger

    def _get_stage_info(self, epoch: int) -> Tuple[int, str, float, float, bool]:
        """è·å–å½“å‰é˜¶æ®µä¿¡æ¯

        Returns:
            Tuple: (stage_num, stage_name, mask_ratio, mask_prob, use_mask)
        """
        cfg = self.cfg

        # æ¨¡å¼1: æ— å¢å¼º (Baseline)
        if self.augmentation_method == "none":
            return 1, "NoAug", 0.0, 0.0, False

        # æ¨¡å¼2: å›ºå®šå‚æ•°æ¨¡å¼
        if not self.use_curriculum:
            return 1, "Fixed", self.fixed_ratio, self.fixed_prob, True

        # æ¨¡å¼3: è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ (ä¸‰é˜¶æ®µ)
        if epoch < cfg.warmup_epochs:
            return 1, "Warmup", 0.0, 0.0, False
        elif epoch < cfg.warmup_epochs + cfg.progressive_epochs:
            progress = (epoch - cfg.warmup_epochs) / cfg.progressive_epochs
            mask_ratio = (
                cfg.mask_start_ratio
                + (cfg.mask_end_ratio - cfg.mask_start_ratio) * progress
            )
            mask_prob = (
                cfg.mask_prob_start
                + (cfg.mask_prob_end - cfg.mask_prob_start) * progress
            )
            return 2, "Progressive", mask_ratio, mask_prob, True
        else:
            return 3, "Finetune", cfg.finetune_mask_ratio, cfg.finetune_mask_prob, True

    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        criterion = nn.CrossEntropyLoss(label_smoothing=self.cfg.label_smoothing)
        stage_num, stage_name, mask_ratio, mask_prob, use_mask = self._get_stage_info(
            epoch
        )

        # é¢„è®¡ç®—maskï¼ˆå¦‚æœéœ€è¦ï¼‰
        for worker in self.workers:
            worker.precompute_masks(self.cfg.mask_pool_size, mask_ratio)

        total_loss = 0.0
        num_batches = 0
        current_lr = self.workers[0].get_lr()
        iterator = tqdm(
            train_loader,
            desc=f"Epoch {epoch + 1}/{self.cfg.total_epochs} [{stage_name}] lr={current_lr:.6f}",
        )

        for inputs, targets in iterator:
            # å¼‚æ­¥è®­ç»ƒ
            for worker in self.workers:
                worker.train_batch_async(
                    inputs, targets, criterion, mask_ratio, mask_prob, use_mask
                )

            # åŒæ­¥å¹¶ç´¯è®¡loss
            batch_loss = 0.0
            for worker in self.workers:
                batch_loss += worker.synchronize()

            total_loss += batch_loss / len(self.workers)
            num_batches += 1

            iterator.set_postfix({"loss": total_loss / num_batches})

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆschedulerä¼šåŸºäºç¼©æ”¾åçš„lrç»§ç»­è°ƒæ•´ï¼‰
        for worker in self.workers:
            worker.step_schedulers()

        return total_loss / num_batches

    @torch.no_grad()
    def _validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯"""
        criterion = nn.CrossEntropyLoss()
        total_loss = 0.0
        correct = 0
        total = 0

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªworkerçš„è®¾å¤‡ä½œä¸ºä¸»è®¾å¤‡è¿›è¡Œè®¡ç®—
        primary_device = self.workers[0].device

        for inputs, targets in val_loader:
            all_logits = []
            for worker in self.workers:
                worker_logits = worker.predict_batch(inputs)
                all_logits.append(worker_logits.to(primary_device))

            all_logits = torch.cat(all_logits, dim=0)
            ensemble_logits = all_logits.mean(dim=0)

            # ç¡®ä¿targetsä¹Ÿåœ¨ä¸»è®¾å¤‡ä¸Š
            targets = targets.to(primary_device)

            loss = criterion(ensemble_logits, targets)
            total_loss += loss.item()

            preds = ensemble_logits.argmax(dim=1)
            correct += (preds == targets).sum().item()
            total += targets.size(0)

        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒ"""
        self.logger.info("=" * 70)
        self.logger.info(f"ğŸ“ Three-Stage Curriculum Learning: {self.name}")
        self.logger.info("=" * 70)

        current_stage = 0
        training_start_time = time.time()

        try:
            for epoch in range(self.cfg.total_epochs):
                epoch_start_time = time.time()
                stage_num, stage_name, mask_ratio, mask_prob, use_mask = (
                    self._get_stage_info(epoch)
                )

                # é˜¶æ®µåˆ‡æ¢æç¤º
                if stage_num != current_stage:
                    # å…±äº« backbone: åœ¨ä» Stage 1 åˆ‡æ¢åˆ° Stage 2 æ—¶å¹¿æ’­
                    if (
                        stage_num == 2
                        and current_stage == 1
                        and self.share_warmup_backbone
                    ):
                        self._broadcast_warmup_backbone()

                    current_stage = stage_num
                    self.logger.info("")
                    self.logger.info("=" * 70)
                    if stage_num == 1:
                        self.logger.info("ğŸ”¥ STAGE 1: WARMUP (No Mask)")
                    elif stage_num == 2:
                        self.logger.info("ğŸ­ STAGE 2: PROGRESSIVE MASKING")
                    else:
                        self.logger.info("ğŸ¯ STAGE 3: FINE-TUNING")
                    self.logger.info("=" * 70)

                # è®­ç»ƒ
                try:
                    train_loss = self._train_epoch(train_loader, epoch)
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        self.logger.error("âŒ GPU Out of Memory!")
                        torch.cuda.empty_cache()
                        raise
                    else:
                        raise

                # éªŒè¯
                val_loss, val_acc = self._validate(val_loader)

                epoch_elapsed = time.time() - epoch_start_time
                current_lr = self.workers[0].optimizers[0].param_groups[0]["lr"]

                # è®°å½•å†å²
                self.history["epoch"].append(epoch + 1)
                self.history["stage"].append(stage_num)
                self.history["train_loss"].append(train_loss)
                self.history["val_loss"].append(val_loss)
                self.history["val_acc"].append(val_acc)
                self.history["mask_ratio"].append(mask_ratio)
                self.history["mask_prob"].append(mask_prob)
                self.history["lr"].append(current_lr)
                self.history["epoch_time"].append(epoch_elapsed)

                # TensorBoard
                if self.writer:
                    self.writer.add_scalar("Loss/train", train_loss, epoch)
                    self.writer.add_scalar("Loss/val", val_loss, epoch)
                    self.writer.add_scalar("Accuracy/val", val_acc, epoch)
                    self.writer.add_scalar("Hyperparameters/lr", current_lr, epoch)
                    self.writer.add_scalar(
                        "Time/epoch_duration_sec", epoch_elapsed, epoch
                    )

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self._save_checkpoint("best")
                    self.logger.info(f"   ğŸ† New best model! Val Loss: {val_loss:.4f}")

                # å®šæœŸä¿å­˜
                if (epoch + 1) % self.cfg.save_every_n_epochs == 0:
                    self._save_checkpoint(f"epoch_{epoch + 1}")
                    self._cleanup_old_checkpoints()

                # æ—¥å¿—
                mask_info = (
                    f"MaskR={mask_ratio:.1%}, MaskP={mask_prob:.1%}"
                    if use_mask
                    else "NoMask"
                )
                self.logger.info(
                    f"Epoch {epoch + 1:3d}/{self.cfg.total_epochs} [{stage_name:11s}] | "
                    f"TrainLoss: {train_loss:.4f} | ValLoss: {val_loss:.4f} | ValAcc: {val_acc:.2f}% | "
                    f"{mask_info} | LR: {current_lr:.6f} | Time: {epoch_elapsed:.1f}s"
                )

                # æ—©åœæ£€æŸ¥
                if self.early_stopping(val_loss, epoch):
                    self.logger.info(
                        f"\nâš ï¸ Early stopping triggered at epoch {epoch + 1}"
                    )
                    break

            self.total_training_time = time.time() - training_start_time
            self.logger.info(
                f"\nâ±ï¸ Total Training Time: {format_duration(self.total_training_time)}"
            )

            self._save_checkpoint("final")
            self.history_saver.save(self.history)
            self.logger.info(f"\nâœ… Training completed: {self.name}")

        except KeyboardInterrupt:
            self.logger.info("\nâš ï¸ Training interrupted by user")
            self.total_training_time = time.time() - training_start_time
            self._save_checkpoint("interrupted")
            self.history_saver.save(self.history)
            raise
        except Exception as e:
            self.logger.error(f"\nâŒ Training failed with error: {e}")
            self._save_checkpoint("error")
            self.history_saver.save(self.history)
            raise
        finally:
            if self.writer:
                self.writer.close()

    def _broadcast_warmup_backbone(self):
        """ä»ç¬¬ä¸€ä¸ªæ¨¡å‹è·å– backboneï¼Œå¹¿æ’­åˆ°æ‰€æœ‰å­æ¨¡å‹å¹¶é‡æ–°åˆå§‹åŒ–å„è‡ªçš„ classifier head"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ª worker çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºæº
        source_model = self.workers[0].models[0]
        backbone_state = source_model.get_backbone_state_dict()

        for worker in self.workers:
            worker.broadcast_backbone_and_reinit_heads(backbone_state)

        self.logger.info(
            "ğŸ”„ Shared warmup backbone to all models, re-initialized classifier heads"
        )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ å®éªŒè¿è¡Œå‡½æ•°                                                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def train_experiment(
    cfg: Config,
    train_loader: DataLoader,
    val_loader: DataLoader,
) -> Tuple["StagedEnsembleTrainer", float]:
    """
    ä»…è®­ç»ƒå®éªŒ (ä¸åŒ…å«è¯„ä¼°)

    æ‰€æœ‰å¢å¼ºå‚æ•°ä» cfg è¯»å–:
    - cfg.experiment_name: å®éªŒåç§°
    - cfg.augmentation_method: å¢å¼ºæ–¹æ³•
    - cfg.use_curriculum: æ˜¯å¦ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ 
    - cfg.fixed_ratio, cfg.fixed_prob: å›ºå®šé®æŒ¡å‚æ•°
    - cfg.share_warmup_backbone: æ˜¯å¦å…±äº« backbone

    å‚æ•°:
        cfg: é…ç½®å¯¹è±¡ (åŒ…å«æ‰€æœ‰å®éªŒå‚æ•°)
        train_loader, val_loader: æ•°æ®åŠ è½½å™¨

    è¿”å›:
        (trainer, training_time)
    """
    trainer = StagedEnsembleTrainer(
        cfg.experiment_name,
        cfg,
        augmentation_method=cfg.augmentation_method,
        use_curriculum=cfg.use_curriculum,
        fixed_ratio=cfg.fixed_ratio,
        fixed_prob=cfg.fixed_prob,
        share_warmup_backbone=cfg.share_warmup_backbone,
    )

    # è®­ç»ƒ
    trainer.train(train_loader, val_loader)
    training_time = trainer.total_training_time

    # åŠ è½½æœ€ä½³æ¨¡å‹
    trainer.load_checkpoint("best")
    trainer.total_training_time = training_time

    get_logger().info(f"\nâœ… Training completed: {cfg.experiment_name}")
    get_logger().info(
        f"   Checkpoint saved to: {Path(cfg.save_dir) / 'checkpoints' / cfg.experiment_name}"
    )

    return trainer, training_time
