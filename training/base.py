"""
================================================================================
è®­ç»ƒå™¨åŸºç±»æ¨¡å—
================================================================================

æ‰€æœ‰è®­ç»ƒå™¨å…±äº«çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£ã€‚
"""

import logging
import sys
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional

import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from ..config import Config
from ..utils import ensure_dir, format_duration
from .scheduler import EarlyStopping
from .worker import HistorySaver


class BaseTrainer(ABC):
    """
    è®­ç»ƒå™¨æŠ½è±¡åŸºç±»

    æ‰€æœ‰è®­ç»ƒå™¨å¿…é¡»å®ç°çš„æ¥å£ï¼š
    - train(train_loader, val_loader): æ‰§è¡Œå®Œæ•´è®­ç»ƒ
    - get_models() -> List[nn.Module]: è·å–æ¨¡å‹åˆ—è¡¨
    - load_checkpoint(tag) -> bool: åŠ è½½æ£€æŸ¥ç‚¹
    - _save_checkpoint(tag): ä¿å­˜æ£€æŸ¥ç‚¹ (å®ç°ç»†èŠ‚)

    å…±äº«å±æ€§ï¼š
    - name: å®éªŒåç§°
    - cfg: é…ç½®å¯¹è±¡
    - total_training_time: æ€»è®­ç»ƒæ—¶é—´
    - history: è®­ç»ƒå†å²
    - logger: æ—¥å¿—è®°å½•å™¨
    """

    def __init__(self, method_name: str, cfg: Config):
        self.name = method_name
        self.cfg = cfg
        self.total_training_time = 0.0

        # è®­ç»ƒå†å² (å­ç±»å¯ä»¥æ‰©å±•)
        self.history: Dict[str, List] = {
            "epoch": [],
            "train_loss": [],
            "val_loss": [],
            "val_acc": [],
            "lr": [],
            "epoch_time": [],
        }

        # æ—©åœ
        self.early_stopping = EarlyStopping(
            patience=cfg.early_stopping_patience, mode="min"
        )
        self.best_val_loss = float("inf")
        self.best_epoch = 0

        # å†å²ä¿å­˜å™¨
        self.history_saver = HistorySaver(cfg.save_dir)

        # TensorBoard
        self.writer: Optional[SummaryWriter] = None
        if cfg.use_tensorboard:
            log_dir = Path(cfg.save_dir) / "tensorboard" / self.name
            ensure_dir(log_dir)
            self.writer = SummaryWriter(str(log_dir))

        # æ—¥å¿— (å»¶è¿Ÿåˆå§‹åŒ–)
        self.logger: Optional[logging.Logger] = None

    def setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.cfg.save_dir) / "logs"
        ensure_dir(log_dir)

        logger = logging.getLogger(self.name)
        logger.handlers.clear()
        logger.setLevel(getattr(logging, self.cfg.log_level))

        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

        file_handler = logging.FileHandler(log_dir / f"{self.name}_train.log", mode="w")
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        self.logger = logger

    @abstractmethod
    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> None:
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒ"""
        pass

    @abstractmethod
    def get_models(self) -> List[nn.Module]:
        """è·å–æ¨¡å‹åˆ—è¡¨ç”¨äºè¯„ä¼°"""
        pass

    @abstractmethod
    def load_checkpoint(self, tag: str = "best") -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        pass

    @abstractmethod
    def _save_checkpoint(self, tag: str) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        pass

    def _log_epoch(
        self,
        epoch: int,
        train_loss: float,
        val_loss: float,
        val_acc: float,
        lr: float,
        epoch_time: float,
        extra_info: str = "",
    ) -> None:
        """è®°å½• epoch ä¿¡æ¯åˆ°å†å²å’Œ TensorBoard"""
        # è®°å½•å†å²
        self.history["epoch"].append(epoch + 1)
        self.history["train_loss"].append(train_loss)
        self.history["val_loss"].append(val_loss)
        self.history["val_acc"].append(val_acc)
        self.history["lr"].append(lr)
        self.history["epoch_time"].append(epoch_time)

        # TensorBoard
        if self.writer:
            self.writer.add_scalar("Loss/train", train_loss, epoch)
            self.writer.add_scalar("Loss/val", val_loss, epoch)
            self.writer.add_scalar("Accuracy/val", val_acc, epoch)
            self.writer.add_scalar("Hyperparameters/lr", lr, epoch)

        # æ§åˆ¶å°æ—¥å¿—
        if self.logger:
            self.logger.info(
                f"Epoch {epoch + 1:3d}/{self.cfg.total_epochs} | "
                f"TrainLoss: {train_loss:.4f} | ValLoss: {val_loss:.4f} | "
                f"ValAcc: {val_acc:.2f}% | LR: {lr:.6f} | Time: {epoch_time:.1f}s"
                f"{extra_info}"
            )

    def _check_best_and_save(self, val_loss: float, epoch: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹å¹¶ä¿å­˜"""
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.best_epoch = epoch
            self._save_checkpoint("best")
            if self.logger:
                self.logger.info(f"   ğŸ† New best! Val Loss: {val_loss:.4f}")
            return True
        return False

    def _finalize_training(self, training_start_time: float) -> None:
        """è®­ç»ƒç»“æŸåçš„æ¸…ç†å·¥ä½œ"""
        self.total_training_time = time.time() - training_start_time

        if self.logger:
            self.logger.info(
                f"\nâ±ï¸ Total Time: {format_duration(self.total_training_time)}"
            )

        self._save_checkpoint("final")
        self.history_saver.save(self.history)

        if self.logger:
            self.logger.info(f"\nâœ… Training completed: {self.name}")

        if self.writer:
            self.writer.close()

    def _handle_interrupt(self) -> None:
        """å¤„ç†ç”¨æˆ·ä¸­æ–­"""
        if self.logger:
            self.logger.info("\nâš ï¸ Interrupted by user")
        self._save_checkpoint("interrupted")
        self.history_saver.save(self.history)
